{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d62ae3-84f1-461e-87f3-c874df193470",
   "metadata": {},
   "source": [
    "# Preprocessing Pipelines for All Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52e79c-556b-4e7b-bfcc-d3f25d98cbcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e4e79287-bb6e-4a27-8a64-ea41792e611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_and_save_edos(path, binary=True, output_dir=\"./splits\"):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"clean_text\"] = df[\"text\"].str.lower().str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "   \n",
    "    if binary:\n",
    "        df[\"label\"] = df[\"label_sexist\"].map({\"not sexist\": 0, \"sexist\": 1})\n",
    "    else:\n",
    "        df[\"label_category\"] = df[\"label_category\"].fillna(\"none\")\n",
    "        cat_map = {cat: i for i, cat in enumerate(df[\"label_category\"].unique())}\n",
    "        df[\"label\"] = df[\"label_category\"].map(cat_map)\n",
    "\n",
    "\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42\n",
    "    )\n",
    "\n",
    "    train_df = train_df[[\"clean_text\", \"label\"]]\n",
    "    val_df   = val_df[[\"clean_text\", \"label\"]]\n",
    "    test_df  = test_df[[\"clean_text\", \"label\"]]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_df.to_csv(f\"{output_dir}/edos_train.csv\", index=False)\n",
    "    val_df.to_csv(f\"{output_dir}/edos_val.csv\", index=False)\n",
    "    test_df.to_csv(f\"{output_dir}/edos_test.csv\", index=False)\n",
    "\n",
    "    print(\"EDOS splits (train/val/test) saved to:\", output_dir)\n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86cc963-483d-4c7a-9360-bc33ccc24c30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Davidson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "95b440fe-1c42-4088-ac3d-c62ea00e8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_and_save_davidson(path, output_dir=\"./splits\"):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.rename(columns={\"tweet\": \"text\", \"class\": \"label\"})\n",
    "    df[\"clean_text\"] = df[\"text\"].str.lower().str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    "    )\n",
    "\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    train_df = train_df[[\"clean_text\", \"label\"]]\n",
    "    val_df   = val_df[[\"clean_text\", \"label\"]]\n",
    "    test_df  = test_df[[\"clean_text\", \"label\"]]\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_df.to_csv(f\"{output_dir}/davidson_train.csv\", index=False)\n",
    "    val_df.to_csv(f\"{output_dir}/davidson_val.csv\", index=False)\n",
    "    test_df.to_csv(f\"{output_dir}/davidson_test.csv\", index=False)\n",
    "\n",
    "    print(\"Davidson splits (train/val/test) saved to:\", output_dir)\n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999c21d-6c6b-4033-8a15-64f1933237f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "14f90d8d-5469-4ae7-8017-5643afa4446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def extract_text(tokens):\n",
    "    return \" \".join(tokens) if isinstance(tokens, list) else \"\"\n",
    "\n",
    "def extract_label(annotators):\n",
    "    if isinstance(annotators, list) and len(annotators) > 0:\n",
    "        return annotators[0].get(\"label\", None)\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)         \n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                  \n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)            \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()           \n",
    "    return text.lower()\n",
    "\n",
    "def preprocess_and_save_hatexplain_json(json_path, output_dir=\"./splits\"):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    df_raw = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"text\"] = df_raw[\"post_tokens\"].apply(extract_text)\n",
    "    df[\"label\"] = df_raw[\"annotators\"].apply(extract_label)\n",
    "    df = df[df[\"label\"].isin([\"normal\", \"offensive\", \"hatespeech\"])].copy()\n",
    "\n",
    "    df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "    label_map = {\"normal\": 0, \"offensive\": 1, \"hatespeech\": 2}\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "\n",
    "\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)\n",
    "\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_df[[\"clean_text\", \"label\"]].to_csv(f\"{output_dir}/hatexplain_train.csv\", index=False)\n",
    "    val_df[[\"clean_text\", \"label\"]].to_csv(f\"{output_dir}/hatexplain_val.csv\", index=False)\n",
    "    test_df[[\"clean_text\", \"label\"]].to_csv(f\"{output_dir}/hatexplain_test.csv\", index=False)\n",
    "\n",
    "    print(\"HateXplain 80/10/10 splits saved successfully.\")\n",
    "    return train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a8c83-fc10-4a9a-818e-573d09f5f21d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Function Calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e0231657-1ab6-4ceb-a2ea-a4328d4a503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edos_path = \"/Users/sandyajaleshkumar/Desktop/Practicum/Datasets/EDOS Dataset/edos_labelled_aggregated.csv\"\n",
    "davidson_path = \"/Users/sandyajaleshkumar/Desktop/Practicum/Datasets/Davidson Dataset/labeled_data.csv\"\n",
    "hatexplain_path = \"/Users/sandyajaleshkumar/Desktop/Practicum/Datasets/HateXplain Dataset/dataset.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "116aec57-a3f7-4635-a166-f4a2261d63dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDOS splits (train/val/test) saved to: ./splits\n",
      "Davidson splits (train/val/test) saved to: ./splits\n",
      "HateXplain 80/10/10 splits saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_dir = \"./splits\"\n",
    "\n",
    "\n",
    "edos_train_df, edos_val_df, edos_test_df = preprocess_and_save_edos(edos_path, binary=True, output_dir=output_dir)\n",
    "davidson_train_df, davidson_val_df, davidson_test_df = preprocess_and_save_davidson(davidson_path, output_dir=output_dir)\n",
    "hatexplain_train_df, hatexplain_val_df, hatexplain_test_df = preprocess_and_save_hatexplain_json(hatexplain_path, output_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eaf1d3-4f05-400a-8500-099d1606b9aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Number of Tokens and Dataset Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a586ba-2718-416e-a0f1-746e528cbf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens per text: 22.15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./splits/edos_train.csv\")  \n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "avg_length = sum(lengths) / len(lengths)\n",
    "print(f\"Average number of tokens per text: {avg_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "988bd3c7-cb13-4ace-99e3-0fa7add6740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # characters: 121.79\n",
      "Average # word tokens: 23.14\n",
      "Average # BERT tokens: 28.25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "df = pd.read_csv(\"./splits/edos_train.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "char_lens = [len(t) for t in texts]\n",
    "avg_char_len = sum(char_lens) / len(char_lens)\n",
    "\n",
    "word_lens = [len(t.split()) for t in texts]\n",
    "avg_word_len = sum(word_lens) / len(word_lens)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_lens = [len(tokenizer.encode(t, truncation=True, max_length=512)) for t in texts]\n",
    "avg_bert_len = sum(bert_lens) / len(bert_lens)\n",
    "\n",
    "\n",
    "print(f\"Average # characters: {avg_char_len:.2f}\")\n",
    "print(f\"Average # word tokens: {avg_word_len:.2f}\")\n",
    "print(f\"Average # BERT tokens: {avg_bert_len:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c906396-f551-4391-b434-4d52e62bcd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length: 125.96\n",
      "Average word token count: 23.35\n",
      "Average BERT token count: 28.65\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_train.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "char_lens = [len(t) for t in texts]\n",
    "avg_char_len = sum(char_lens) / len(char_lens)\n",
    "\n",
    "word_lens = [len(t.split()) for t in texts]\n",
    "avg_word_len = sum(word_lens) / len(word_lens)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_lens = [len(tokenizer.encode(t, truncation=True, max_length=512)) for t in texts]\n",
    "avg_bert_len = sum(bert_lens) / len(bert_lens)\n",
    "\n",
    "print(f\"Average character length: {avg_char_len:.2f}\")\n",
    "print(f\"Average word token count: {avg_word_len:.2f}\")\n",
    "print(f\"Average BERT token count: {avg_bert_len:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28bbeba9-b94d-4061-aa54-03b333ee09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length: 79.36\n",
      "Average word token count: 14.01\n",
      "Average BERT token count: 23.91\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_train.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "char_lens = [len(t) for t in texts]\n",
    "avg_char_len = sum(char_lens) / len(char_lens)\n",
    "\n",
    "word_lens = [len(t.split()) for t in texts]\n",
    "avg_word_len = sum(word_lens) / len(word_lens)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_lens = [len(tokenizer.encode(t, truncation=True, max_length=512)) for t in texts]\n",
    "avg_bert_len = sum(bert_lens) / len(bert_lens)\n",
    "\n",
    "print(f\"Average character length: {avg_char_len:.2f}\")\n",
    "print(f\"Average word token count: {avg_word_len:.2f}\")\n",
    "print(f\"Average BERT token count: {avg_bert_len:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f2f5bb2d-4ecd-4bbe-9384-b44c21a156f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EDOS Dataset Summary\n",
      "Split  Total Rows  Class 0  Class 1\n",
      "Train       16000    12117     3883\n",
      "  Val        2000     1514      486\n",
      " Test        2000     1515      485\n",
      "\n",
      "DAVIDSON Dataset Summary\n",
      "Split  Total Rows  Class 0  Class 1  Class 2\n",
      "Train       19826     1144    15352     3330\n",
      "  Val        2478      143     1919      416\n",
      " Test        2479      143     1919      417\n",
      "\n",
      "HATEXPLAIN Dataset Summary\n",
      "Split  Total Rows  Class 0  Class 1  Class 2\n",
      "Train       16118     6567     4758     4793\n",
      "  Val        2015      821      595      599\n",
      " Test        2015      821      595      599\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datasets = [\"edos\", \"davidson\", \"hatexplain\"]\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n{dataset.upper()} Dataset Summary\")\n",
    "    \n",
    "    rows = []\n",
    "    for split in splits:\n",
    "        path = f\"./splits/{dataset}_{split}.csv\"\n",
    "        df = pd.read_csv(path)\n",
    "        total_rows = len(df)\n",
    "        class_counts = df[\"label\"].value_counts().to_dict()\n",
    "        \n",
    "        row = {\n",
    "            \"Split\": split.capitalize(),\n",
    "            \"Total Rows\": total_rows\n",
    "        }\n",
    "        for label in sorted(class_counts):\n",
    "            row[f\"Class {label}\"] = class_counts[label]\n",
    "        rows.append(row)\n",
    "    \n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    summary_df = summary_df.fillna(0).astype({\"Total Rows\": int, **{col: int for col in summary_df.columns if col.startswith(\"Class\")}})\n",
    "    print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e8edb-d8c8-4ba6-9900-79d9d5071cf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Section A: Shallow (Static) Embeddings: TF-IDF + Classifiers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef584ab7-2c85-47f0-8fab-9c5cf4285cd0",
   "metadata": {},
   "source": [
    "## EDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2b7d1f3b-43be-4e50-9c35-ff6c7867bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EDOS_SVC ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8530    0.9392    0.8941      1514\n",
      "           1     0.7237    0.4959    0.5885       486\n",
      "\n",
      "    accuracy                         0.8315      2000\n",
      "   macro avg     0.7884    0.7176    0.7413      2000\n",
      "weighted avg     0.8216    0.8315    0.8198      2000\n",
      "\n",
      "\n",
      "--- EDOS_LR ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8170    0.9729    0.8882      1514\n",
      "           1     0.7919    0.3210    0.4568       486\n",
      "\n",
      "    accuracy                         0.8145      2000\n",
      "   macro avg     0.8044    0.6470    0.6725      2000\n",
      "weighted avg     0.8109    0.8145    0.7833      2000\n",
      "\n",
      "\n",
      "--- EDOS_RF ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8248    0.9921    0.9007      1514\n",
      "           1     0.9330    0.3436    0.5023       486\n",
      "\n",
      "    accuracy                         0.8345      2000\n",
      "   macro avg     0.8789    0.6678    0.7015      2000\n",
      "weighted avg     0.8511    0.8345    0.8039      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:28:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EDOS_XGB ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8393    0.9624    0.8966      1514\n",
      "           1     0.7841    0.4259    0.5520       486\n",
      "\n",
      "    accuracy                         0.8320      2000\n",
      "   macro avg     0.8117    0.6941    0.7243      2000\n",
      "weighted avg     0.8259    0.8320    0.8129      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib, os\n",
    "\n",
    "edos_train = pd.read_csv(\"./splits/edos_train.csv\")\n",
    "edos_val   = pd.read_csv(\"./splits/edos_val.csv\")\n",
    "X_train, y_train = edos_train[\"clean_text\"], edos_train[\"label\"]\n",
    "X_val, y_val     = edos_val[\"clean_text\"],   edos_val[\"label\"]\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "classifiers = {\n",
    "    \"edos_svc\": LinearSVC(),\n",
    "    \"edos_lr\":  LogisticRegression(max_iter=1000),\n",
    "    \"edos_rf\":  RandomForestClassifier(n_estimators=100),\n",
    "    \"edos_xgb\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    print(f\"\\n--- {name.upper()} ---\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "    joblib.dump(pipe, f\"./models/{name}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "52c11d12-4aa6-48ba-9f11-def90d9a77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: TF-IDF + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8410    0.8382    0.8396      1514\n",
      "           1     0.5010    0.5062    0.5036       486\n",
      "\n",
      "    accuracy                         0.7575      2000\n",
      "   macro avg     0.6710    0.6722    0.6716      2000\n",
      "weighted avg     0.7583    0.7575    0.7579      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/edos_mlp.joblib']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd, joblib, os\n",
    "\n",
    "edos_train = pd.read_csv(\"./splits/edos_train.csv\")\n",
    "edos_val   = pd.read_csv(\"./splits/edos_val.csv\")\n",
    "X_train, y_train = edos_train[\"clean_text\"], edos_train[\"label\"]\n",
    "X_val, y_val     = edos_val[\"clean_text\"],   edos_val[\"label\"]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "    (\"mlp\",   MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(\"\\n=== EDOS: TF-IDF + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "joblib.dump(pipeline, \"./models/edos_mlp.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a155ef-0867-4c81-a4ba-a6043ea65101",
   "metadata": {},
   "source": [
    "## Davidson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7cce102a-09db-48e4-9a48-76328d41b5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DAVIDSON_SVC ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6964    0.2727    0.3920       143\n",
      "           1     0.9123    0.9599    0.9355      1919\n",
      "           2     0.8189    0.7933    0.8059       416\n",
      "\n",
      "    accuracy                         0.8923      2478\n",
      "   macro avg     0.8092    0.6753    0.7111      2478\n",
      "weighted avg     0.8842    0.8923    0.8824      2478\n",
      "\n",
      "\n",
      "--- DAVIDSON_LR ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8065    0.1748    0.2874       143\n",
      "           1     0.9041    0.9729    0.9372      1919\n",
      "           2     0.8508    0.7812    0.8145       416\n",
      "\n",
      "    accuracy                         0.8947      2478\n",
      "   macro avg     0.8538    0.6430    0.6797      2478\n",
      "weighted avg     0.8895    0.8947    0.8791      2478\n",
      "\n",
      "\n",
      "--- DAVIDSON_RF ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.0839    0.1519       143\n",
      "           1     0.8749    0.9807    0.9248      1919\n",
      "           2     0.8654    0.6490    0.7418       416\n",
      "\n",
      "    accuracy                         0.8733      2478\n",
      "   macro avg     0.8468    0.5712    0.6062      2478\n",
      "weighted avg     0.8690    0.8733    0.8495      2478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:37:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DAVIDSON_XGB ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5500    0.2308    0.3251       143\n",
      "           1     0.9350    0.9437    0.9393      1919\n",
      "           2     0.7879    0.9111    0.8450       416\n",
      "\n",
      "    accuracy                         0.8971      2478\n",
      "   macro avg     0.7576    0.6952    0.7032      2478\n",
      "weighted avg     0.8881    0.8971    0.8880      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "davidson_train = pd.read_csv(\"./splits/davidson_train.csv\")\n",
    "davidson_val   = pd.read_csv(\"./splits/davidson_val.csv\")\n",
    "X_train, y_train = davidson_train[\"clean_text\"], davidson_train[\"label\"]\n",
    "X_val, y_val     = davidson_val[\"clean_text\"],   davidson_val[\"label\"]\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "classifiers = {\n",
    "    \"davidson_svc\": LinearSVC(),\n",
    "    \"davidson_lr\":  LogisticRegression(max_iter=1000),\n",
    "    \"davidson_rf\":  RandomForestClassifier(n_estimators=100),\n",
    "    \"davidson_xgb\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    print(f\"\\n--- {name.upper()} ---\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "    joblib.dump(pipe, f\"./models/{name}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a6231b9a-1744-4547-a0ba-6ddb9cf4a09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON: TF-IDF + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4068    0.3357    0.3678       143\n",
      "           1     0.9119    0.9281    0.9199      1919\n",
      "           2     0.7862    0.7692    0.7776       416\n",
      "\n",
      "    accuracy                         0.8672      2478\n",
      "   macro avg     0.7017    0.6777    0.6885      2478\n",
      "weighted avg     0.8617    0.8672    0.8642      2478\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/davidson_mlp.joblib']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davidson_train = pd.read_csv(\"./splits/davidson_train.csv\")\n",
    "davidson_val   = pd.read_csv(\"./splits/davidson_val.csv\")\n",
    "X_train, y_train = davidson_train[\"clean_text\"], davidson_train[\"label\"]\n",
    "X_val, y_val     = davidson_val[\"clean_text\"],   davidson_val[\"label\"]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "    (\"mlp\",   MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(\"\\n=== DAVIDSON: TF-IDF + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "joblib.dump(pipeline, \"./models/davidson_mlp.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc414c6-0314-47ec-9c96-1d011d1ca5f8",
   "metadata": {},
   "source": [
    "## HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9767decd-18a6-45bb-b958-2f69a9d88c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HATEXPLAIN_SVC ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6039    0.6797    0.6395       821\n",
      "           1     0.4432    0.3933    0.4167       595\n",
      "           2     0.6341    0.5960    0.6145       599\n",
      "\n",
      "    accuracy                         0.5702      2015\n",
      "   macro avg     0.5604    0.5563    0.5569      2015\n",
      "weighted avg     0.5654    0.5702    0.5663      2015\n",
      "\n",
      "\n",
      "--- HATEXPLAIN_LR ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6034    0.7357    0.6630       821\n",
      "           1     0.4905    0.3899    0.4345       595\n",
      "           2     0.6876    0.6210    0.6526       599\n",
      "\n",
      "    accuracy                         0.5995      2015\n",
      "   macro avg     0.5938    0.5822    0.5834      2015\n",
      "weighted avg     0.5951    0.5995    0.5924      2015\n",
      "\n",
      "\n",
      "--- HATEXPLAIN_RF ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5682    0.8319    0.6752       821\n",
      "           1     0.5617    0.2908    0.3832       595\n",
      "           2     0.6970    0.5876    0.6377       599\n",
      "\n",
      "    accuracy                         0.5995      2015\n",
      "   macro avg     0.6090    0.5701    0.5654      2015\n",
      "weighted avg     0.6046    0.5995    0.5778      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:39:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HATEXPLAIN_XGB ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6013    0.7881    0.6821       821\n",
      "           1     0.5034    0.3731    0.4286       595\n",
      "           2     0.7129    0.5927    0.6472       599\n",
      "\n",
      "    accuracy                         0.6074      2015\n",
      "   macro avg     0.6059    0.5846    0.5860      2015\n",
      "weighted avg     0.6056    0.6074    0.5969      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hatexplain_train = pd.read_csv(\"./splits/hatexplain_train.csv\")\n",
    "hatexplain_val   = pd.read_csv(\"./splits/hatexplain_val.csv\")\n",
    "X_train, y_train = hatexplain_train[\"clean_text\"], hatexplain_train[\"label\"]\n",
    "X_val, y_val     = hatexplain_val[\"clean_text\"],   hatexplain_val[\"label\"]\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "classifiers = {\n",
    "    \"hatexplain_svc\": LinearSVC(),\n",
    "    \"hatexplain_lr\":  LogisticRegression(max_iter=1000),\n",
    "    \"hatexplain_rf\":  RandomForestClassifier(n_estimators=100),\n",
    "    \"hatexplain_xgb\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_val)\n",
    "    print(f\"\\n--- {name.upper()} ---\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "    joblib.dump(pipe, f\"./models/{name}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c4c69a3b-4866-484e-92bd-bf7f4c9f7b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN: TF-IDF + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5653    0.5570    0.5611      2463\n",
      "           1     0.3754    0.3933    0.3841      1785\n",
      "           2     0.5429    0.5281    0.5354      1797\n",
      "\n",
      "    accuracy                         0.5001      6045\n",
      "   macro avg     0.4945    0.4928    0.4936      6045\n",
      "weighted avg     0.5026    0.5001    0.5012      6045\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/hatexplain_mlp.joblib']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hatexplain_train = pd.read_csv(\"./splits/hatexplain_train.csv\")\n",
    "hatexplain_val   = pd.read_csv(\"./splits/hatexplain_val.csv\")\n",
    "X_train, y_train = hatexplain_train[\"clean_text\"], hatexplain_train[\"label\"]\n",
    "X_val, y_val     = hatexplain_val[\"clean_text\"],   hatexplain_val[\"label\"]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "    (\"mlp\",   MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN: TF-IDF + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "joblib.dump(pipeline, \"./models/hatexplain_mlp.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23853475-4e8a-4507-817e-2bf15d6d0f12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Section B: Shallow (Static) Embeddings: GloVe + Neural Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328045f-2fbd-4415-a7b2-ba7c2d20d9cd",
   "metadata": {},
   "source": [
    "## EDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cfbf672d-b9c0-48bc-b1c6-cbb21e24ea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537us/step - accuracy: 0.7505 - loss: 0.5466 - val_accuracy: 0.7615 - val_loss: 0.5095\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411us/step - accuracy: 0.7538 - loss: 0.5173 - val_accuracy: 0.7605 - val_loss: 0.4971\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403us/step - accuracy: 0.7681 - loss: 0.4914 - val_accuracy: 0.7655 - val_loss: 0.4917\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399us/step - accuracy: 0.7602 - loss: 0.4968 - val_accuracy: 0.7735 - val_loss: 0.4887\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 403us/step - accuracy: 0.7663 - loss: 0.4890 - val_accuracy: 0.7675 - val_loss: 0.4854\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421us/step - accuracy: 0.7712 - loss: 0.4847 - val_accuracy: 0.7640 - val_loss: 0.4858\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433us/step - accuracy: 0.7725 - loss: 0.4778 - val_accuracy: 0.7775 - val_loss: 0.4782\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423us/step - accuracy: 0.7760 - loss: 0.4738 - val_accuracy: 0.7780 - val_loss: 0.4779\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step - accuracy: 0.7738 - loss: 0.4708 - val_accuracy: 0.7785 - val_loss: 0.4764\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412us/step - accuracy: 0.7737 - loss: 0.4710 - val_accuracy: 0.7760 - val_loss: 0.4848\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: GloVe-Avg + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.97      0.87      1514\n",
      "           1       0.68      0.17      0.27       486\n",
      "\n",
      "    accuracy                           0.78      2000\n",
      "   macro avg       0.73      0.57      0.57      2000\n",
      "weighted avg       0.76      0.78      0.72      2000\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7532 - loss: 0.5534 - val_accuracy: 0.7890 - val_loss: 0.4580\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8016 - loss: 0.4448 - val_accuracy: 0.7740 - val_loss: 0.4742\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8307 - loss: 0.3861 - val_accuracy: 0.8130 - val_loss: 0.4229\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8532 - loss: 0.3359 - val_accuracy: 0.8180 - val_loss: 0.4853\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8733 - loss: 0.2942 - val_accuracy: 0.8170 - val_loss: 0.4835\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8952 - loss: 0.2455 - val_accuracy: 0.8180 - val_loss: 0.5359\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "=== EDOS: GloVe-Emb + CNN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88      1514\n",
      "           1       0.66      0.49      0.56       486\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.75      0.70      0.72      2000\n",
      "weighted avg       0.80      0.81      0.80      2000\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 29ms/step - accuracy: 0.7525 - loss: 0.5556 - val_accuracy: 0.7610 - val_loss: 0.4931\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 32ms/step - accuracy: 0.7908 - loss: 0.4636 - val_accuracy: 0.8210 - val_loss: 0.4211\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 32ms/step - accuracy: 0.8248 - loss: 0.4158 - val_accuracy: 0.8235 - val_loss: 0.4069\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 33ms/step - accuracy: 0.8402 - loss: 0.3859 - val_accuracy: 0.8290 - val_loss: 0.4022\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 33ms/step - accuracy: 0.8351 - loss: 0.3893 - val_accuracy: 0.8265 - val_loss: 0.4070\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 33ms/step - accuracy: 0.8491 - loss: 0.3670 - val_accuracy: 0.8245 - val_loss: 0.4139\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 34ms/step - accuracy: 0.8508 - loss: 0.3571 - val_accuracy: 0.8310 - val_loss: 0.4093\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: GloVe-Emb + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89      1514\n",
      "           1       0.71      0.50      0.58       486\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.78      0.72      0.74      2000\n",
      "weighted avg       0.82      0.83      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "MAX_LEN = 100\n",
    "MAX_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "GLOVE_PATH = \"/Users/sandyajaleshkumar/Desktop/Practicum/glove.6B/glove.6B.100d.txt\"\n",
    "MODEL_DIR = \"./models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/edos_train.csv\")\n",
    "val_df = pd.read_csv(\"./splits/edos_val.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train = train_df[\"label\"].astype(int).values\n",
    "y_val   = val_df[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_val   = tokenizer.texts_to_sequences(texts_val)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=MAX_LEN)\n",
    "X_val   = pad_sequences(sequences_val,   maxlen=MAX_LEN)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word, coefs = values[0], np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "# GloVe-Average + MLP\n",
    "X_train_avg = np.array([\n",
    "    np.mean(embedding_matrix[x[x > 0]], axis=0) if len(x[x > 0]) > 0 else np.zeros(EMBEDDING_DIM)\n",
    "    for x in X_train\n",
    "])\n",
    "X_val_avg = np.array([\n",
    "    np.mean(embedding_matrix[x[x > 0]], axis=0) if len(x[x > 0]) > 0 else np.zeros(EMBEDDING_DIM)\n",
    "    for x in X_val\n",
    "])\n",
    "\n",
    "mlp = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(EMBEDDING_DIM,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "mlp.fit(X_train_avg, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val_avg, y_val), callbacks=[early_stop])\n",
    "y_pred = (mlp.predict(X_val_avg) > 0.5).astype(int)\n",
    "print(\"\\n=== EDOS: GloVe-Avg + MLP ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "mlp.save(os.path.join(MODEL_DIR, \"edos_glove_mlp.h5\"))\n",
    "\n",
    "# GloVe-Emb + CNN\n",
    "cnn = Sequential([\n",
    "    Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_LEN, trainable=False),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val), callbacks=[early_stop])\n",
    "y_pred = (cnn.predict(X_val) > 0.5).astype(int)\n",
    "print(\"\\n=== EDOS: GloVe-Emb + CNN ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "cnn.save(os.path.join(MODEL_DIR, \"edos_glove_cnn.h5\"))\n",
    "\n",
    "# GloVe-Emb + BiLSTM \n",
    "bilstm = Sequential([\n",
    "    Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_LEN, trainable=False),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "bilstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bilstm.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "           validation_data=(X_val, y_val), callbacks=[early_stop])\n",
    "y_pred = (bilstm.predict(X_val) > 0.5).astype(int)\n",
    "print(\"\\n=== EDOS: GloVe-Emb + BiLSTM ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "bilstm.save(os.path.join(MODEL_DIR, \"edos_glove_bilstm.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ee082-c53a-4734-8ec5-0ba31cf3c5fe",
   "metadata": {},
   "source": [
    "## DAVIDSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "30c9a1da-b42d-435b-9b6d-5024ff72baa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551us/step - accuracy: 0.7892 - loss: 0.5729 - val_accuracy: 0.8442 - val_loss: 0.4345\n",
      "Epoch 2/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427us/step - accuracy: 0.8336 - loss: 0.4465 - val_accuracy: 0.8483 - val_loss: 0.4133\n",
      "Epoch 3/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431us/step - accuracy: 0.8534 - loss: 0.4044 - val_accuracy: 0.8483 - val_loss: 0.4044\n",
      "Epoch 4/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432us/step - accuracy: 0.8536 - loss: 0.3993 - val_accuracy: 0.8483 - val_loss: 0.4021\n",
      "Epoch 5/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431us/step - accuracy: 0.8544 - loss: 0.3852 - val_accuracy: 0.8584 - val_loss: 0.3976\n",
      "Epoch 6/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432us/step - accuracy: 0.8577 - loss: 0.3851 - val_accuracy: 0.8575 - val_loss: 0.3934\n",
      "Epoch 7/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418us/step - accuracy: 0.8637 - loss: 0.3764 - val_accuracy: 0.8458 - val_loss: 0.4002\n",
      "Epoch 8/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422us/step - accuracy: 0.8638 - loss: 0.3703 - val_accuracy: 0.8555 - val_loss: 0.3873\n",
      "Epoch 9/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436us/step - accuracy: 0.8634 - loss: 0.3728 - val_accuracy: 0.8515 - val_loss: 0.3904\n",
      "Epoch 10/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434us/step - accuracy: 0.8613 - loss: 0.3736 - val_accuracy: 0.8575 - val_loss: 0.3812\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON: GloVe-Avg + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.10      0.18       143\n",
      "           1       0.88      0.96      0.92      1919\n",
      "           2       0.75      0.66      0.70       416\n",
      "\n",
      "    accuracy                           0.86      2478\n",
      "   macro avg       0.75      0.57      0.60      2478\n",
      "weighted avg       0.84      0.86      0.84      2478\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8176 - loss: 0.5289 - val_accuracy: 0.8769 - val_loss: 0.3334\n",
      "Epoch 2/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8766 - loss: 0.3256 - val_accuracy: 0.8781 - val_loss: 0.3208\n",
      "Epoch 3/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8950 - loss: 0.2746 - val_accuracy: 0.8834 - val_loss: 0.3181\n",
      "Epoch 4/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9115 - loss: 0.2331 - val_accuracy: 0.8785 - val_loss: 0.3305\n",
      "Epoch 5/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9224 - loss: 0.1983 - val_accuracy: 0.8810 - val_loss: 0.3908\n",
      "Epoch 6/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9324 - loss: 0.1703 - val_accuracy: 0.8697 - val_loss: 0.4142\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON: GloVe-Emb + CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.08      0.14       143\n",
      "           1       0.90      0.97      0.93      1919\n",
      "           2       0.82      0.78      0.80       416\n",
      "\n",
      "    accuracy                           0.88      2478\n",
      "   macro avg       0.77      0.61      0.62      2478\n",
      "weighted avg       0.87      0.88      0.86      2478\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.8034 - loss: 0.5315 - val_accuracy: 0.8652 - val_loss: 0.3634\n",
      "Epoch 2/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 35ms/step - accuracy: 0.8738 - loss: 0.3384 - val_accuracy: 0.8765 - val_loss: 0.3250\n",
      "Epoch 3/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.8851 - loss: 0.3122 - val_accuracy: 0.8777 - val_loss: 0.3164\n",
      "Epoch 4/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.8928 - loss: 0.2912 - val_accuracy: 0.8814 - val_loss: 0.3157\n",
      "Epoch 5/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.8967 - loss: 0.2774 - val_accuracy: 0.8842 - val_loss: 0.3037\n",
      "Epoch 6/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.9021 - loss: 0.2618 - val_accuracy: 0.8838 - val_loss: 0.3058\n",
      "Epoch 7/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.9051 - loss: 0.2650 - val_accuracy: 0.8818 - val_loss: 0.3148\n",
      "Epoch 8/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - accuracy: 0.9065 - loss: 0.2477 - val_accuracy: 0.8878 - val_loss: 0.3145\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON: GloVe-Emb + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.21      0.31       143\n",
      "           1       0.92      0.95      0.93      1919\n",
      "           2       0.77      0.83      0.80       416\n",
      "\n",
      "    accuracy                           0.88      2478\n",
      "   macro avg       0.77      0.66      0.68      2478\n",
      "weighted avg       0.87      0.88      0.87      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "MAX_LEN = 100\n",
    "MAX_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "GLOVE_PATH = \"/Users/sandyajaleshkumar/Desktop/Practicum/glove.6B/glove.6B.100d.txt\"\n",
    "MODEL_DIR = \"./models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/davidson_train.csv\")\n",
    "val_df   = pd.read_csv(\"./splits/davidson_val.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train = train_df[\"label\"].astype(int).values\n",
    "y_val   = val_df[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(texts_train), maxlen=MAX_LEN)\n",
    "X_val   = pad_sequences(tokenizer.texts_to_sequences(texts_val),   maxlen=MAX_LEN)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "y_val_cat   = to_categorical(y_val, num_classes=3)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word, coefs = values[0], np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "# GloVe-Average + MLP\n",
    "X_train_avg = np.array([\n",
    "    np.mean(embedding_matrix[x[x > 0]], axis=0) if len(x[x > 0]) > 0 else np.zeros(EMBEDDING_DIM)\n",
    "    for x in X_train\n",
    "])\n",
    "X_val_avg = np.array([\n",
    "    np.mean(embedding_matrix[x[x > 0]], axis=0) if len(x[x > 0]) > 0 else np.zeros(EMBEDDING_DIM)\n",
    "    for x in X_val\n",
    "])\n",
    "\n",
    "mlp = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(EMBEDDING_DIM,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "mlp.fit(X_train_avg, y_train_cat, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val_avg, y_val_cat), callbacks=[early_stop])\n",
    "\n",
    "y_pred = np.argmax(mlp.predict(X_val_avg), axis=1)\n",
    "print(\"\\n=== DAVIDSON: GloVe-Avg + MLP ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "mlp.save(os.path.join(MODEL_DIR, \"davidson_glove_mlp.h5\"))\n",
    "\n",
    "# GloVe-Emb + CNN\n",
    "cnn = Sequential([\n",
    "    Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_LEN, trainable=False),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn.fit(X_train, y_train_cat, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val_cat), callbacks=[early_stop])\n",
    "y_pred = np.argmax(cnn.predict(X_val), axis=1)\n",
    "print(\"\\n=== DAVIDSON: GloVe-Emb + CNN ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "cnn.save(os.path.join(MODEL_DIR, \"davidson_glove_cnn.h5\"))\n",
    "\n",
    "# GloVe-Emb + BiLSTM \n",
    "bilstm = Sequential([\n",
    "    Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_LEN, trainable=False),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "bilstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bilstm.fit(X_train, y_train_cat, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "           validation_data=(X_val, y_val_cat), callbacks=[early_stop])\n",
    "y_pred = np.argmax(bilstm.predict(X_val), axis=1)\n",
    "print(\"\\n=== DAVIDSON: GloVe-Emb + BiLSTM ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "bilstm.save(os.path.join(MODEL_DIR, \"davidson_glove_bilstm.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db547b11-0098-4226-b16c-3e610d7b6461",
   "metadata": {},
   "source": [
    "## HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "41b44bd3-a09e-4984-a143-6702f4af7f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 567us/step - accuracy: 0.4408 - loss: 1.0601 - val_accuracy: 0.4928 - val_loss: 1.0018\n",
      "Epoch 2/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420us/step - accuracy: 0.5152 - loss: 0.9880 - val_accuracy: 0.5092 - val_loss: 0.9900\n",
      "Epoch 3/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443us/step - accuracy: 0.5305 - loss: 0.9716 - val_accuracy: 0.5112 - val_loss: 0.9772\n",
      "Epoch 4/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 444us/step - accuracy: 0.5381 - loss: 0.9623 - val_accuracy: 0.5042 - val_loss: 0.9859\n",
      "Epoch 5/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - accuracy: 0.5418 - loss: 0.9507 - val_accuracy: 0.5127 - val_loss: 0.9719\n",
      "Epoch 6/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453us/step - accuracy: 0.5489 - loss: 0.9442 - val_accuracy: 0.5246 - val_loss: 0.9760\n",
      "Epoch 7/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447us/step - accuracy: 0.5416 - loss: 0.9529 - val_accuracy: 0.5156 - val_loss: 0.9721\n",
      "Epoch 8/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433us/step - accuracy: 0.5526 - loss: 0.9407 - val_accuracy: 0.5246 - val_loss: 0.9648\n",
      "Epoch 9/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440us/step - accuracy: 0.5557 - loss: 0.9324 - val_accuracy: 0.5206 - val_loss: 0.9680\n",
      "Epoch 10/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step - accuracy: 0.5592 - loss: 0.9272 - val_accuracy: 0.5370 - val_loss: 0.9589\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN: GloVe-Avg + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.70      0.63       821\n",
      "           1       0.43      0.33      0.37       595\n",
      "           2       0.56      0.53      0.55       599\n",
      "\n",
      "    accuracy                           0.54      2015\n",
      "   macro avg       0.52      0.52      0.51      2015\n",
      "weighted avg       0.53      0.54      0.53      2015\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.4500 - loss: 1.0659 - val_accuracy: 0.5395 - val_loss: 0.9498\n",
      "Epoch 2/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5645 - loss: 0.9220 - val_accuracy: 0.5782 - val_loss: 0.9105\n",
      "Epoch 3/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6047 - loss: 0.8613 - val_accuracy: 0.5846 - val_loss: 0.8960\n",
      "Epoch 4/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6357 - loss: 0.8110 - val_accuracy: 0.5916 - val_loss: 0.8996\n",
      "Epoch 5/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6712 - loss: 0.7458 - val_accuracy: 0.5831 - val_loss: 0.9184\n",
      "Epoch 6/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7008 - loss: 0.6835 - val_accuracy: 0.5821 - val_loss: 0.9553\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN: GloVe-Emb + CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.82      0.67       821\n",
      "           1       0.56      0.24      0.33       595\n",
      "           2       0.63      0.61      0.62       599\n",
      "\n",
      "    accuracy                           0.58      2015\n",
      "   macro avg       0.59      0.55      0.54      2015\n",
      "weighted avg       0.58      0.58      0.56      2015\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.4522 - loss: 1.0493 - val_accuracy: 0.5489 - val_loss: 0.9419\n",
      "Epoch 2/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.5578 - loss: 0.9233 - val_accuracy: 0.5747 - val_loss: 0.9115\n",
      "Epoch 3/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 35ms/step - accuracy: 0.5916 - loss: 0.8949 - val_accuracy: 0.5846 - val_loss: 0.8944\n",
      "Epoch 4/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 35ms/step - accuracy: 0.6138 - loss: 0.8602 - val_accuracy: 0.5931 - val_loss: 0.8841\n",
      "Epoch 5/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 35ms/step - accuracy: 0.6226 - loss: 0.8432 - val_accuracy: 0.5975 - val_loss: 0.8869\n",
      "Epoch 6/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.6181 - loss: 0.8411 - val_accuracy: 0.6040 - val_loss: 0.8705\n",
      "Epoch 7/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.6308 - loss: 0.8179 - val_accuracy: 0.6005 - val_loss: 0.8795\n",
      "Epoch 8/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 38ms/step - accuracy: 0.6546 - loss: 0.7888 - val_accuracy: 0.6069 - val_loss: 0.8640\n",
      "Epoch 9/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 35ms/step - accuracy: 0.6657 - loss: 0.7688 - val_accuracy: 0.6040 - val_loss: 0.8826\n",
      "Epoch 10/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 35ms/step - accuracy: 0.6686 - loss: 0.7525 - val_accuracy: 0.6134 - val_loss: 0.8686\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN: GloVe-Emb + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68       821\n",
      "           1       0.49      0.37      0.42       595\n",
      "           2       0.68      0.64      0.66       599\n",
      "\n",
      "    accuracy                           0.61      2015\n",
      "   macro avg       0.60      0.59      0.59      2015\n",
      "weighted avg       0.60      0.61      0.60      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "MAX_LEN = 100\n",
    "MAX_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "GLOVE_PATH = \"/Users/sandyajaleshkumar/Desktop/Practicum/glove.6B/glove.6B.100d.txt\"\n",
    "MODEL_DIR = \"./models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/hatexplain_train.csv\")\n",
    "val_df   = pd.read_csv(\"./splits/hatexplain_val.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train = train_df[\"label\"].astype(int).values\n",
    "y_val   = val_df[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(texts_train), maxlen=MAX_LEN)\n",
    "X_val   = pad_sequences(tokenizer.texts_to_sequences(texts_val), maxlen=MAX_LEN)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=3)\n",
    "y_val_cat   = to_categorical(y_val, num_classes=3)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word, coefs = values[0], np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "# GloVe-Average + MLP \n",
    "X_train_avg = np.array([\n",
    "    np.mean(embedding_matrix[x[x > 0]], axis=0) if len(x[x > 0]) > 0 else np.zeros(EMBEDDING_DIM)\n",
    "    for x in X_train\n",
    "])\n",
    "X_val_avg = np.array([\n",
    "    np.mean(embedding_matrix[x[x > 0]], axis=0) if len(x[x > 0]) > 0 else np.zeros(EMBEDDING_DIM)\n",
    "    for x in X_val\n",
    "])\n",
    "\n",
    "mlp = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(EMBEDDING_DIM,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "mlp.fit(X_train_avg, y_train_cat, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val_avg, y_val_cat), callbacks=[early_stop])\n",
    "\n",
    "y_pred = np.argmax(mlp.predict(X_val_avg), axis=1)\n",
    "print(\"\\n=== HATEXPLAIN: GloVe-Avg + MLP ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "mlp.save(os.path.join(MODEL_DIR, \"hatexplain_glove_mlp.h5\"))\n",
    "\n",
    "# GloVe-Emb + CNN \n",
    "cnn = Sequential([\n",
    "    Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_LEN, trainable=False),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn.fit(X_train, y_train_cat, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val_cat), callbacks=[early_stop])\n",
    "y_pred = np.argmax(cnn.predict(X_val), axis=1)\n",
    "print(\"\\n=== HATEXPLAIN: GloVe-Emb + CNN ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "cnn.save(os.path.join(MODEL_DIR, \"hatexplain_glove_cnn.h5\"))\n",
    "\n",
    "# GloVe-Emb + BiLSTM \n",
    "bilstm = Sequential([\n",
    "    Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "              input_length=MAX_LEN, trainable=False),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "bilstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bilstm.fit(X_train, y_train_cat, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "           validation_data=(X_val, y_val_cat), callbacks=[early_stop])\n",
    "y_pred = np.argmax(bilstm.predict(X_val), axis=1)\n",
    "print(\"\\n=== HATEXPLAIN: GloVe-Emb + BiLSTM ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "bilstm.save(os.path.join(MODEL_DIR, \"hatexplain_glove_bilstm.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e9a39-d12a-45ce-a0e0-668e624317dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Section C: Contextual Embeddings: Transformers + Neural Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6fa64-13fa-46af-8d3d-8a8a4c76b7e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55e783b-e4ad-4acf-9c24-63745b276950",
   "metadata": {},
   "source": [
    "### BERT-Embed + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a23fd04c-f7a1-43e6-a575-23964c7ce42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings: 100%|█████████| 16000/16000 [15:58<00:00, 16.68it/s]\n",
      "Extracting BERT embeddings: 100%|███████████| 2000/2000 [02:26<00:00, 13.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP with manual early stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Accuracy: 0.8000\n",
      "Model improved and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Accuracy: 0.8050\n",
      "Model improved and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Accuracy: 0.8045\n",
      "Epoch 4 - Val Accuracy: 0.8040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val Accuracy: 0.8095\n",
      "Model improved and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val Accuracy: 0.8090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val Accuracy: 0.8110\n",
      "Model improved and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val Accuracy: 0.8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val Accuracy: 0.8115\n",
      "Model improved and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val Accuracy: 0.8145\n",
      "Model improved and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Val Accuracy: 0.8115\n",
      "Epoch 12 - Val Accuracy: 0.8140\n",
      "Epoch 13 - Val Accuracy: 0.8110\n",
      "Early stopping triggered.\n",
      "\n",
      "=== EDOS: BERT-Embed + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8483    0.9194    0.8824      1514\n",
      "           1     0.6602    0.4877    0.5609       486\n",
      "\n",
      "    accuracy                         0.8145      2000\n",
      "   macro avg     0.7542    0.7035    0.7217      2000\n",
      "weighted avg     0.8026    0.8145    0.8043      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "EMBEDDING_DIM = 768\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(f\"{DATA_PATH}/edos_train.csv\")\n",
    "val_df   = pd.read_csv(f\"{DATA_PATH}/edos_val.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).tolist()\n",
    "y_val       = val_df[\"label\"].astype(int).tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            mean_vec = last_hidden.mean(dim=1).squeeze().cpu().numpy()\n",
    "            all_embeddings.append(mean_vec)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "X_train = get_bert_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_bert_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    max_iter=1,\n",
    "    warm_start=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_score = 0\n",
    "patience = 3\n",
    "wait = 0\n",
    "\n",
    "print(\"\\nTraining MLP with manual early stopping...\")\n",
    "\n",
    "for epoch in range(50):  \n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_val, y_val)\n",
    "    print(f\"Epoch {epoch+1} - Val Accuracy: {score:.4f}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        wait = 0\n",
    "        joblib.dump(clf, os.path.join(SAVE_PATH, \"edos_bert_mlp.joblib\"))\n",
    "        print(\"Model improved and saved.\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "clf = joblib.load(os.path.join(SAVE_PATH, \"edos_bert_mlp.joblib\"))\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"\\n=== EDOS: BERT-Embed + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f4e19-4ed0-432c-bdf7-34883825cba2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BERT-Embed + BiLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c2ff46ce-5301-4776-ac5c-733315f08c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT sequence embeddings: 100%|█| 16000/16000 [20:16<00:00, 13.15it/s\n",
      "Extracting BERT sequence embeddings: 100%|██| 2000/2000 [01:36<00:00, 20.71it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 - 45s - 90ms/step - accuracy: 0.7747 - loss: 0.4805 - val_accuracy: 0.8115 - val_loss: 0.4174\n",
      "Epoch 2/20\n",
      "500/500 - 50s - 100ms/step - accuracy: 0.8141 - loss: 0.4112 - val_accuracy: 0.8245 - val_loss: 0.3860\n",
      "Epoch 3/20\n",
      "500/500 - 52s - 104ms/step - accuracy: 0.8434 - loss: 0.3609 - val_accuracy: 0.8380 - val_loss: 0.3542\n",
      "Epoch 4/20\n",
      "500/500 - 42s - 83ms/step - accuracy: 0.8651 - loss: 0.3155 - val_accuracy: 0.8360 - val_loss: 0.3592\n",
      "Epoch 5/20\n",
      "500/500 - 41s - 83ms/step - accuracy: 0.8882 - loss: 0.2726 - val_accuracy: 0.8490 - val_loss: 0.3550\n",
      "Epoch 6/20\n",
      "500/500 - 35s - 70ms/step - accuracy: 0.9078 - loss: 0.2247 - val_accuracy: 0.8285 - val_loss: 0.3990\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
      "\n",
      "=== EDOS: BERT-Embed + BiLSTM ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8597    0.9392    0.8977      1514\n",
      "           1     0.7341    0.5226    0.6106       486\n",
      "\n",
      "    accuracy                         0.8380      2000\n",
      "   macro avg     0.7969    0.7309    0.7542      2000\n",
      "weighted avg     0.8292    0.8380    0.8279      2000\n",
      "\n",
      "Model saved as: edos_bert_bilstm.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "df_train = pd.read_csv(f\"{DATA_PATH}/edos_train.csv\")\n",
    "df_val   = pd.read_csv(f\"{DATA_PATH}/edos_val.csv\")\n",
    "texts_train = df_train[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = df_val[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = df_train[\"label\"].astype(int).to_numpy()\n",
    "y_val       = df_val[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_bert_sequence_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_bert_sequence_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(64), input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "bilstm_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "bilstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "y_pred = (bilstm_model.predict(X_val) > 0.5).astype(int).reshape(-1)\n",
    "print(\"\\n=== EDOS: BERT-Embed + BiLSTM ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "bilstm_model.save(os.path.join(SAVE_PATH, \"edos_bert_bilstm.h5\"))\n",
    "print(\"Model saved as: edos_bert_bilstm.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d33bdd-c78a-46aa-b555-49d5e2cbc396",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  BERT-Embed + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e7c0154c-e33a-48fc-bde1-a388a76c1ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings: 100%|█████████| 16000/16000 [30:31<00:00,  8.74it/s]\n",
      "Extracting BERT embeddings: 100%|███████████| 2000/2000 [02:37<00:00, 12.69it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [19:42:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: BERT-Embed + XGBoost ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8195    0.9267    0.8698      1514\n",
      "           1     0.6146    0.3642    0.4574       486\n",
      "\n",
      "    accuracy                         0.7900      2000\n",
      "   macro avg     0.7170    0.6454    0.6636      2000\n",
      "weighted avg     0.7697    0.7900    0.7696      2000\n",
      "\n",
      "Model saved as: edos_bert_xgb.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "df_train = pd.read_csv(f\"{DATA_PATH}/edos_train.csv\")\n",
    "df_val   = pd.read_csv(f\"{DATA_PATH}/edos_val.csv\")\n",
    "texts_train = df_train[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = df_val[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = df_train[\"label\"].astype(int).to_numpy()\n",
    "y_val       = df_val[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                               padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            mean_vec = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            all_embeddings.append(mean_vec)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "X_train = get_bert_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_bert_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_val)\n",
    "\n",
    "print(\"\\n=== EDOS: BERT-Embed + XGBoost ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "model_name = \"edos_bert_xgb.joblib\"\n",
    "joblib.dump(xgb_model, os.path.join(SAVE_PATH, model_name))\n",
    "print(f\"Model saved as: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c1a76-2b88-4114-8c9a-1cc3eedaa839",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BERT-Embed + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8b0734da-35e1-401c-aed2-64e3beb255cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT sequence embeddings: 100%|█| 16000/16000 [23:15<00:00, 11.47it/s\n",
      "Extracting BERT sequence embeddings: 100%|██| 2000/2000 [02:14<00:00, 14.90it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 99ms/step - accuracy: 0.7364 - loss: 0.6144 - val_accuracy: 0.8255 - val_loss: 0.4049\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 101ms/step - accuracy: 0.8274 - loss: 0.3999 - val_accuracy: 0.8385 - val_loss: 0.3685\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 153ms/step - accuracy: 0.8463 - loss: 0.3673 - val_accuracy: 0.8465 - val_loss: 0.3515\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 70ms/step - accuracy: 0.8622 - loss: 0.3268 - val_accuracy: 0.8440 - val_loss: 0.3504\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - accuracy: 0.8739 - loss: 0.3016 - val_accuracy: 0.8580 - val_loss: 0.3469\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 55ms/step - accuracy: 0.8856 - loss: 0.2715 - val_accuracy: 0.8495 - val_loss: 0.3493\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 60ms/step - accuracy: 0.9064 - loss: 0.2420 - val_accuracy: 0.8450 - val_loss: 0.3833\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "\n",
      "=== EDOS: BERT-Embed + CNN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8863    0.9320    0.9086      1514\n",
      "           1     0.7475    0.6276    0.6823       486\n",
      "\n",
      "    accuracy                         0.8580      2000\n",
      "   macro avg     0.8169    0.7798    0.7954      2000\n",
      "weighted avg     0.8526    0.8580    0.8536      2000\n",
      "\n",
      "Model saved as: edos_bert_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "df_train = pd.read_csv(f\"{DATA_PATH}/edos_train.csv\")\n",
    "df_val   = pd.read_csv(f\"{DATA_PATH}/edos_val.csv\")\n",
    "texts_train = df_train[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = df_val[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = df_train[\"label\"].astype(int).to_numpy()\n",
    "y_val       = df_val[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()  # (max_len, hidden)\n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_bert_sequence_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_bert_sequence_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "y_pred = (cnn_model.predict(X_val) > 0.5).astype(int).reshape(-1)\n",
    "print(\"\\n=== EDOS: BERT-Embed + CNN ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "cnn_model.save(os.path.join(SAVE_PATH, \"edos_bert_cnn.h5\"))\n",
    "print(\"Model saved as: edos_bert_cnn.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb503bd7-af54-45d6-ad2d-558200326708",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ALBERT-Embed + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "aed9badf-cd2c-4aec-99fb-c2f9c0828264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT embeddings: 100%|███████| 16000/16000 [41:48<00:00,  6.38it/s]\n",
      "Extracting ALBERT embeddings: 100%|█████████| 2000/2000 [01:49<00:00, 18.25it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_accuracy = 0.7660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_accuracy = 0.7815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: val_accuracy = 0.7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: val_accuracy = 0.7895\n",
      "Epoch 5: val_accuracy = 0.7910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: val_accuracy = 0.7905\n",
      "Epoch 7: val_accuracy = 0.7925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: val_accuracy = 0.7895\n",
      "Epoch 9: val_accuracy = 0.7895\n",
      "Early stopping triggered.\n",
      "\n",
      "=== EDOS: ALBERT-Embed + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8085    0.9511    0.8741      1514\n",
      "           1     0.6621    0.2984    0.4113       486\n",
      "\n",
      "    accuracy                         0.7925      2000\n",
      "   macro avg     0.7353    0.6247    0.6427      2000\n",
      "weighted avg     0.7730    0.7925    0.7616      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "EMBEDDING_DIM = 768 \n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "df_train = pd.read_csv(f\"{DATA_PATH}/edos_train.csv\")\n",
    "df_val   = pd.read_csv(f\"{DATA_PATH}/edos_val.csv\")\n",
    "texts_train = df_train[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = df_val[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = df_train[\"label\"].astype(int).to_numpy()\n",
    "y_val       = df_val[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_albert_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            mean_vec = last_hidden.mean(dim=1).squeeze().cpu().numpy()\n",
    "            all_embeddings.append(mean_vec)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "X_train = get_albert_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_albert_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128,), max_iter=1, warm_start=True, random_state=42)\n",
    "best_val_score = 0\n",
    "patience = 2\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    mlp.fit(X_train, y_train)\n",
    "    score = mlp.score(X_val, y_val)\n",
    "    print(f\"Epoch {epoch}: val_accuracy = {score:.4f}\")\n",
    "\n",
    "    if score > best_val_score:\n",
    "        best_val_score = score\n",
    "        joblib.dump(mlp, os.path.join(SAVE_PATH, \"edos_albert_mlp.joblib\"))\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "clf = joblib.load(os.path.join(SAVE_PATH, \"edos_albert_mlp.joblib\"))\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"\\n=== EDOS: ALBERT-Embed + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b31df-d365-4c62-8e82-865d94f6d5c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ALBERT-Embed + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8186a7b8-f38d-46cf-bd4a-c63f46019717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT sequence embeddings: 100%|█| 16000/16000 [13:48<00:00, 19.31it\n",
      "Extracting ALBERT sequence embeddings: 100%|█| 2000/2000 [01:39<00:00, 20.01it/s\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.7507 - loss: 0.5416 - val_accuracy: 0.7850 - val_loss: 0.4546\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 83ms/step - accuracy: 0.7883 - loss: 0.4599 - val_accuracy: 0.8115 - val_loss: 0.4195\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 150ms/step - accuracy: 0.8149 - loss: 0.4210 - val_accuracy: 0.8250 - val_loss: 0.3997\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - accuracy: 0.8387 - loss: 0.3851 - val_accuracy: 0.8450 - val_loss: 0.3805\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 70ms/step - accuracy: 0.8464 - loss: 0.3671 - val_accuracy: 0.8400 - val_loss: 0.3664\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 78ms/step - accuracy: 0.8651 - loss: 0.3350 - val_accuracy: 0.8345 - val_loss: 0.3698\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 88ms/step - accuracy: 0.8693 - loss: 0.3176 - val_accuracy: 0.8345 - val_loss: 0.3777\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: ALBERT-Embed + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8636    0.9366    0.8986      1514\n",
      "           1     0.7318    0.5391    0.6209       486\n",
      "\n",
      "    accuracy                         0.8400      2000\n",
      "   macro avg     0.7977    0.7378    0.7597      2000\n",
      "weighted avg     0.8316    0.8400    0.8311      2000\n",
      "\n",
      "Model saved as: edos_albert_bilstm.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "df_train = pd.read_csv(f\"{DATA_PATH}/edos_train.csv\")\n",
    "df_val   = pd.read_csv(f\"{DATA_PATH}/edos_val.csv\")\n",
    "\n",
    "texts_train = df_train[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = df_val[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = df_train[\"label\"].astype(int).to_numpy()\n",
    "y_val       = df_val[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_albert_sequence_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_albert_sequence_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_albert_sequence_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "bilstm_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "bilstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = (bilstm_model.predict(X_val) > 0.5).astype(int).reshape(-1)\n",
    "print(\"\\n=== EDOS: ALBERT-Embed + BiLSTM ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "bilstm_model.save(os.path.join(SAVE_PATH, \"edos_albert_bilstm.h5\"))\n",
    "print(\"Model saved as: edos_albert_bilstm.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7826483-f78d-4044-a16b-f9f31409cdb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ALBERT-Embed + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c2000038-ae03-4fbf-a637-5d74d801a86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT sequence embeddings: 100%|█| 16000/16000 [15:08<00:00, 17.62it\n",
      "Extracting ALBERT sequence embeddings: 100%|█| 2000/2000 [01:40<00:00, 19.84it/s\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 59ms/step - accuracy: 0.7303 - loss: 0.8011 - val_accuracy: 0.8095 - val_loss: 0.4444\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 68ms/step - accuracy: 0.8059 - loss: 0.4501 - val_accuracy: 0.8130 - val_loss: 0.4221\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 61ms/step - accuracy: 0.8218 - loss: 0.4146 - val_accuracy: 0.8265 - val_loss: 0.3998\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 67ms/step - accuracy: 0.8386 - loss: 0.3908 - val_accuracy: 0.8330 - val_loss: 0.3929\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 75ms/step - accuracy: 0.8465 - loss: 0.3657 - val_accuracy: 0.8315 - val_loss: 0.3944\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 61ms/step - accuracy: 0.8574 - loss: 0.3462 - val_accuracy: 0.8295 - val_loss: 0.3991\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "\n",
      "=== EDOS: ALBERT-Embed + CNN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8567    0.9359    0.8946      1514\n",
      "           1     0.7197    0.5123    0.5986       486\n",
      "\n",
      "    accuracy                         0.8330      2000\n",
      "   macro avg     0.7882    0.7241    0.7466      2000\n",
      "weighted avg     0.8234    0.8330    0.8226      2000\n",
      "\n",
      "Model saved as: edos_albert_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "df_train = pd.read_csv(f\"{DATA_PATH}/edos_train.csv\")\n",
    "df_val   = pd.read_csv(f\"{DATA_PATH}/edos_val.csv\")\n",
    "\n",
    "texts_train = df_train[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = df_val[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = df_train[\"label\"].astype(int).to_numpy()\n",
    "y_val       = df_val[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_albert_sequence_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_albert_sequence_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_albert_sequence_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "# CNN Model\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = (cnn_model.predict(X_val) > 0.5).astype(int).reshape(-1)\n",
    "print(\"\\n=== EDOS: ALBERT-Embed + CNN ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "cnn_model.save(os.path.join(SAVE_PATH, \"edos_albert_cnn.h5\"))\n",
    "print(\"Model saved as: edos_albert_cnn.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e614d-2f1a-4078-ba5d-69bce414d3dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ELECTRA-Embed + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "59ca0674-eabe-4a7b-8edf-28c08dd76a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA embeddings: 100%|████| 16000/16000 [1:04:35<00:00,  4.13it/s]\n",
      "Extracting ELECTRA embeddings: 100%|████████| 2000/2000 [02:34<00:00, 12.95it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 - F1: 0.6523\n",
      "Epoch 02 - F1: 0.6523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 - F1: 0.6658\n",
      "Epoch 04 - F1: 0.6721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 - F1: 0.6779\n",
      "Epoch 06 - F1: 0.6830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 - F1: 0.6867\n",
      "Epoch 08 - F1: 0.6835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 - F1: 0.6930\n",
      "Epoch 10 - F1: 0.6970\n",
      "Epoch 11 - F1: 0.6849\n",
      "Epoch 12 - F1: 0.6958\n",
      "Early stopping triggered.\n",
      "\n",
      "=== EDOS: ELECTRA-Embed + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7732    0.9683    0.8598      1514\n",
      "           1     0.5385    0.1152    0.1898       486\n",
      "\n",
      "    accuracy                         0.7610      2000\n",
      "   macro avg     0.6558    0.5418    0.5248      2000\n",
      "weighted avg     0.7162    0.7610    0.6970      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "df_train = pd.read_csv(f\"{DATA_PATH}/edos_train.csv\")\n",
    "df_val   = pd.read_csv(f\"{DATA_PATH}/edos_val.csv\")\n",
    "\n",
    "texts_train = df_train[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = df_val[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = df_train[\"label\"].astype(int).tolist()\n",
    "y_val       = df_val[\"label\"].astype(int).tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_electra_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            pooled = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(pooled)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "X_train = get_electra_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_electra_embeddings(texts_val, tokenizer, model)\n",
    "y_train = np.array(y_train)\n",
    "y_val   = np.array(y_val)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(128,), max_iter=1, warm_start=True, random_state=42)\n",
    "best_f1 = 0\n",
    "patience = 2\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(1, 31):  \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    f1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "    print(f\"Epoch {epoch:02d} - F1: {f1:.4f}\")\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        joblib.dump(clf, os.path.join(SAVE_PATH, \"edos_electra_mlp.joblib\"))\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "clf = joblib.load(os.path.join(SAVE_PATH, \"edos_electra_mlp.joblib\"))\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"\\n=== EDOS: ELECTRA-Embed + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4142d-26da-4667-9c75-3ea197729b0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ELECTRA-Embed + BiLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3fac5c6d-5455-4afe-98c4-3ef74bd3a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA sequence embeddings: 100%|█| 16000/16000 [25:24<00:00, 10.49i\n",
      "Extracting ELECTRA sequence embeddings: 100%|█| 2000/2000 [02:48<00:00, 11.85it/\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 119ms/step - accuracy: 0.7515 - loss: 0.5538 - val_accuracy: 0.7550 - val_loss: 0.4949\n",
      "Epoch 2/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 90ms/step - accuracy: 0.7717 - loss: 0.4808 - val_accuracy: 0.7875 - val_loss: 0.4494\n",
      "Epoch 3/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 81ms/step - accuracy: 0.8018 - loss: 0.4265 - val_accuracy: 0.8080 - val_loss: 0.4254\n",
      "Epoch 4/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 83ms/step - accuracy: 0.8362 - loss: 0.3810 - val_accuracy: 0.8130 - val_loss: 0.4049\n",
      "Epoch 5/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 84ms/step - accuracy: 0.8736 - loss: 0.3124 - val_accuracy: 0.8130 - val_loss: 0.4231\n",
      "Epoch 6/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 76ms/step - accuracy: 0.8971 - loss: 0.2556 - val_accuracy: 0.8225 - val_loss: 0.4345\n",
      "Epoch 7/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - accuracy: 0.9292 - loss: 0.1948 - val_accuracy: 0.8145 - val_loss: 0.5137\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: ELECTRA-Embed + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8459    0.9207    0.8817      1514\n",
      "           1     0.6591    0.4774    0.5537       486\n",
      "\n",
      "    accuracy                         0.8130      2000\n",
      "   macro avg     0.7525    0.6991    0.7177      2000\n",
      "weighted avg     0.8005    0.8130    0.8020      2000\n",
      "\n",
      "Model saved as: edos_electra_bilstm.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"edos_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"edos_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "texts_val   = val_df[\"clean_text\"].tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).to_numpy()\n",
    "y_val       = val_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_electra_sequence_embeddings(texts, tokenizer, model, max_len):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            embeddings.append(last_hidden)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "X_train = get_electra_sequence_embeddings(texts_train, tokenizer, model, MAX_LEN)\n",
    "X_val   = get_electra_sequence_embeddings(texts_val, tokenizer, model, MAX_LEN)\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "bilstm_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "bilstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = (bilstm_model.predict(X_val) > 0.5).astype(int).reshape(-1)\n",
    "print(\"\\n=== EDOS: ELECTRA-Embed + BiLSTM ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "bilstm_model.save(os.path.join(SAVE_PATH, \"edos_electra_bilstm.h5\"))\n",
    "print(\"Model saved as: edos_electra_bilstm.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964bb96b-de6a-4668-a670-810c1152235e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ELECTRA-Embed + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ad4b54-9c38-4824-aedd-10fa213fa195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA sequence embeddings: 100%|█| 16000/16000 [43:52<00:00,  6.08i\n",
      "Extracting ELECTRA sequence embeddings: 100%|█| 2000/2000 [11:44<00:00,  2.84it/\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 46ms/step - accuracy: 0.7175 - loss: 0.6673 - val_accuracy: 0.7770 - val_loss: 0.4852\n",
      "Epoch 2/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.7830 - loss: 0.4857 - val_accuracy: 0.7860 - val_loss: 0.4671\n",
      "Epoch 3/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.8040 - loss: 0.4477 - val_accuracy: 0.7875 - val_loss: 0.4581\n",
      "Epoch 4/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.8341 - loss: 0.3799 - val_accuracy: 0.7995 - val_loss: 0.4443\n",
      "Epoch 5/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 48ms/step - accuracy: 0.8667 - loss: 0.3216 - val_accuracy: 0.7915 - val_loss: 0.4550\n",
      "Epoch 6/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.8818 - loss: 0.2777 - val_accuracy: 0.7935 - val_loss: 0.4568\n",
      "Epoch 7/20\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.9054 - loss: 0.2344 - val_accuracy: 0.8005 - val_loss: 0.4551\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "=== EDOS: ELECTRA-Embed + CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8363    0.9141    0.8735      1514\n",
      "           1     0.6232    0.4424    0.5174       486\n",
      "\n",
      "    accuracy                         0.7995      2000\n",
      "   macro avg     0.7297    0.6783    0.6955      2000\n",
      "weighted avg     0.7845    0.7995    0.7870      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as: edos_electra_cnn.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"edos_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"edos_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "texts_val   = val_df[\"clean_text\"].tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).to_numpy()\n",
    "y_val       = val_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_electra_sequence_embeddings(texts, tokenizer, model, max_len):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_electra_sequence_embeddings(texts_train, tokenizer, model, MAX_LEN)\n",
    "X_val   = get_electra_sequence_embeddings(texts_val, tokenizer, model, MAX_LEN)\n",
    "\n",
    "# CNN \n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = (cnn_model.predict(X_val) > 0.5).astype(int).reshape(-1)\n",
    "print(\"\\n=== EDOS: ELECTRA-Embed + CNN ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "cnn_model.save(os.path.join(SAVE_PATH, \"edos_electra_cnn.h5\"))\n",
    "print(\"Model saved as: edos_electra_cnn.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4a279-3ba3-4486-a06f-d2da1580a2ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DAVIDSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88a59d-bd6c-472e-ae2a-4bec060e72e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BERT-Embed + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04cbad0-cbe2-4eb5-9182-8a2aac1ffe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings: 100%|███████| 19826/19826 [1:40:28<00:00,  3.29it/s]\n",
      "Extracting BERT embeddings: 100%|███████████| 2478/2478 [03:03<00:00, 13.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON: BERT-Embed + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5758    0.1329    0.2159       143\n",
      "           1     0.8796    0.9630    0.9194      1919\n",
      "           2     0.7878    0.6514    0.7132       416\n",
      "\n",
      "    accuracy                         0.8628      2478\n",
      "   macro avg     0.7477    0.5824    0.6162      2478\n",
      "weighted avg     0.8466    0.8628    0.8442      2478\n",
      "\n",
      "Model saved as: davidson_bert_mlp.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"davidson_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"davidson_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "texts_val   = val_df[\"clean_text\"].tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).to_numpy()\n",
    "y_val       = val_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_vecs = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            mean_vec = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            all_vecs.append(mean_vec)\n",
    "    return np.array(all_vecs)\n",
    "\n",
    "X_train = get_bert_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_bert_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=5,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_val)\n",
    "print(\"\\n=== DAVIDSON: BERT-Embed + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "joblib.dump(mlp, os.path.join(SAVE_PATH, \"davidson_bert_mlp.joblib\"))\n",
    "print(\"Model saved as: davidson_bert_mlp.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc612d-c5a0-4d63-a0b2-1fca5465a47c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BERT + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f5940d7-1f27-4c0a-afcb-daaaeaa6353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT sequence embeddings: 100%|█| 19826/19826 [30:17<00:00, 10.91it/s\n",
      "Extracting BERT sequence embeddings: 100%|██| 2478/2478 [18:56<00:00,  2.18it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8294 - loss: 0.4879"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 750ms/step - accuracy: 0.8294 - loss: 0.4878 - val_accuracy: 0.8797 - val_loss: 0.3281\n",
      "Epoch 2/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8895 - loss: 0.3077"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 91ms/step - accuracy: 0.8895 - loss: 0.3077 - val_accuracy: 0.8935 - val_loss: 0.2866\n",
      "Epoch 3/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9065 - loss: 0.2543"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 98ms/step - accuracy: 0.9065 - loss: 0.2543 - val_accuracy: 0.8995 - val_loss: 0.2794\n",
      "Epoch 4/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9184 - loss: 0.2377"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 89ms/step - accuracy: 0.9184 - loss: 0.2377 - val_accuracy: 0.9003 - val_loss: 0.2634\n",
      "Epoch 5/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m962s\u001b[0m 2s/step - accuracy: 0.9274 - loss: 0.2121 - val_accuracy: 0.8947 - val_loss: 0.2841\n",
      "Epoch 6/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 85ms/step - accuracy: 0.9345 - loss: 0.1847 - val_accuracy: 0.8975 - val_loss: 0.2889\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
      "\n",
      "=== DAVIDSON: BERT-Embed + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6462    0.2937    0.4038       143\n",
      "           1     0.9198    0.9625    0.9407      1919\n",
      "           2     0.8444    0.8221    0.8331       416\n",
      "\n",
      "    accuracy                         0.9003      2478\n",
      "   macro avg     0.8035    0.6928    0.7259      2478\n",
      "weighted avg     0.8914    0.9003    0.8916      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(f\"{DATA_PATH}/davidson_train.csv\")\n",
    "val_df   = pd.read_csv(f\"{DATA_PATH}/davidson_val.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int).values, num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int).values,   num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            seq_embeds = outputs.last_hidden_state.squeeze(0).cpu().numpy()  # shape: (max_len, hidden)\n",
    "            all_seq.append(seq_embeds)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_bert_sequence_embeddings(texts_train, tokenizer, model, max_len=MAX_LEN)\n",
    "X_val   = get_bert_sequence_embeddings(texts_val, tokenizer, model, max_len=MAX_LEN)\n",
    "\n",
    "model_bilstm = Sequential([\n",
    "    Bidirectional(LSTM(64), input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "])\n",
    "model_bilstm.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"davidson_bert_bilstm_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "model_bilstm.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(model_bilstm.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== DAVIDSON: BERT-Embed + BiLSTM ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3155c2-f9de-4bd8-8256-b48d3fc302b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BERT-Embed + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64a635a-895e-4f9a-a3a0-a9698e6e0908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT sequence embeddings: 100%|█| 19826/19826 [1:28:42<00:00,  3.73it\n",
      "Extracting BERT sequence embeddings: 100%|██| 2478/2478 [09:01<00:00,  4.58it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m618/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8205 - loss: 0.5864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 139ms/step - accuracy: 0.8207 - loss: 0.5857 - val_accuracy: 0.8878 - val_loss: 0.3024\n",
      "Epoch 2/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8893 - loss: 0.3120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 62ms/step - accuracy: 0.8893 - loss: 0.3120 - val_accuracy: 0.8955 - val_loss: 0.2882\n",
      "Epoch 3/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9019 - loss: 0.2847"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 54ms/step - accuracy: 0.9019 - loss: 0.2847 - val_accuracy: 0.9015 - val_loss: 0.2777\n",
      "Epoch 4/10\n",
      "\u001b[1m619/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9089 - loss: 0.2597"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 49ms/step - accuracy: 0.9089 - loss: 0.2598 - val_accuracy: 0.9011 - val_loss: 0.2757\n",
      "Epoch 5/10\n",
      "\u001b[1m619/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9154 - loss: 0.2396"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 52ms/step - accuracy: 0.9154 - loss: 0.2397 - val_accuracy: 0.9027 - val_loss: 0.2746\n",
      "Epoch 6/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 56ms/step - accuracy: 0.9212 - loss: 0.2253 - val_accuracy: 0.9019 - val_loss: 0.2833\n",
      "Epoch 7/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 45ms/step - accuracy: 0.9270 - loss: 0.2075 - val_accuracy: 0.8983 - val_loss: 0.2835\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "=== DAVIDSON: BERT-Embed + CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6875    0.1538    0.2514       143\n",
      "           1     0.9331    0.9521    0.9425      1919\n",
      "           2     0.7951    0.9327    0.8584       416\n",
      "\n",
      "    accuracy                         0.9027      2478\n",
      "   macro avg     0.8052    0.6795    0.6841      2478\n",
      "weighted avg     0.8958    0.9027    0.8885      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(f\"{DATA_PATH}/davidson_train.csv\")\n",
    "val_df   = pd.read_csv(f\"{DATA_PATH}/davidson_val.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int).values, num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int).values,   num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_bert_sequence_embeddings(texts_train, tokenizer, model, max_len=MAX_LEN)\n",
    "X_val   = get_bert_sequence_embeddings(texts_val, tokenizer, model, max_len=MAX_LEN)\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"davidson_bert_cnn_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(cnn_model.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== DAVIDSON: BERT-Embed + CNN ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a3065-7ea2-4140-9a41-a388867f27dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### BERT-Embed + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cbe2f2a0-bcb3-4a42-abcf-029e6c737b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings: 100%|███████| 19826/19826 [7:04:14<00:00,  1.28s/it]\n",
      "Extracting BERT embeddings: 100%|███████████| 2478/2478 [53:44<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.01086\teval-mlogloss:1.01597\n",
      "[1]\ttrain-mlogloss:0.93630\teval-mlogloss:0.94746\n",
      "[2]\ttrain-mlogloss:0.87249\teval-mlogloss:0.88877\n",
      "[3]\ttrain-mlogloss:0.81707\teval-mlogloss:0.83896\n",
      "[4]\ttrain-mlogloss:0.76812\teval-mlogloss:0.79586\n",
      "[5]\ttrain-mlogloss:0.72560\teval-mlogloss:0.75786\n",
      "[6]\ttrain-mlogloss:0.68792\teval-mlogloss:0.72419\n",
      "[7]\ttrain-mlogloss:0.65402\teval-mlogloss:0.69569\n",
      "[8]\ttrain-mlogloss:0.62370\teval-mlogloss:0.66986\n",
      "[9]\ttrain-mlogloss:0.59635\teval-mlogloss:0.64679\n",
      "[10]\ttrain-mlogloss:0.57165\teval-mlogloss:0.62603\n",
      "[11]\ttrain-mlogloss:0.54939\teval-mlogloss:0.60803\n",
      "[12]\ttrain-mlogloss:0.52917\teval-mlogloss:0.59186\n",
      "[13]\ttrain-mlogloss:0.51069\teval-mlogloss:0.57671\n",
      "[14]\ttrain-mlogloss:0.49400\teval-mlogloss:0.56359\n",
      "[15]\ttrain-mlogloss:0.47833\teval-mlogloss:0.55245\n",
      "[16]\ttrain-mlogloss:0.46384\teval-mlogloss:0.54198\n",
      "[17]\ttrain-mlogloss:0.45013\teval-mlogloss:0.53210\n",
      "[18]\ttrain-mlogloss:0.43770\teval-mlogloss:0.52359\n",
      "[19]\ttrain-mlogloss:0.42593\teval-mlogloss:0.51571\n",
      "[20]\ttrain-mlogloss:0.41522\teval-mlogloss:0.50869\n",
      "[21]\ttrain-mlogloss:0.40487\teval-mlogloss:0.50198\n",
      "[22]\ttrain-mlogloss:0.39547\teval-mlogloss:0.49571\n",
      "[23]\ttrain-mlogloss:0.38634\teval-mlogloss:0.49013\n",
      "[24]\ttrain-mlogloss:0.37816\teval-mlogloss:0.48559\n",
      "[25]\ttrain-mlogloss:0.36962\teval-mlogloss:0.48082\n",
      "[26]\ttrain-mlogloss:0.36198\teval-mlogloss:0.47694\n",
      "[27]\ttrain-mlogloss:0.35436\teval-mlogloss:0.47286\n",
      "[28]\ttrain-mlogloss:0.34733\teval-mlogloss:0.46908\n",
      "[29]\ttrain-mlogloss:0.34057\teval-mlogloss:0.46612\n",
      "[30]\ttrain-mlogloss:0.33427\teval-mlogloss:0.46268\n",
      "[31]\ttrain-mlogloss:0.32792\teval-mlogloss:0.45944\n",
      "[32]\ttrain-mlogloss:0.32196\teval-mlogloss:0.45690\n",
      "[33]\ttrain-mlogloss:0.31615\teval-mlogloss:0.45423\n",
      "[34]\ttrain-mlogloss:0.31038\teval-mlogloss:0.45179\n",
      "[35]\ttrain-mlogloss:0.30536\teval-mlogloss:0.44991\n",
      "[36]\ttrain-mlogloss:0.30001\teval-mlogloss:0.44826\n",
      "[37]\ttrain-mlogloss:0.29500\teval-mlogloss:0.44633\n",
      "[38]\ttrain-mlogloss:0.28998\teval-mlogloss:0.44453\n",
      "[39]\ttrain-mlogloss:0.28550\teval-mlogloss:0.44322\n",
      "[40]\ttrain-mlogloss:0.28089\teval-mlogloss:0.44143\n",
      "[41]\ttrain-mlogloss:0.27637\teval-mlogloss:0.43995\n",
      "[42]\ttrain-mlogloss:0.27158\teval-mlogloss:0.43836\n",
      "[43]\ttrain-mlogloss:0.26753\teval-mlogloss:0.43703\n",
      "[44]\ttrain-mlogloss:0.26319\teval-mlogloss:0.43577\n",
      "[45]\ttrain-mlogloss:0.25919\teval-mlogloss:0.43419\n",
      "[46]\ttrain-mlogloss:0.25548\teval-mlogloss:0.43304\n",
      "[47]\ttrain-mlogloss:0.25142\teval-mlogloss:0.43195\n",
      "[48]\ttrain-mlogloss:0.24749\teval-mlogloss:0.43089\n",
      "[49]\ttrain-mlogloss:0.24338\teval-mlogloss:0.42981\n",
      "[50]\ttrain-mlogloss:0.23963\teval-mlogloss:0.42850\n",
      "[51]\ttrain-mlogloss:0.23600\teval-mlogloss:0.42770\n",
      "[52]\ttrain-mlogloss:0.23238\teval-mlogloss:0.42622\n",
      "[53]\ttrain-mlogloss:0.22897\teval-mlogloss:0.42554\n",
      "[54]\ttrain-mlogloss:0.22570\teval-mlogloss:0.42476\n",
      "[55]\ttrain-mlogloss:0.22235\teval-mlogloss:0.42356\n",
      "[56]\ttrain-mlogloss:0.21893\teval-mlogloss:0.42297\n",
      "[57]\ttrain-mlogloss:0.21556\teval-mlogloss:0.42216\n",
      "[58]\ttrain-mlogloss:0.21231\teval-mlogloss:0.42149\n",
      "[59]\ttrain-mlogloss:0.20957\teval-mlogloss:0.42117\n",
      "[60]\ttrain-mlogloss:0.20663\teval-mlogloss:0.42012\n",
      "[61]\ttrain-mlogloss:0.20367\teval-mlogloss:0.41948\n",
      "[62]\ttrain-mlogloss:0.20083\teval-mlogloss:0.41910\n",
      "[63]\ttrain-mlogloss:0.19763\teval-mlogloss:0.41839\n",
      "[64]\ttrain-mlogloss:0.19498\teval-mlogloss:0.41780\n",
      "[65]\ttrain-mlogloss:0.19227\teval-mlogloss:0.41737\n",
      "[66]\ttrain-mlogloss:0.18954\teval-mlogloss:0.41695\n",
      "[67]\ttrain-mlogloss:0.18693\teval-mlogloss:0.41680\n",
      "[68]\ttrain-mlogloss:0.18449\teval-mlogloss:0.41623\n",
      "[69]\ttrain-mlogloss:0.18196\teval-mlogloss:0.41527\n",
      "[70]\ttrain-mlogloss:0.17929\teval-mlogloss:0.41473\n",
      "[71]\ttrain-mlogloss:0.17682\teval-mlogloss:0.41415\n",
      "[72]\ttrain-mlogloss:0.17454\teval-mlogloss:0.41383\n",
      "[73]\ttrain-mlogloss:0.17232\teval-mlogloss:0.41324\n",
      "[74]\ttrain-mlogloss:0.16976\teval-mlogloss:0.41289\n",
      "[75]\ttrain-mlogloss:0.16752\teval-mlogloss:0.41271\n",
      "[76]\ttrain-mlogloss:0.16526\teval-mlogloss:0.41217\n",
      "[77]\ttrain-mlogloss:0.16299\teval-mlogloss:0.41185\n",
      "[78]\ttrain-mlogloss:0.16069\teval-mlogloss:0.41139\n",
      "[79]\ttrain-mlogloss:0.15898\teval-mlogloss:0.41089\n",
      "[80]\ttrain-mlogloss:0.15681\teval-mlogloss:0.41075\n",
      "[81]\ttrain-mlogloss:0.15490\teval-mlogloss:0.41018\n",
      "[82]\ttrain-mlogloss:0.15288\teval-mlogloss:0.40965\n",
      "[83]\ttrain-mlogloss:0.15103\teval-mlogloss:0.40928\n",
      "[84]\ttrain-mlogloss:0.14900\teval-mlogloss:0.40932\n",
      "[85]\ttrain-mlogloss:0.14729\teval-mlogloss:0.40901\n",
      "[86]\ttrain-mlogloss:0.14566\teval-mlogloss:0.40894\n",
      "[87]\ttrain-mlogloss:0.14390\teval-mlogloss:0.40864\n",
      "[88]\ttrain-mlogloss:0.14207\teval-mlogloss:0.40865\n",
      "[89]\ttrain-mlogloss:0.14043\teval-mlogloss:0.40826\n",
      "[90]\ttrain-mlogloss:0.13858\teval-mlogloss:0.40802\n",
      "[91]\ttrain-mlogloss:0.13718\teval-mlogloss:0.40793\n",
      "[92]\ttrain-mlogloss:0.13544\teval-mlogloss:0.40751\n",
      "[93]\ttrain-mlogloss:0.13376\teval-mlogloss:0.40734\n",
      "[94]\ttrain-mlogloss:0.13186\teval-mlogloss:0.40725\n",
      "[95]\ttrain-mlogloss:0.13018\teval-mlogloss:0.40718\n",
      "[96]\ttrain-mlogloss:0.12856\teval-mlogloss:0.40712\n",
      "[97]\ttrain-mlogloss:0.12716\teval-mlogloss:0.40697\n",
      "[98]\ttrain-mlogloss:0.12581\teval-mlogloss:0.40669\n",
      "[99]\ttrain-mlogloss:0.12446\teval-mlogloss:0.40673\n",
      "[100]\ttrain-mlogloss:0.12313\teval-mlogloss:0.40671\n",
      "[101]\ttrain-mlogloss:0.12148\teval-mlogloss:0.40723\n",
      "[102]\ttrain-mlogloss:0.12002\teval-mlogloss:0.40686\n",
      "[103]\ttrain-mlogloss:0.11842\teval-mlogloss:0.40695\n",
      "\n",
      "=== DAVIDSON: BERT-Embed + XGBoost ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6923    0.0629    0.1154       143\n",
      "           1     0.8645    0.9739    0.9160      1919\n",
      "           2     0.7954    0.5793    0.6704       416\n",
      "\n",
      "    accuracy                         0.8551      2478\n",
      "   macro avg     0.7841    0.5387    0.5672      2478\n",
      "weighted avg     0.8429    0.8551    0.8285      2478\n",
      "\n",
      "Model saved as: davidson_bert_xgb.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import DMatrix, train as xgb_train, Booster\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "EARLY_STOPPING_ROUNDS = 5\n",
    "NUM_BOOST_ROUND = 1000\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(f\"{DATA_PATH}/davidson_train.csv\")\n",
    "val_df   = pd.read_csv(f\"{DATA_PATH}/davidson_val.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).values\n",
    "y_val       = val_df[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            mean_vec = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            all_embeddings.append(mean_vec)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "X_train = get_embeddings(texts_train)\n",
    "X_val   = get_embeddings(texts_val)\n",
    "\n",
    "dtrain = DMatrix(X_train, label=y_train)\n",
    "dval   = DMatrix(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"num_class\": 3,\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "    \"verbosity\": 1\n",
    "}\n",
    "\n",
    "watchlist = [(dtrain, \"train\"), (dval, \"eval\")]\n",
    "xgb_model: Booster = xgb_train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=NUM_BOOST_ROUND,\n",
    "    evals=watchlist,\n",
    "    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n",
    "y_pred = xgb_model.predict(dval)\n",
    "print(\"\\n=== DAVIDSON: BERT-Embed + XGBoost ===\")\n",
    "print(classification_report(y_val, y_pred.astype(int), digits=4))\n",
    "\n",
    "xgb_model.save_model(os.path.join(SAVE_PATH, \"davidson_bert_xgb.json\"))\n",
    "print(\"Model saved as: davidson_bert_xgb.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0314221-5b7a-4814-af6c-7a30d9dd44b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ALBERT-Embed + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fddfb502-8a3f-45d0-8e5a-53f3b4f0aafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT embeddings: 100%|███████| 19826/19826 [28:52<00:00, 11.45it/s]\n",
      "Extracting ALBERT embeddings: 100%|█████████| 2478/2478 [02:34<00:00, 16.00it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss = 0.4810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: val_loss = 0.4600\n",
      "Epoch 3: val_loss = 0.4560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: val_loss = 0.4522\n",
      "Epoch 5: val_loss = 0.4407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: val_loss = 0.4409\n",
      "Epoch 7: val_loss = 0.4401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: val_loss = 0.4328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: val_loss = 0.4259\n",
      "Epoch 10: val_loss = 0.4271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: val_loss = 0.4234\n",
      "Epoch 12: val_loss = 0.4229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: val_loss = 0.4213\n",
      "Epoch 14: val_loss = 0.4208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: val_loss = 0.4189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: val_loss = 0.4190\n",
      "Epoch 17: val_loss = 0.4212\n",
      "Early stopping triggered.\n",
      "\n",
      "=== DAVIDSON: ALBERT-Embed + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3684    0.1469    0.2100       143\n",
      "           1     0.8877    0.9390    0.9126      1919\n",
      "           2     0.7161    0.6731    0.6939       416\n",
      "\n",
      "    accuracy                         0.8487      2478\n",
      "   macro avg     0.6574    0.5863    0.6055      2478\n",
      "weighted avg     0.8289    0.8487    0.8354      2478\n",
      "\n",
      "Model saved as: davidson_albert_mlp.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report, log_loss\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "import copy\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "PATIENCE = 2\n",
    "MAX_EPOCHS = 20\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(f\"{DATA_PATH}/davidson_train.csv\")\n",
    "val_df   = pd.read_csv(f\"{DATA_PATH}/davidson_val.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).values\n",
    "texts_val   = val_df[\"clean_text\"].tolist()\n",
    "y_val       = val_df[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_albert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            mean_vec = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(mean_vec)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "X_train = get_albert_embeddings(texts_train)\n",
    "X_val   = get_albert_embeddings(texts_val)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "best_model = None\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(128,), max_iter=1, warm_start=True, random_state=42)\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_proba = clf.predict_proba(X_val)\n",
    "    val_loss = log_loss(y_val, y_proba)\n",
    "    print(f\"Epoch {epoch}: val_loss = {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = copy.deepcopy(clf)  \n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "y_pred = best_model.predict(X_val)\n",
    "print(\"\\n=== DAVIDSON: ALBERT-Embed + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "joblib.dump(best_model, os.path.join(SAVE_PATH, \"davidson_albert_mlp.joblib\"))\n",
    "print(\"Model saved as: davidson_albert_mlp.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c6a7e-e738-4d84-a182-173dc56344f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ALBERT-Embed + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "942e4713-52fc-4a6c-bc67-6e1b9de2b9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT token embeddings: 100%|█| 19826/19826 [40:37<00:00,  8.13it/s]\n",
      "Extracting ALBERT token embeddings: 100%|███| 2478/2478 [03:58<00:00, 10.38it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7973 - loss: 0.5788"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 237ms/step - accuracy: 0.7973 - loss: 0.5786 - val_accuracy: 0.8644 - val_loss: 0.3768\n",
      "Epoch 2/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8696 - loss: 0.3706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 88ms/step - accuracy: 0.8697 - loss: 0.3706 - val_accuracy: 0.8862 - val_loss: 0.3393\n",
      "Epoch 3/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8862 - loss: 0.3308"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 94ms/step - accuracy: 0.8862 - loss: 0.3308 - val_accuracy: 0.8866 - val_loss: 0.3135\n",
      "Epoch 4/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8912 - loss: 0.3077"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 92ms/step - accuracy: 0.8912 - loss: 0.3077 - val_accuracy: 0.8862 - val_loss: 0.3060\n",
      "Epoch 5/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.9035 - loss: 0.2792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 181ms/step - accuracy: 0.9035 - loss: 0.2792 - val_accuracy: 0.8923 - val_loss: 0.2988\n",
      "Epoch 6/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 91ms/step - accuracy: 0.9127 - loss: 0.2517 - val_accuracy: 0.8902 - val_loss: 0.3248\n",
      "Epoch 7/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 243ms/step - accuracy: 0.9158 - loss: 0.2421 - val_accuracy: 0.8850 - val_loss: 0.3003\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
      "\n",
      "=== DAVIDSON: ALBERT-Embed + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4737    0.0629    0.1111       143\n",
      "           1     0.9153    0.9630    0.9385      1919\n",
      "           2     0.8045    0.8510    0.8271       416\n",
      "\n",
      "    accuracy                         0.8923      2478\n",
      "   macro avg     0.7312    0.6256    0.6256      2478\n",
      "weighted avg     0.8712    0.8923    0.8721      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(f\"{DATA_PATH}/davidson_train.csv\")\n",
    "val_df   = pd.read_csv(f\"{DATA_PATH}/davidson_val.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int), num_classes=NUM_CLASSES)\n",
    "\n",
    "texts_val = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_val     = to_categorical(val_df[\"label\"].astype(int), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Tokenizer + ALBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_albert_seq_embeddings(texts):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_albert_seq_embeddings(texts_train)\n",
    "X_val   = get_albert_seq_embeddings(texts_val)\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(64), input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "bilstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"davidson_albert_bilstm_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "bilstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(bilstm_model.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== DAVIDSON: ALBERT-Embed + BiLSTM ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca0a5e-f298-4a83-80f2-628cc2c378c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ALBERT-Embed + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0f7858b-dfdb-4631-a86a-4acf78056d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT token embeddings: 100%|█| 19826/19826 [49:22<00:00,  6.69it/s]\n",
      "Extracting ALBERT token embeddings: 100%|███| 2478/2478 [13:54<00:00,  2.97it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m619/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7859 - loss: 0.8914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.7860 - loss: 0.8904 - val_accuracy: 0.8705 - val_loss: 0.3696\n",
      "Epoch 2/10\n",
      "\u001b[1m619/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8598 - loss: 0.3827"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 72ms/step - accuracy: 0.8598 - loss: 0.3827 - val_accuracy: 0.8664 - val_loss: 0.3372\n",
      "Epoch 3/10\n",
      "\u001b[1m619/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8796 - loss: 0.3377"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 56ms/step - accuracy: 0.8795 - loss: 0.3377 - val_accuracy: 0.8862 - val_loss: 0.3220\n",
      "Epoch 4/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8844 - loss: 0.3217"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 57ms/step - accuracy: 0.8844 - loss: 0.3217 - val_accuracy: 0.8866 - val_loss: 0.3095\n",
      "Epoch 5/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8851 - loss: 0.3206 - val_accuracy: 0.8910 - val_loss: 0.3168\n",
      "Epoch 6/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 48ms/step - accuracy: 0.8922 - loss: 0.3019 - val_accuracy: 0.8882 - val_loss: 0.3130\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "=== DAVIDSON: ALBERT-Embed + CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.1259    0.2081       143\n",
      "           1     0.9090    0.9583    0.9330      1919\n",
      "           2     0.8000    0.8173    0.8086       416\n",
      "\n",
      "    accuracy                         0.8866      2478\n",
      "   macro avg     0.7697    0.6338    0.6499      2478\n",
      "weighted avg     0.8729    0.8866    0.8703      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"davidson_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"davidson_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "texts_val   = val_df[\"clean_text\"].tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int), num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int),   num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_albert_sequence_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_albert_sequence_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_albert_sequence_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"davidson_albert_cnn_best.h5\"),\n",
    "        monitor='val_loss', save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(cnn_model.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== DAVIDSON: ALBERT-Embed + CNN ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70daf225-eec1-4e4c-91be-281511a6324c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ELECTRA-Embed + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aeeb1a9d-e26c-453b-81fb-c7b915b28817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA embeddings: 100%|██████| 19826/19826 [20:54<00:00, 15.80it/s]\n",
      "Extracting ELECTRA embeddings: 100%|████████| 2478/2478 [02:43<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON: ELECTRA-Embed + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4444    0.0280    0.0526       143\n",
      "           1     0.8319    0.9776    0.8989      1919\n",
      "           2     0.7570    0.3894    0.5143       416\n",
      "\n",
      "    accuracy                         0.8241      2478\n",
      "   macro avg     0.6778    0.4650    0.4886      2478\n",
      "weighted avg     0.7970    0.8241    0.7855      2478\n",
      "\n",
      "Model saved as: davidson_electra_mlp.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"davidson_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"davidson_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "texts_val   = val_df[\"clean_text\"].tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).to_numpy()\n",
    "y_val       = val_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_electra_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            mean_vec = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            all_embeddings.append(mean_vec)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "X_train = get_electra_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_electra_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1, \n",
    "    n_iter_no_change=5,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_val)\n",
    "print(\"\\n=== DAVIDSON: ELECTRA-Embed + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "joblib.dump(mlp, os.path.join(SAVE_PATH, \"davidson_electra_mlp.joblib\"))\n",
    "print(\"Model saved as: davidson_electra_mlp.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f9272-5058-4f71-a8c5-054dc29945f4",
   "metadata": {},
   "source": [
    "### ELECTRA-Embed + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81a560e3-175b-45cd-ad36-340ee4f355df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA token embeddings: 100%|█| 19826/19826 [25:40<00:00, 12.87it/s\n",
      "Extracting ELECTRA token embeddings: 100%|██| 2478/2478 [03:30<00:00, 11.80it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7870 - loss: 0.6014"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 93ms/step - accuracy: 0.7871 - loss: 0.6013 - val_accuracy: 0.8358 - val_loss: 0.4492\n",
      "Epoch 2/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8526 - loss: 0.4224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 85ms/step - accuracy: 0.8526 - loss: 0.4224 - val_accuracy: 0.8620 - val_loss: 0.3711\n",
      "Epoch 3/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8826 - loss: 0.3330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 96ms/step - accuracy: 0.8826 - loss: 0.3330 - val_accuracy: 0.8717 - val_loss: 0.3520\n",
      "Epoch 4/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8978 - loss: 0.2828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 108ms/step - accuracy: 0.8978 - loss: 0.2828 - val_accuracy: 0.8765 - val_loss: 0.3460\n",
      "Epoch 5/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 102ms/step - accuracy: 0.9051 - loss: 0.2599 - val_accuracy: 0.8753 - val_loss: 0.3498\n",
      "Epoch 6/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 92ms/step - accuracy: 0.9185 - loss: 0.2283 - val_accuracy: 0.8753 - val_loss: 0.3783\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "\n",
      "=== DAVIDSON: ELECTRA-Embed + BiLSTM ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4203    0.2028    0.2736       143\n",
      "           1     0.9015    0.9588    0.9293      1919\n",
      "           2     0.8234    0.7284    0.7730       416\n",
      "\n",
      "    accuracy                         0.8765      2478\n",
      "   macro avg     0.7151    0.6300    0.6586      2478\n",
      "weighted avg     0.8606    0.8765    0.8652      2478\n",
      "\n",
      "Model saved as: davidson_electra_bilstm.h5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"davidson_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"davidson_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_electra_seq_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_electra_seq_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_electra_seq_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(64), input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "bilstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"davidson_electra_bilstm_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "bilstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(bilstm_model.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== DAVIDSON: ELECTRA-Embed + BiLSTM ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "bilstm_model.save(os.path.join(SAVE_PATH, \"davidson_electra_bilstm.h5\"))\n",
    "print(\"Model saved as: davidson_electra_bilstm.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4608118-915e-4f77-81ca-c6946f756424",
   "metadata": {},
   "source": [
    "### ELECTRA-Embed + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67387510-7d36-4c0b-87dc-6e3a03c78911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA token embeddings: 100%|█| 19826/19826 [21:36<00:00, 15.29it/s\n",
      "Extracting ELECTRA token embeddings: 100%|██| 2478/2478 [03:14<00:00, 12.76it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7746 - loss: 0.6867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 66ms/step - accuracy: 0.7747 - loss: 0.6864 - val_accuracy: 0.8547 - val_loss: 0.4098\n",
      "Epoch 2/10\n",
      "\u001b[1m619/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8525 - loss: 0.4101"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 54ms/step - accuracy: 0.8525 - loss: 0.4100 - val_accuracy: 0.8604 - val_loss: 0.3644\n",
      "Epoch 3/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8760 - loss: 0.3463"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 104ms/step - accuracy: 0.8760 - loss: 0.3463 - val_accuracy: 0.8773 - val_loss: 0.3389\n",
      "Epoch 4/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 85ms/step - accuracy: 0.8931 - loss: 0.2926 - val_accuracy: 0.8737 - val_loss: 0.3393\n",
      "Epoch 5/10\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 64ms/step - accuracy: 0.9006 - loss: 0.2692 - val_accuracy: 0.8741 - val_loss: 0.3434\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\n",
      "=== DAVIDSON: ELECTRA-Embed + CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5676    0.1469    0.2333       143\n",
      "           1     0.9064    0.9484    0.9269      1919\n",
      "           2     0.7691    0.8005    0.7845       416\n",
      "\n",
      "    accuracy                         0.8773      2478\n",
      "   macro avg     0.7477    0.6319    0.6482      2478\n",
      "weighted avg     0.8638    0.8773    0.8630      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"davidson_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"davidson_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "texts_val   = val_df[\"clean_text\"].tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_electra_seq_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_electra_seq_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_electra_seq_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"davidson_electra_cnn_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(cnn_model.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== DAVIDSON: ELECTRA-Embed + CNN ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49c46c-dbde-4d88-8ef7-7a57bc4d83bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c03d66a-011f-4235-85e2-de88ce81c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.read_csv(f\"{DATA_PATH}/hatexplain_train.csv\"),\n",
    "    pd.read_csv(f\"{DATA_PATH}/hatexplain_val.csv\")\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25329144-25f5-4221-b173-1c7e8cd25e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trump containing silence on the sa genocide an...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>might fuck around and drunk text this nigga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fuck those faggots and dykes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>women lie to kick it niggas lie to hit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flair into the white trash period of my life o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20143</th>\n",
       "      <td>deflect all you want kike spawn the whole worl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20144</th>\n",
       "      <td>recently arrested antifa protestor tweets abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20145</th>\n",
       "      <td>that new nike react technology on that running...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20146</th>\n",
       "      <td>south african white people are some of the wor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20147</th>\n",
       "      <td>wow this d bag if he not a kike he might as we...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20148 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_text  label\n",
       "0      trump containing silence on the sa genocide an...      2\n",
       "1            might fuck around and drunk text this nigga      0\n",
       "2                           fuck those faggots and dykes      2\n",
       "3                 women lie to kick it niggas lie to hit      0\n",
       "4      flair into the white trash period of my life o...      0\n",
       "...                                                  ...    ...\n",
       "20143  deflect all you want kike spawn the whole worl...      2\n",
       "20144  recently arrested antifa protestor tweets abou...      0\n",
       "20145  that new nike react technology on that running...      0\n",
       "20146  south african white people are some of the wor...      2\n",
       "20147  wow this d bag if he not a kike he might as we...      2\n",
       "\n",
       "[20148 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698adbf-9154-41ba-83bd-d7fb20dd4ece",
   "metadata": {},
   "source": [
    "### BERT-Embed + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "574160a6-90c3-4d8e-a0b1-d745a8a5257b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings: 100%|█████████| 16118/16118 [22:49<00:00, 11.77it/s]\n",
      "Extracting BERT embeddings: 100%|███████████| 2015/2015 [02:37<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN: BERT-Embed + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6130    0.7698    0.6825       821\n",
      "           1     0.4907    0.3546    0.4117       595\n",
      "           2     0.6282    0.5810    0.6036       599\n",
      "\n",
      "    accuracy                         0.5911      2015\n",
      "   macro avg     0.5773    0.5685    0.5660      2015\n",
      "weighted avg     0.5814    0.5911    0.5791      2015\n",
      "\n",
      "Model saved as: hatexplain_bert_mlp.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "texts_val   = val_df[\"clean_text\"].tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).to_numpy()\n",
    "y_val       = val_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                               padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            mean_vec = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            all_embeddings.append(mean_vec)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "X_train = get_bert_embeddings(texts_train, tokenizer, model)\n",
    "X_val   = get_bert_embeddings(texts_val, tokenizer, model)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=5,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_val)\n",
    "print(\"\\n=== HATEXPLAIN: BERT-Embed + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "joblib.dump(mlp, os.path.join(SAVE_PATH, \"hatexplain_bert_mlp.joblib\"))\n",
    "print(\"Model saved as: hatexplain_bert_mlp.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cb5ce-ac6f-41d6-ad68-f16faed9a943",
   "metadata": {},
   "source": [
    "### BERT-Embed + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d952a05-db32-4a30-99c6-2b9795a41a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT token embeddings: 100%|███| 16118/16118 [21:45<00:00, 12.35it/s]\n",
      "Extracting BERT token embeddings: 100%|█████| 2015/2015 [01:59<00:00, 16.80it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4895 - loss: 1.0125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 83ms/step - accuracy: 0.4896 - loss: 1.0124 - val_accuracy: 0.5965 - val_loss: 0.8846\n",
      "Epoch 2/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6005 - loss: 0.8730"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 71ms/step - accuracy: 0.6006 - loss: 0.8730 - val_accuracy: 0.6124 - val_loss: 0.8628\n",
      "Epoch 3/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6337 - loss: 0.8156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 73ms/step - accuracy: 0.6337 - loss: 0.8156 - val_accuracy: 0.6164 - val_loss: 0.8462\n",
      "Epoch 4/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 76ms/step - accuracy: 0.6632 - loss: 0.7693 - val_accuracy: 0.6129 - val_loss: 0.8768\n",
      "Epoch 5/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 74ms/step - accuracy: 0.6859 - loss: 0.7300 - val_accuracy: 0.6020 - val_loss: 0.8837\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n",
      "\n",
      "=== HATEXPLAIN: BERT-Embed + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6050    0.8319    0.7005       821\n",
      "           1     0.5645    0.2353    0.3321       595\n",
      "           2     0.6567    0.6995    0.6774       599\n",
      "\n",
      "    accuracy                         0.6164      2015\n",
      "   macro avg     0.6087    0.5889    0.5700      2015\n",
      "weighted avg     0.6084    0.6164    0.5849      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int), num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int),   num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_seq_embeddings(texts):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                               padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_bert_seq_embeddings(texts_train)\n",
    "X_val   = get_bert_seq_embeddings(texts_val)\n",
    "\n",
    "bilstm_model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "bilstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"hatexplain_bert_bilstm_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "bilstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(bilstm_model.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== HATEXPLAIN: BERT-Embed + BiLSTM ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e58124e-3634-4440-969f-6cac094e0667",
   "metadata": {},
   "source": [
    "### BERT-Embed + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb366e64-20c8-4026-8d4e-640b0302636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT token embeddings: 100%|███| 16118/16118 [34:57<00:00,  7.68it/s]\n",
      "Extracting BERT token embeddings: 100%|█████| 2015/2015 [13:03<00:00,  2.57it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4976 - loss: 1.1810"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 48ms/step - accuracy: 0.4977 - loss: 1.1806 - val_accuracy: 0.5965 - val_loss: 0.9014\n",
      "Epoch 2/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5832 - loss: 0.8953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 45ms/step - accuracy: 0.5832 - loss: 0.8953 - val_accuracy: 0.5995 - val_loss: 0.8748\n",
      "Epoch 3/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 48ms/step - accuracy: 0.6053 - loss: 0.8615 - val_accuracy: 0.6055 - val_loss: 0.8752\n",
      "Epoch 4/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.6245 - loss: 0.8269 - val_accuracy: 0.6020 - val_loss: 0.8767\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "=== HATEXPLAIN: BERT-Embed + CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5949    0.8210    0.6899       821\n",
      "           1     0.5357    0.2521    0.3429       595\n",
      "           2     0.6379    0.6411    0.6395       599\n",
      "\n",
      "    accuracy                         0.5995      2015\n",
      "   macro avg     0.5895    0.5714    0.5574      2015\n",
      "weighted avg     0.5902    0.5995    0.5724      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int), num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int),   num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_seq_embeddings(texts):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                               padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_bert_seq_embeddings(texts_train)\n",
    "X_val   = get_bert_seq_embeddings(texts_val)\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"hatexplain_bert_cnn_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(cnn_model.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== HATEXPLAIN: BERT-Embed + CNN ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d11ee9-d3e5-4693-b1e2-87e467f054db",
   "metadata": {},
   "source": [
    "### ALBERT-Embed + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d2d6fda-7e80-40f7-b276-1746866620fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT embeddings: 100%|███████| 16118/16118 [31:19<00:00,  8.57it/s]\n",
      "Extracting ALBERT embeddings: 100%|█████████| 2015/2015 [01:52<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN: ALBERT-Embed + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5484    0.7515    0.6341       821\n",
      "           1     0.5357    0.1261    0.2041       595\n",
      "           2     0.5093    0.6377    0.5663       599\n",
      "\n",
      "    accuracy                         0.5330      2015\n",
      "   macro avg     0.5312    0.5051    0.4682      2015\n",
      "weighted avg     0.5331    0.5330    0.4870      2015\n",
      "\n",
      "Model saved as: hatexplain_albert_mlp.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).to_numpy()\n",
    "y_val       = val_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_albert_embeddings(texts):\n",
    "    all_vecs = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                               padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            mean_vec = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            all_vecs.append(mean_vec)\n",
    "    return np.array(all_vecs)\n",
    "\n",
    "X_train = get_albert_embeddings(texts_train)\n",
    "X_val   = get_albert_embeddings(texts_val)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=5,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_val)\n",
    "print(\"\\n=== HATEXPLAIN: ALBERT-Embed + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "joblib.dump(mlp, os.path.join(SAVE_PATH, \"hatexplain_albert_mlp.joblib\"))\n",
    "print(\"Model saved as: hatexplain_albert_mlp.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f24b0-15d3-40f9-a02b-d20d75777017",
   "metadata": {},
   "source": [
    "### ALBERT-Embed + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2dfc8c7-879f-4c52-ac27-0f28e48cc0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT token embeddings: 100%|█| 16118/16118 [38:10<00:00,  7.04it/s]\n",
      "Extracting ALBERT token embeddings: 100%|███| 2015/2015 [02:45<00:00, 12.15it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4243 - loss: 1.0806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 77ms/step - accuracy: 0.4244 - loss: 1.0805 - val_accuracy: 0.5489 - val_loss: 0.9483\n",
      "Epoch 2/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.5363 - loss: 0.9630"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 82ms/step - accuracy: 0.5363 - loss: 0.9629 - val_accuracy: 0.5628 - val_loss: 0.9134\n",
      "Epoch 3/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5752 - loss: 0.9070"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - accuracy: 0.5752 - loss: 0.9070 - val_accuracy: 0.5782 - val_loss: 0.9111\n",
      "Epoch 4/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5942 - loss: 0.8887"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 74ms/step - accuracy: 0.5942 - loss: 0.8887 - val_accuracy: 0.5901 - val_loss: 0.8805\n",
      "Epoch 5/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6105 - loss: 0.8601"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 154ms/step - accuracy: 0.6105 - loss: 0.8602 - val_accuracy: 0.6040 - val_loss: 0.8713\n",
      "Epoch 6/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 65ms/step - accuracy: 0.6198 - loss: 0.8307 - val_accuracy: 0.5950 - val_loss: 0.8876\n",
      "Epoch 7/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 63ms/step - accuracy: 0.6375 - loss: 0.8097 - val_accuracy: 0.5960 - val_loss: 0.8827\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\n",
      "=== HATEXPLAIN: ALBERT-Embed + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5958    0.7917    0.6799       821\n",
      "           1     0.4801    0.3042    0.3724       595\n",
      "           2     0.7057    0.6444    0.6736       599\n",
      "\n",
      "    accuracy                         0.6040      2015\n",
      "   macro avg     0.5939    0.5801    0.5753      2015\n",
      "weighted avg     0.5943    0.6040    0.5873      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int).to_numpy(),   num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_seq_embeddings(texts):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                               padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_seq_embeddings(texts_train)\n",
    "X_val   = get_seq_embeddings(texts_val)\n",
    "\n",
    "model_bilstm = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=False), input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "model_bilstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"hatexplain_albert_bilstm_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "model_bilstm.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(model_bilstm.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== HATEXPLAIN: ALBERT-Embed + BiLSTM ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806713b5-9ae0-4eb7-8969-3b2409a35f46",
   "metadata": {},
   "source": [
    "### ALBERT-Embed + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "683b69dd-7885-4d76-81d3-bb9f1e5c4e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT token embeddings: 100%|█| 16118/16118 [38:03<00:00,  7.06it/s]\n",
      "Extracting ALBERT token embeddings: 100%|███| 2015/2015 [01:49<00:00, 18.47it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m503/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.4354 - loss: 1.7689"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 136ms/step - accuracy: 0.4356 - loss: 1.7666 - val_accuracy: 0.5737 - val_loss: 0.9211\n",
      "Epoch 2/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5375 - loss: 0.9713"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.5375 - loss: 0.9713 - val_accuracy: 0.5782 - val_loss: 0.9044\n",
      "Epoch 3/10\n",
      "\u001b[1m502/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5627 - loss: 0.9274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 58ms/step - accuracy: 0.5627 - loss: 0.9274 - val_accuracy: 0.5960 - val_loss: 0.8904\n",
      "Epoch 4/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.5787 - loss: 0.9068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 46ms/step - accuracy: 0.5787 - loss: 0.9068 - val_accuracy: 0.5836 - val_loss: 0.8890\n",
      "Epoch 5/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 45ms/step - accuracy: 0.5918 - loss: 0.8759 - val_accuracy: 0.5926 - val_loss: 0.8924\n",
      "Epoch 6/10\n",
      "\u001b[1m503/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6015 - loss: 0.8653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 47ms/step - accuracy: 0.6015 - loss: 0.8653 - val_accuracy: 0.5896 - val_loss: 0.8842\n",
      "Epoch 7/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 57ms/step - accuracy: 0.6087 - loss: 0.8490 - val_accuracy: 0.5881 - val_loss: 0.8912\n",
      "Epoch 8/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 47ms/step - accuracy: 0.6207 - loss: 0.8364 - val_accuracy: 0.5821 - val_loss: 0.8953\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "=== HATEXPLAIN: ALBERT-Embed + CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5663    0.8526    0.6806       821\n",
      "           1     0.4776    0.1966    0.2786       595\n",
      "           2     0.6948    0.6194    0.6549       599\n",
      "\n",
      "    accuracy                         0.5896      2015\n",
      "   macro avg     0.5796    0.5562    0.5380      2015\n",
      "weighted avg     0.5783    0.5896    0.5542      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int).to_numpy(),   num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_seq_embeddings(texts):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                               padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_seq_embeddings(texts_train)\n",
    "X_val   = get_seq_embeddings(texts_val)\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"hatexplain_albert_cnn_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(cnn_model.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== HATEXPLAIN: ALBERT-Embed + CNN ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2910ab7-16fb-45af-ae9d-305b4b7ea72c",
   "metadata": {},
   "source": [
    "### ELECTRA-Embed + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb96a126-ab22-4dde-b8a6-0fc5cf464aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA embeddings: 100%|██████| 16118/16118 [39:06<00:00,  6.87it/s]\n",
      "Extracting ELECTRA embeddings: 100%|████████| 2015/2015 [03:57<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN: ELECTRA-Embed + MLP ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5567    0.7540    0.6405       821\n",
      "           1     0.4699    0.1966    0.2773       595\n",
      "           2     0.5306    0.5793    0.5539       599\n",
      "\n",
      "    accuracy                         0.5375      2015\n",
      "   macro avg     0.5190    0.5100    0.4905      2015\n",
      "weighted avg     0.5233    0.5375    0.5075      2015\n",
      "\n",
      "Model saved as: hatexplain_electra_mlp.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = train_df[\"label\"].astype(int).to_numpy()\n",
    "y_val       = val_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    all_vecs = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            mean_vec = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            all_vecs.append(mean_vec)\n",
    "    return np.array(all_vecs)\n",
    "\n",
    "X_train = get_embeddings(texts_train)\n",
    "X_val   = get_embeddings(texts_val)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128,),\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=5,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_val)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN: ELECTRA-Embed + MLP ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "joblib.dump(mlp, os.path.join(SAVE_PATH, \"hatexplain_electra_mlp.joblib\"))\n",
    "print(\"Model saved as: hatexplain_electra_mlp.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e6fc2-6998-4412-9e5b-fd511eb0151a",
   "metadata": {},
   "source": [
    "### ELECTRA-Embed + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fb3dda0-508d-4c47-9f57-cb4a86838bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA token embeddings: 100%|█| 16118/16118 [36:01<00:00,  7.46it/s\n",
      "Extracting ELECTRA token embeddings: 100%|██| 2015/2015 [13:58<00:00,  2.40it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.4658 - loss: 1.0471"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 271ms/step - accuracy: 0.4658 - loss: 1.0470 - val_accuracy: 0.5692 - val_loss: 0.9342\n",
      "Epoch 2/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5666 - loss: 0.9220"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 70ms/step - accuracy: 0.5667 - loss: 0.9220 - val_accuracy: 0.5906 - val_loss: 0.8987\n",
      "Epoch 3/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5923 - loss: 0.8746"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - accuracy: 0.5923 - loss: 0.8746 - val_accuracy: 0.5931 - val_loss: 0.8932\n",
      "Epoch 4/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6284 - loss: 0.8315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 71ms/step - accuracy: 0.6284 - loss: 0.8315 - val_accuracy: 0.6065 - val_loss: 0.8713\n",
      "Epoch 5/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 71ms/step - accuracy: 0.6566 - loss: 0.7796 - val_accuracy: 0.6089 - val_loss: 0.8890\n",
      "Epoch 6/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 272ms/step - accuracy: 0.6852 - loss: 0.7356 - val_accuracy: 0.5940 - val_loss: 0.9251\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\n",
      "=== HATEXPLAIN: ELECTRA-Embed + BiLSTM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6098    0.7881    0.6876       821\n",
      "           1     0.5072    0.2958    0.3737       595\n",
      "           2     0.6573    0.6661    0.6617       599\n",
      "\n",
      "    accuracy                         0.6065      2015\n",
      "   macro avg     0.5914    0.5833    0.5743      2015\n",
      "weighted avg     0.5936    0.6065    0.5872      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_seq_embeddings(texts):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_seq_embeddings(texts_train)\n",
    "X_val   = get_seq_embeddings(texts_val)\n",
    "\n",
    "model_bilstm = Sequential([\n",
    "    Bidirectional(LSTM(64), input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "model_bilstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"hatexplain_electra_bilstm_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "model_bilstm.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(model_bilstm.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== HATEXPLAIN: ELECTRA-Embed + BiLSTM ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a591b6a-f364-4759-a322-6149f7b95acd",
   "metadata": {},
   "source": [
    "### ELECTRA-Embed + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd5b5dd3-6c29-46c2-aaba-edf811f35d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA token embeddings: 100%|█| 16118/16118 [25:33<00:00, 10.51it/s\n",
      "Extracting ELECTRA token embeddings: 100%|██| 2015/2015 [03:44<00:00,  8.98it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m503/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4605 - loss: 1.2188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 48ms/step - accuracy: 0.4607 - loss: 1.2181 - val_accuracy: 0.5707 - val_loss: 0.9279\n",
      "Epoch 2/10\n",
      "\u001b[1m503/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5756 - loss: 0.9204"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.5755 - loss: 0.9204 - val_accuracy: 0.5931 - val_loss: 0.8948\n",
      "Epoch 3/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 44ms/step - accuracy: 0.6021 - loss: 0.8758 - val_accuracy: 0.5965 - val_loss: 0.8959\n",
      "Epoch 4/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6117 - loss: 0.8481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.6117 - loss: 0.8481 - val_accuracy: 0.5931 - val_loss: 0.8892\n",
      "Epoch 5/10\n",
      "\u001b[1m503/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6472 - loss: 0.7937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.6472 - loss: 0.7938 - val_accuracy: 0.5980 - val_loss: 0.8823\n",
      "Epoch 6/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.6697 - loss: 0.7504 - val_accuracy: 0.5846 - val_loss: 0.9130\n",
      "Epoch 7/10\n",
      "\u001b[1m504/504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 53ms/step - accuracy: 0.6863 - loss: 0.7147 - val_accuracy: 0.5901 - val_loss: 0.9147\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "=== HATEXPLAIN: ELECTRA-Embed + CNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5926    0.8185    0.6875       821\n",
      "           1     0.4971    0.2857    0.3629       595\n",
      "           2     0.6735    0.6060    0.6380       599\n",
      "\n",
      "    accuracy                         0.5980      2015\n",
      "   macro avg     0.5877    0.5701    0.5628      2015\n",
      "weighted avg     0.5884    0.5980    0.5769      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_CLASSES = 3\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_train.csv\"))\n",
    "val_df   = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_val.csv\"))\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val   = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_train     = to_categorical(train_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "y_val       = to_categorical(val_df[\"label\"].astype(int).to_numpy(), num_classes=NUM_CLASSES)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_seq_embeddings(texts):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA token embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_seq_embeddings(texts_train)\n",
    "X_val   = get_seq_embeddings(texts_val)\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(MAX_LEN, HIDDEN_SIZE)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(SAVE_PATH, \"hatexplain_electra_cnn_best.h5\"),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(cnn_model.predict(X_val), axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "print(\"\\n=== HATEXPLAIN: ELECTRA-Embed + CNN ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4f035-df76-4c98-900c-635b20ae32dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ensembles (based on best performing architectures on validation results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1dbb49-7ed0-48d6-a38d-e7b2aeaf4fe2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDOS (validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de7947-57a9-4288-8e9c-7b77e5954788",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### miscellaneous (required for Soft Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e64c1b53-d21d-4920-84cc-91308800460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: TF-IDF + SVC (probability=True) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90      1514\n",
      "           1       0.91      0.32      0.48       486\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.86      0.66      0.69      2000\n",
      "weighted avg       0.84      0.83      0.79      2000\n",
      "\n",
      " Model saved as: edos_svc_prob.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/edos_train.csv\")\n",
    "val_df   = pd.read_csv(\"./splits/edos_val.csv\")\n",
    "\n",
    "X_train = train_df[\"clean_text\"].astype(str)\n",
    "y_train = train_df[\"label\"].astype(int)\n",
    "\n",
    "X_val = val_df[\"clean_text\"].astype(str)\n",
    "y_val = val_df[\"label\"].astype(int)\n",
    "\n",
    "svc_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000)),\n",
    "    ('svc', SVC(probability=True))  \n",
    "])\n",
    "\n",
    "svc_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc_pipeline.predict(X_val)\n",
    "print(\"\\n=== EDOS: TF-IDF + SVC (probability=True) ===\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "joblib.dump(svc_pipeline, \"./models/edos_svc_prob.joblib\")\n",
    "print(\" Model saved as: edos_svc_prob.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "68a7a77a-9439-496a-bf00-0e6799dc3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = joblib.load(\"./models/edos_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(X_val) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a44484a1-be3f-4f35-87f3-40dc673bd292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT sequence embeddings: 100%|██| 2000/2000 [01:53<00:00, 17.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: ./models/edos_bert_embed_val.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "val_df = pd.read_csv(\"./splits/edos_val.csv\")\n",
    "texts = val_df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "bert_val_embeddings = get_bert_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/edos_bert_embed_val.npy\", bert_val_embeddings)\n",
    "print(\" Saved: ./models/edos_bert_embed_val.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd23d7-776d-48ea-ba32-e3dd47c83ed9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Soft Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a23ac6f-47c3-412e-abeb-365586779cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step\n",
      "\n",
      "=== EDOS: Soft Voting Ensemble (SVC + GloVe + BERT) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8814    0.9899    0.9325      4544\n",
      "           1     0.9487    0.5845    0.7233      1456\n",
      "\n",
      "    accuracy                         0.8915      6000\n",
      "   macro avg     0.9151    0.7872    0.8279      6000\n",
      "weighted avg     0.8978    0.8915    0.8818      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# === Load validation data ===\n",
    "val_df = pd.read_csv(\"./splits/edos_val.csv\")\n",
    "texts = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = val_df[\"label\"].astype(int).values\n",
    "\n",
    "# === 1. TF-IDF + SVC (with probability=True) ===\n",
    "svc = joblib.load(\"./models/edos_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "# === 2. GloVe + BiLSTM ===\n",
    "glove_model = load_model(\"./models/edos_glove_bilstm.h5\")\n",
    "\n",
    "# Load tokenizer used during GloVe training\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)  # Replace with loaded tokenizer if available\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "# === 3. BERT + BiLSTM ===\n",
    "bert_model = load_model(\"./models/edos_bert_bilstm.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_val.npy\")  # ← load precomputed frozen embeddings\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "# === Soft Voting Ensemble ===\n",
    "avg_probs = (svc_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = (avg_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "# === Report ===\n",
    "print(\"\\n=== EDOS: Soft Voting Ensemble (SVC + GloVe + BERT) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6144573-0ff4-4e5b-9e89-a1432894aff0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb4cdf87-222d-49a4-9452-a41e4c4e6e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8140    0.9980    0.8967      4544\n",
      "           1     0.9790    0.2885    0.4456      1456\n",
      "\n",
      "    accuracy                         0.8258      6000\n",
      "   macro avg     0.8965    0.6432    0.6712      6000\n",
      "weighted avg     0.8541    0.8258    0.7872      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "svc_preds   = svc.predict(texts)\n",
    "glove_preds = np.argmax(glove_probs, axis=1)\n",
    "bert_preds  = (bert_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "stacked_preds = np.vstack([svc_preds, glove_preds, bert_preds])\n",
    "final_preds, _ = mode(stacked_preds, axis=0)\n",
    "\n",
    "print(classification_report(y_true, final_preds.flatten(), digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a70caf-566b-4be7-bade-72ce52a0f9f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Weighted Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7926bfcb-eb38-4a02-83e7-af3bb4a826f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9384    0.9859    0.9616      4544\n",
      "           1     0.9478    0.7981    0.8665      1456\n",
      "\n",
      "    accuracy                         0.9403      6000\n",
      "   macro avg     0.9431    0.8920    0.9140      6000\n",
      "weighted avg     0.9407    0.9403    0.9385      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_w, glove_w, bert_w = 0.2, 0.3, 0.5\n",
    "\n",
    "weighted_probs = (svc_w * svc_probs) + (glove_w * glove_probs) + (bert_w * bert_probs)\n",
    "y_pred = (weighted_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c3f5949e-edbe-4b3d-81cb-7b9e5457d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9476    0.9833    0.9651      4544\n",
      "           1     0.9409    0.8304    0.8822      1456\n",
      "\n",
      "    accuracy                         0.9462      6000\n",
      "   macro avg     0.9442    0.9068    0.9236      6000\n",
      "weighted avg     0.9460    0.9462    0.9450      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svc_w, glove_w, bert_w = 0.1, 0.3, 0.6\n",
    "\n",
    "\n",
    "weighted_probs = (svc_w * svc_probs) + (glove_w * glove_probs) + (bert_w * bert_probs)\n",
    "y_pred = (weighted_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "35111497-e79e-4f43-bc9f-675e0d44f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9521    0.9806    0.9662      4544\n",
      "           1     0.9333    0.8462    0.8876      1456\n",
      "\n",
      "    accuracy                         0.9480      6000\n",
      "   macro avg     0.9427    0.9134    0.9269      6000\n",
      "weighted avg     0.9476    0.9480    0.9471      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_w, glove_w, bert_w = 0.1, 0.2, 0.7\n",
    "\n",
    "\n",
    "weighted_probs = (svc_w * svc_probs) + (glove_w * glove_probs) + (bert_w * bert_probs)\n",
    "y_pred = (weighted_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2e681c66-88bf-4fa5-88fa-ff37eaf5b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9551    0.9778    0.9663      4544\n",
      "           1     0.9251    0.8565    0.8894      1456\n",
      "\n",
      "    accuracy                         0.9483      6000\n",
      "   macro avg     0.9401    0.9171    0.9279      6000\n",
      "weighted avg     0.9478    0.9483    0.9476      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_w, glove_w, bert_w = 0.1, 0.1, 0.8\n",
    "\n",
    "\n",
    "weighted_probs = (svc_w * svc_probs) + (glove_w * glove_probs) + (bert_w * bert_probs)\n",
    "y_pred = (weighted_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b36c3a-833f-4643-b0c4-942188787abb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "683946d3-06ca-4db1-a2fd-c5c1b13c8d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: Stacking Ensemble (Logistic Regression) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8691    0.9419    0.9040      1515\n",
      "           1     0.7542    0.5567    0.6406       485\n",
      "\n",
      "    accuracy                         0.8485      2000\n",
      "   macro avg     0.8116    0.7493    0.7723      2000\n",
      "weighted avg     0.8412    0.8485    0.8401      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svc_class1   = svc_probs[:, 1]\n",
    "glove_class1 = glove_probs.flatten()\n",
    "bert_class1  = bert_probs.flatten()\n",
    "\n",
    "X_stack = np.vstack([svc_class1, glove_class1, bert_class1]).T\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_stack, y_true)\n",
    "y_pred_stack = meta_clf.predict(X_stack)\n",
    "\n",
    "print(\"\\n=== EDOS: Stacking Ensemble (Logistic Regression) ===\")\n",
    "print(classification_report(y_true, y_pred_stack, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "200972b4-7dce-4a79-844a-8209608d7b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_stack)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-Toxic\", \"Toxic\"])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix: Stacked Ensemble (EDOS)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b48df-016a-4d7b-ab52-1deea661aec1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDOS (SVC + GloVe + BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1fe0bc3f-d569-4b0c-8330-0fbc0f5c542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8ccaa995-a524-41de-8972-1bd441899ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/edos_glove_tokenizer.joblib']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/edos_train.csv\")\n",
    "val_df   = pd.read_csv(\"./splits/edos_val.csv\")\n",
    "all_texts = pd.concat([train_df, val_df])[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "joblib.dump(tokenizer, \"./models/edos_glove_tokenizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a3a046d-a2a7-4cef-89ba-4a0fcf504540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT token embeddings: 100%|█████| 2000/2000 [01:34<00:00, 21.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sequence embeddings to: ./models/edos_bert_embed_test_seq.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits/edos_test.csv\"\n",
    "SAVE_PATH = \"./models/edos_bert_embed_test_seq.npy\"\n",
    "MAX_LEN = 100\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_seq_embeddings(texts):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT token embeddings\"):\n",
    "            inputs = tokenizer(\n",
    "                text, return_tensors=\"pt\", truncation=True,\n",
    "                padding=\"max_length\", max_length=MAX_LEN\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_test_seq = get_bert_seq_embeddings(texts)\n",
    "np.save(SAVE_PATH, X_test_seq)\n",
    "print(f\"Saved sequence embeddings to: {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ded87b6-e4d9-4463-9d6e-8f60c21f792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT token embeddings: 100%|█████| 2000/2000 [02:18<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sequence embeddings to: ./models/edos_bert_embed_test_seq.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits/edos_test.csv\"\n",
    "SAVE_PATH = \"./models/edos_bert_embed_test_seq.npy\"\n",
    "MAX_LEN = 100\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_seq_embeddings(texts):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT token embeddings\"):\n",
    "            inputs = tokenizer(\n",
    "                text, return_tensors=\"pt\", truncation=True,\n",
    "                padding=\"max_length\", max_length=MAX_LEN\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_test_seq = get_bert_seq_embeddings(texts)\n",
    "np.save(SAVE_PATH, X_test_seq)\n",
    "print(f\"Saved sequence embeddings to: {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bf55d7c5-167d-46a2-951e-e5352e1970df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
      "\n",
      "=== EDOS TEST: Soft Voting Ensemble (SVC + GloVe + BERT) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8480    0.9723    0.9059      1515\n",
      "           1     0.8403    0.4557    0.5909       485\n",
      "\n",
      "    accuracy                         0.8470      2000\n",
      "   macro avg     0.8442    0.7140    0.7484      2000\n",
      "weighted avg     0.8461    0.8470    0.8295      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "# TF-IDF + SVC\n",
    "svc = joblib.load(\"./models/edos_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "# GloVe + BiLSTM\n",
    "glove_model = load_model(\"./models/edos_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\") \n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "# BERT + BiLSTM\n",
    "bert_model = load_model(\"./models/edos_bert_bilstm.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")  \n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "avg_probs = (svc_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = (avg_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== EDOS TEST: Soft Voting Ensemble (SVC + GloVe + BERT) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9096ee4c-4244-4a0a-8d8c-4de3b8f5a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8640    0.9564    0.9079      1515\n",
      "           1     0.7957    0.5299    0.6361       485\n",
      "\n",
      "    accuracy                         0.8530      2000\n",
      "   macro avg     0.8299    0.7432    0.7720      2000\n",
      "weighted avg     0.8475    0.8530    0.8420      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_w, glove_w, bert_w = 0.1, 0.1, 0.8\n",
    "\n",
    "\n",
    "weighted_probs = (svc_w * svc_probs) + (glove_w * glove_probs) + (bert_w * bert_probs)\n",
    "y_pred = (weighted_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "107d092d-d21b-4e5c-9469-7b02c85b1df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8870    0.9122    0.8994      1515\n",
      "           1     0.6991    0.6371    0.6667       485\n",
      "\n",
      "    accuracy                         0.8455      2000\n",
      "   macro avg     0.7931    0.7747    0.7831      2000\n",
      "weighted avg     0.8415    0.8455    0.8430      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_w, glove_w, bert_w = 0.1, 0.4, 0.8\n",
    "\n",
    "\n",
    "weighted_probs = (svc_w * svc_probs) + (glove_w * glove_probs) + (bert_w * bert_probs)\n",
    "y_pred = (weighted_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7c8d8e7b-fe63-4c20-b872-7d17c274b382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: Stacking Ensemble (Logistic Regression) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8727    0.9505    0.9100      1515\n",
      "           1     0.7857    0.5670    0.6587       485\n",
      "\n",
      "    accuracy                         0.8575      2000\n",
      "   macro avg     0.8292    0.7588    0.7843      2000\n",
      "weighted avg     0.8516    0.8575    0.8490      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svc_class1   = svc_probs[:, 1]\n",
    "glove_class1 = glove_probs.flatten()\n",
    "bert_class1  = bert_probs.flatten()\n",
    "\n",
    "X_stack = np.vstack([svc_class1, glove_class1, bert_class1]).T\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_stack, y_true)\n",
    "y_pred_stack = meta_clf.predict(X_stack)\n",
    "\n",
    "print(\"\\n=== EDOS: Stacking Ensemble (Logistic Regression) ===\")\n",
    "print(classification_report(y_true, y_pred_stack, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9683e7-33ef-4aa2-bff7-1759d1b72d3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DAVIDSON (SVC + GloVe + BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "44d0e4f5-4a3e-4743-9c19-84643cd3ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/davidson_train.csv\")\n",
    "val_df   = pd.read_csv(\"./splits/davidson_val.csv\")\n",
    "\n",
    "X_train = train_df[\"clean_text\"]\n",
    "y_train = train_df[\"label\"].astype(int)\n",
    "X_val   = val_df[\"clean_text\"]\n",
    "y_val   = val_df[\"label\"].astype(int)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "    (\"svc\", SVC(probability=True))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "8b520894-55a3-4aa2-bd98-5afa50b47199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON: TF-IDF + SVC (val set) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8182    0.1259    0.2182       143\n",
      "           1     0.9089    0.9719    0.9393      1919\n",
      "           2     0.8515    0.8269    0.8390       416\n",
      "\n",
      "    accuracy                         0.8987      2478\n",
      "   macro avg     0.8595    0.6416    0.6655      2478\n",
      "weighted avg     0.8940    0.8987    0.8809      2478\n",
      "\n",
      " Saved as: ./models/davidson_svc_prob.joblib\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== DAVIDSON: TF-IDF + SVC (val set) ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "joblib.dump(pipe, \"./models/davidson_svc_prob.joblib\")\n",
    "print(\" Saved as: ./models/davidson_svc_prob.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a2c02997-364c-430e-98bb-df64ed1c8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_svc_prob.joblib\")  \n",
    "svc_probs = svc.predict_proba(texts)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f24b09a2-1e3c-4f41-a5a2-ae7bdac9f5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved as: davidson_glove_tokenizer.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "MAX_WORDS = 10000\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/davidson_train.csv\")\n",
    "texts = train_df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "joblib.dump(tokenizer, \"./models/davidson_glove_tokenizer.joblib\")\n",
    "print(\"Tokenizer saved as: davidson_glove_tokenizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2c8bdbcd-99a0-46f1-b107-5565f4ff17b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT test embeddings: 100%|██████| 2479/2479 [01:59<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved Davidson test embeddings to: davidson_bert_embed_test.npy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, \"davidson_test.csv\"))\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT test embeddings\"):\n",
    "            inputs = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_len\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy() \n",
    "            all_seq.append(token_vecs)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_test = get_bert_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(os.path.join(SAVE_PATH, \"davidson_bert_embed_test.npy\"), X_test)\n",
    "print(\" Saved Davidson test embeddings to: davidson_bert_embed_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "91669b06-bf55-4587-8613-34ecad6b38f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
      "\n",
      "=== DAVIDSON TEST: Soft Voting Ensemble (SVC + GloVe + BERT) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4898    0.1678    0.2500       143\n",
      "           1     0.9269    0.9640    0.9451      1919\n",
      "           2     0.8641    0.8993    0.8813       417\n",
      "\n",
      "    accuracy                         0.9072      2479\n",
      "   macro avg     0.7602    0.6771    0.6921      2479\n",
      "weighted avg     0.8911    0.9072    0.8943      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "# TF-IDF + SVC\n",
    "svc = joblib.load(\"./models/davidson_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "# GloVe + BiLSTM \n",
    "glove_model = load_model(\"./models/davidson_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "# BERT + BiLSTM\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "avg_probs = (svc_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Soft Voting Ensemble (SVC + GloVe + BERT) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "bfdb9982-efac-43b6-b844-3b492fa870fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON TEST: Weighted Soft Voting Ensemble ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5490    0.1958    0.2887       143\n",
      "           1     0.9281    0.9687    0.9480      1919\n",
      "           2     0.8682    0.8849    0.8765       417\n",
      "\n",
      "    accuracy                         0.9100      2479\n",
      "   macro avg     0.7818    0.6831    0.7044      2479\n",
      "weighted avg     0.8962    0.9100    0.8979      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = [0.1, 0.2, 0.8] \n",
    "avg_probs = (\n",
    "    weights[0] * svc_probs +\n",
    "    weights[1] * glove_probs +\n",
    "    weights[2] * bert_probs\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Weighted Soft Voting Ensemble ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4780022c-0e8f-4b37-8362-74cef4a72ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON TEST: Stacking Ensemble (LogReg) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4815    0.1818    0.2640       143\n",
      "           1     0.9242    0.9651    0.9442      1919\n",
      "           2     0.8789    0.8873    0.8831       417\n",
      "\n",
      "    accuracy                         0.9068      2479\n",
      "   macro avg     0.7615    0.6781    0.6971      2479\n",
      "weighted avg     0.8910    0.9068    0.8947      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_stack = np.hstack([\n",
    "    svc_probs,\n",
    "    glove_probs,\n",
    "    bert_probs\n",
    "]) \n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000)\n",
    "meta_clf.fit(X_stack, y_true)\n",
    "y_pred_stack = meta_clf.predict(X_stack)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Stacking Ensemble (LogReg) ===\")\n",
    "print(classification_report(y_true, y_pred_stack, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c5fdc-d65d-4f2d-b602-6add19a026c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HateXplain (SVC + GloVe + BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "952778ac-ab0a-445f-b614-611716f9d2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN: TF-IDF + SVC ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5915    0.7990    0.6798       821\n",
      "           1     0.5239    0.3496    0.4194       595\n",
      "           2     0.7092    0.6027    0.6516       599\n",
      "\n",
      "    accuracy                         0.6079      2015\n",
      "   macro avg     0.6082    0.5838    0.5836      2015\n",
      "weighted avg     0.6066    0.6079    0.5945      2015\n",
      "\n",
      "Saved as: ./models/hatexplain_svc_prob.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/hatexplain_train.csv\")\n",
    "val_df   = pd.read_csv(\"./splits/hatexplain_val.csv\")\n",
    "X_train, y_train = train_df[\"clean_text\"], train_df[\"label\"]\n",
    "X_val, y_val     = val_df[\"clean_text\"],   val_df[\"label\"]\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "    (\"svc\", SVC(probability=True))\n",
    "])\n",
    "svc_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc_pipe.predict(X_val)\n",
    "print(\"\\n=== HATEXPLAIN: TF-IDF + SVC ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "joblib.dump(svc_pipe, \"./models/hatexplain_svc_prob.joblib\")\n",
    "print(\"Saved as: ./models/hatexplain_svc_prob.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6d77a435-9616-4834-afb7-7b38ed9f29ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 35ms/step - accuracy: 0.4503 - loss: 1.0491 - val_accuracy: 0.5575 - val_loss: 0.9507\n",
      "Epoch 2/10\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.5590 - loss: 0.9303 - val_accuracy: 0.5743 - val_loss: 0.9132\n",
      "Epoch 3/10\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.5762 - loss: 0.9071 - val_accuracy: 0.5868 - val_loss: 0.9021\n",
      "Epoch 4/10\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 40ms/step - accuracy: 0.6110 - loss: 0.8701 - val_accuracy: 0.5961 - val_loss: 0.8862\n",
      "Epoch 5/10\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.6240 - loss: 0.8429 - val_accuracy: 0.5866 - val_loss: 0.9086\n",
      "Epoch 6/10\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.6266 - loss: 0.8382 - val_accuracy: 0.5945 - val_loss: 0.8840\n",
      "Epoch 7/10\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 39ms/step - accuracy: 0.6342 - loss: 0.8117 - val_accuracy: 0.5952 - val_loss: 0.8851\n",
      "Epoch 8/10\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.6535 - loss: 0.7944 - val_accuracy: 0.5899 - val_loss: 0.8891\n",
      "Epoch 9/10\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 44ms/step - accuracy: 0.6656 - loss: 0.7680 - val_accuracy: 0.5965 - val_loss: 0.8921\n",
      "Epoch 10/10\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.6678 - loss: 0.7584 - val_accuracy: 0.5998 - val_loss: 0.8974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib\n",
    "\n",
    "MAX_LEN = 100\n",
    "MAX_WORDS = 10000\n",
    "EMBED_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "GLOVE_PATH = \"/Users/sandyajaleshkumar/Desktop/Practicum/glove.6B/glove.6B.100d.txt\"  \n",
    "\n",
    "train_df = pd.read_csv(\"./splits/hatexplain_train.csv\")\n",
    "val_df   = pd.read_csv(\"./splits/hatexplain_val.csv\")\n",
    "texts    = pd.concat([train_df[\"clean_text\"], val_df[\"clean_text\"]]).tolist()\n",
    "labels   = pd.concat([train_df[\"label\"], val_df[\"label\"]]).astype(int).tolist()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
    "y = to_categorical(labels, num_classes=3)\n",
    "\n",
    "# Save tokenizer\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "joblib.dump(tokenizer, \"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_PATH, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word, vec = values[0], np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = vec\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_WORDS and word in embeddings_index:\n",
    "        embedding_matrix[i] = embeddings_index[word]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, stratify=np.argmax(y, axis=1), random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(len(word_index) + 1, EMBED_DIM, weights=[embedding_matrix], input_length=MAX_LEN, trainable=False),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "model.save(\"./models/hatexplain_glove_bilstm.h5\")\n",
    "print(\"Model and tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "1887355f-79e7-4ae6-8dca-5d17d6e5b06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings: 100%|███████████| 2015/2015 [01:43<00:00, 19.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: hatexplain_bert_embed_test.npy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "DATA_PATH = \"./splits\"\n",
    "SAVE_PATH = \"./models\"\n",
    "MAX_LEN = 100\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"hatexplain_test.csv\"))\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Generating BERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_LEN)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            token_vecs = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            embeddings.append(token_vecs)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "embeddings = get_bert_embeddings(texts)\n",
    "np.save(os.path.join(SAVE_PATH, \"hatexplain_bert_embed_test.npy\"), embeddings)\n",
    "print(\" Saved: hatexplain_bert_embed_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "d5b0631e-f34d-4ba8-a057-2ddce507b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
      "\n",
      "=== HATEXPLAIN TEST: Soft Voting Ensemble (SVC + GloVe + BERT) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6296    0.7990    0.7042       821\n",
      "           1     0.5406    0.3244    0.4055       595\n",
      "           2     0.6591    0.6778    0.6683       599\n",
      "\n",
      "    accuracy                         0.6228      2015\n",
      "   macro avg     0.6098    0.6004    0.5927      2015\n",
      "weighted avg     0.6121    0.6228    0.6053      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "# TF-IDF + SVC\n",
    "svc = joblib.load(\"./models/hatexplain_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "# GloVe + BiLSTM \n",
    "glove_model = load_model(\"./models/hatexplain_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "#BERT + BiLSTM \n",
    "bert_model = load_model(\"./models/hatexplain_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/hatexplain_bert_embed_test.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "avg_probs = (svc_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Soft Voting Ensemble (SVC + GloVe + BERT) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "68396542-e340-4c1e-90be-3128720038fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN: Weighted Soft Voting Ensemble ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6099    0.8112    0.6963       821\n",
      "           1     0.5672    0.2555    0.3523       595\n",
      "           2     0.6458    0.7062    0.6746       599\n",
      "\n",
      "    accuracy                         0.6159      2015\n",
      "   macro avg     0.6076    0.5909    0.5744      2015\n",
      "weighted avg     0.6079    0.6159    0.5883      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1, w2, w3 = 0.1, 0.1, 0.8 \n",
    "final_probs = w1 * svc_probs + w2 * glove_probs + w3 * bert_probs\n",
    "y_pred = np.argmax(final_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN: Weighted Soft Voting Ensemble ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cb3cd953-745f-42e8-9a23-59515d2b55fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN TEST: Stacking Ensemble (Logistic Regression) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6415    0.7759    0.7023       821\n",
      "           1     0.5106    0.3630    0.4244       595\n",
      "           2     0.6694    0.6694    0.6694       599\n",
      "\n",
      "    accuracy                         0.6223      2015\n",
      "   macro avg     0.6072    0.6028    0.5987      2015\n",
      "weighted avg     0.6112    0.6223    0.6105      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_stack = np.hstack([\n",
    "    svc_probs,\n",
    "    glove_probs,\n",
    "    bert_probs\n",
    "]) \n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=500)\n",
    "meta_clf.fit(X_stack, y_true)\n",
    "y_pred_stack = meta_clf.predict(X_stack)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Stacking Ensemble (Logistic Regression) ===\")\n",
    "print(classification_report(y_true, y_pred_stack, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0abae2-e8bc-4036-9fb6-f228951e83a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Oversampling Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b88ce1d2-7530-42b8-9616-c6904e95b144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EDOS_OS_SVC (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8736    0.8494    0.8614      1514\n",
      "           1     0.5682    0.6173    0.5917       486\n",
      "\n",
      "    accuracy                         0.7930      2000\n",
      "   macro avg     0.7209    0.7333    0.7265      2000\n",
      "weighted avg     0.7994    0.7930    0.7958      2000\n",
      "\n",
      "\n",
      "--- EDOS_OS_LR (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8887    0.8487    0.8682      1514\n",
      "           1     0.5866    0.6687    0.6250       486\n",
      "\n",
      "    accuracy                         0.8050      2000\n",
      "   macro avg     0.7377    0.7587    0.7466      2000\n",
      "weighted avg     0.8153    0.8050    0.8091      2000\n",
      "\n",
      "\n",
      "--- EDOS_OS_RF (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8350    0.9590    0.8927      1514\n",
      "           1     0.7625    0.4095    0.5328       486\n",
      "\n",
      "    accuracy                         0.8255      2000\n",
      "   macro avg     0.7987    0.6843    0.7128      2000\n",
      "weighted avg     0.8173    0.8255    0.8053      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EDOS_OS_XGB (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8785    0.8593    0.8688      1514\n",
      "           1     0.5896    0.6296    0.6090       486\n",
      "\n",
      "    accuracy                         0.8035      2000\n",
      "   macro avg     0.7340    0.7445    0.7389      2000\n",
      "weighted avg     0.8083    0.8035    0.8056      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib, os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "edos_train = pd.read_csv(\"./splits/edos_train.csv\")\n",
    "edos_val   = pd.read_csv(\"./splits/edos_val.csv\")\n",
    "\n",
    "X_train_raw, y_train_raw = edos_train[\"clean_text\"], edos_train[\"label\"]\n",
    "X_val, y_val             = edos_val[\"clean_text\"],   edos_val[\"label\"]\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_raw)\n",
    "X_val_vec   = vectorizer.transform(X_val)\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_os, y_train_os = ros.fit_resample(X_train_vec, y_train_raw)\n",
    "\n",
    "classifiers = {\n",
    "    \"edos_os_svc\": LinearSVC(),\n",
    "    \"edos_os_lr\":  LogisticRegression(max_iter=1000),\n",
    "    \"edos_os_rf\":  RandomForestClassifier(n_estimators=100),\n",
    "    \"edos_os_xgb\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_os, y_train_os)\n",
    "    y_pred = clf.predict(X_val_vec)\n",
    "    print(f\"\\n--- {name.upper()} (Oversampled) ---\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "    joblib.dump((vectorizer, clf), f\"./models/{name}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3f984f56-b774-4361-a113-e248f43a6c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DAVIDSON_OS_SVC (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3122    0.4825    0.3791       143\n",
      "           1     0.9343    0.8812    0.9069      1919\n",
      "           2     0.7785    0.8365    0.8065       416\n",
      "\n",
      "    accuracy                         0.8507      2478\n",
      "   macro avg     0.6750    0.7334    0.6975      2478\n",
      "weighted avg     0.8722    0.8507    0.8596      2478\n",
      "\n",
      "\n",
      "--- DAVIDSON_OS_LR (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3451    0.5455    0.4228       143\n",
      "           1     0.9561    0.8744    0.9134      1919\n",
      "           2     0.7606    0.9087    0.8280       416\n",
      "\n",
      "    accuracy                         0.8612      2478\n",
      "   macro avg     0.6873    0.7762    0.7214      2478\n",
      "weighted avg     0.8880    0.8612    0.8708      2478\n",
      "\n",
      "\n",
      "--- DAVIDSON_OS_RF (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5536    0.2168    0.3116       143\n",
      "           1     0.9117    0.9526    0.9317      1919\n",
      "           2     0.8010    0.8029    0.8019       416\n",
      "\n",
      "    accuracy                         0.8850      2478\n",
      "   macro avg     0.7554    0.6574    0.6817      2478\n",
      "weighted avg     0.8725    0.8850    0.8741      2478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:51:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DAVIDSON_OS_XGB (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3347    0.5874    0.4264       143\n",
      "           1     0.9692    0.8541    0.9080      1919\n",
      "           2     0.7351    0.9471    0.8277       416\n",
      "\n",
      "    accuracy                         0.8543      2478\n",
      "   macro avg     0.6797    0.7962    0.7207      2478\n",
      "weighted avg     0.8933    0.8543    0.8668      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib, os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "davidson_train = pd.read_csv(\"./splits/davidson_train.csv\")\n",
    "davidson_val   = pd.read_csv(\"./splits/davidson_val.csv\")\n",
    "X_train_raw, y_train_raw = davidson_train[\"clean_text\"], davidson_train[\"label\"]\n",
    "X_val, y_val             = davidson_val[\"clean_text\"],   davidson_val[\"label\"]\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "# TF-IDF vectorizer (fit on train only)\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_raw)\n",
    "X_val_vec   = vectorizer.transform(X_val)\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_os, y_train_os = ros.fit_resample(X_train_vec, y_train_raw)\n",
    "\n",
    "classifiers = {\n",
    "    \"davidson_os_svc\": LinearSVC(),\n",
    "    \"davidson_os_lr\":  LogisticRegression(max_iter=1000),\n",
    "    \"davidson_os_rf\":  RandomForestClassifier(n_estimators=100),\n",
    "    \"davidson_os_xgb\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_os, y_train_os)\n",
    "    y_pred = clf.predict(X_val_vec)\n",
    "    print(f\"\\n--- {name.upper()} (Oversampled) ---\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "    joblib.dump((vectorizer, clf), f\"./models/{name}.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f4ef3975-9f89-4ddd-9299-5de40ccb60b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HATEXPLAIN_OS_SVC (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6081    0.6066    0.6073       821\n",
      "           1     0.4229    0.4286    0.4257       595\n",
      "           2     0.6121    0.6060    0.6091       599\n",
      "\n",
      "    accuracy                         0.5538      2015\n",
      "   macro avg     0.5477    0.5471    0.5474      2015\n",
      "weighted avg     0.5546    0.5538    0.5542      2015\n",
      "\n",
      "\n",
      "--- HATEXPLAIN_OS_LR (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6310    0.6456    0.6382       821\n",
      "           1     0.4513    0.4521    0.4517       595\n",
      "           2     0.6563    0.6344    0.6452       599\n",
      "\n",
      "    accuracy                         0.5851      2015\n",
      "   macro avg     0.5795    0.5773    0.5784      2015\n",
      "weighted avg     0.5855    0.5851    0.5852      2015\n",
      "\n",
      "\n",
      "--- HATEXPLAIN_OS_RF (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5756    0.7649    0.6569       821\n",
      "           1     0.5158    0.3563    0.4215       595\n",
      "           2     0.6940    0.5943    0.6403       599\n",
      "\n",
      "    accuracy                         0.5935      2015\n",
      "   macro avg     0.5951    0.5718    0.5729      2015\n",
      "weighted avg     0.5931    0.5935    0.5824      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HATEXPLAIN_OS_XGB (Oversampled) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6406    0.7077    0.6725       821\n",
      "           1     0.4896    0.4756    0.4825       595\n",
      "           2     0.6943    0.6144    0.6519       599\n",
      "\n",
      "    accuracy                         0.6114      2015\n",
      "   macro avg     0.6082    0.5992    0.6023      2015\n",
      "weighted avg     0.6120    0.6114    0.6103      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib, os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "hatexplain_train = pd.read_csv(\"./splits/hatexplain_train.csv\")\n",
    "hatexplain_val   = pd.read_csv(\"./splits/hatexplain_val.csv\")\n",
    "X_train_raw, y_train_raw = hatexplain_train[\"clean_text\"], hatexplain_train[\"label\"]\n",
    "X_val, y_val             = hatexplain_val[\"clean_text\"],   hatexplain_val[\"label\"]\n",
    "\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_raw)\n",
    "X_val_vec   = vectorizer.transform(X_val)\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_os, y_train_os = ros.fit_resample(X_train_vec, y_train_raw)\n",
    "\n",
    "classifiers = {\n",
    "    \"hatexplain_os_svc\": LinearSVC(),\n",
    "    \"hatexplain_os_lr\":  LogisticRegression(max_iter=1000),\n",
    "    \"hatexplain_os_rf\":  RandomForestClassifier(n_estimators=100),\n",
    "    \"hatexplain_os_xgb\": XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_os, y_train_os)\n",
    "    y_pred = clf.predict(X_val_vec)\n",
    "    print(f\"\\n--- {name.upper()} (Oversampled) ---\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "    joblib.dump((vectorizer, clf), f\"./models/{name}.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcacb293-93a1-4774-9e14-eae7ce805571",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Embedding-Classifier results on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272cf4a6-b0d0-48f8-9071-d249b525cb42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDOS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558afda-af72-4caa-96bd-08477592a090",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Section A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1a22fd94-ffcc-4eab-9a73-8d0e096b00b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " EDOS_SVC on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8493    0.9340    0.8897      1515\n",
      "           1     0.7006    0.4825    0.5714       485\n",
      "\n",
      "    accuracy                         0.8245      2000\n",
      "   macro avg     0.7750    0.7082    0.7305      2000\n",
      "weighted avg     0.8133    0.8245    0.8125      2000\n",
      "\n",
      "\n",
      " EDOS_LR on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8226    0.9795    0.8942      1515\n",
      "           1     0.8418    0.3402    0.4846       485\n",
      "\n",
      "    accuracy                         0.8245      2000\n",
      "   macro avg     0.8322    0.6599    0.6894      2000\n",
      "weighted avg     0.8273    0.8245    0.7949      2000\n",
      "\n",
      "\n",
      " EDOS_RF on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8222    0.9921    0.8992      1515\n",
      "           1     0.9302    0.3299    0.4871       485\n",
      "\n",
      "    accuracy                         0.8315      2000\n",
      "   macro avg     0.8762    0.6610    0.6931      2000\n",
      "weighted avg     0.8484    0.8315    0.7993      2000\n",
      "\n",
      "\n",
      " EDOS_XGB on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8452    0.9696    0.9032      1515\n",
      "           1     0.8244    0.4454    0.5783       485\n",
      "\n",
      "    accuracy                         0.8425      2000\n",
      "   macro avg     0.8348    0.7075    0.7407      2000\n",
      "weighted avg     0.8402    0.8425    0.8244      2000\n",
      "\n",
      "\n",
      " EDOS_MLP on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8461    0.8455    0.8458      1515\n",
      "           1     0.5185    0.5196    0.5191       485\n",
      "\n",
      "    accuracy                         0.7665      2000\n",
      "   macro avg     0.6823    0.6826    0.6824      2000\n",
      "weighted avg     0.7667    0.7665    0.7666      2000\n",
      "\n",
      "\n",
      "=== Evaluation Summary Table ===\n",
      "   Model  Macro Precision  Macro Recall  Macro F1 Score  Accuracy\n",
      "EDOS_SVC         0.774969      0.708234        0.730543    0.8245\n",
      " EDOS_LR         0.832227      0.659872        0.689413    0.8245\n",
      " EDOS_RF         0.876221      0.660988        0.693127    0.8315\n",
      "EDOS_XGB         0.834826      0.707499        0.740740    0.8425\n",
      "EDOS_MLP         0.682311      0.682566        0.682438    0.7665\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "edos_test = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "X_test, y_test = edos_test[\"clean_text\"], edos_test[\"label\"]\n",
    "\n",
    "model_paths = {\n",
    "    \"EDOS_SVC\": \"./models/edos_svc.joblib\",\n",
    "    \"EDOS_LR\": \"./models/edos_lr.joblib\",\n",
    "    \"EDOS_RF\": \"./models/edos_rf.joblib\",\n",
    "    \"EDOS_XGB\": \"./models/edos_xgb.joblib\",\n",
    "    \"EDOS_MLP\": \"./models/edos_mlp.joblib\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, path in model_paths.items():\n",
    "    model = joblib.load(path)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n {name} on EDOS Test Set:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results.append([name, precision, recall, f1, acc])\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Model\", \"Macro Precision\", \"Macro Recall\", \"Macro F1 Score\", \"Accuracy\"]\n",
    ")\n",
    "print(\"\\n=== Evaluation Summary Table ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffb3c5-2adb-4810-a2c5-84680b61ac63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Section B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1d0cafe9-6716-455a-9abb-2e664faeb7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 431us/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GloVe-Avg + MLP on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7810    0.9696    0.8651      1515\n",
      "           1     0.6134    0.1505    0.2417       485\n",
      "\n",
      "    accuracy                         0.7710      2000\n",
      "   macro avg     0.6972    0.5601    0.5534      2000\n",
      "weighted avg     0.7403    0.7710    0.7140      2000\n",
      "\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GloVe-Emb + CNN on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8569    0.9406    0.8968      1515\n",
      "           1     0.7329    0.5093    0.6010       485\n",
      "\n",
      "    accuracy                         0.8360      2000\n",
      "   macro avg     0.7949    0.7249    0.7489      2000\n",
      "weighted avg     0.8268    0.8360    0.8251      2000\n",
      "\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      " GloVe-Emb + BiLSTM on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8614    0.9314    0.8950      1515\n",
      "           1     0.7127    0.5320    0.6092       485\n",
      "\n",
      "    accuracy                         0.8345      2000\n",
      "   macro avg     0.7871    0.7317    0.7521      2000\n",
      "weighted avg     0.8254    0.8345    0.8257      2000\n",
      "\n",
      "\n",
      "=== Summary ===\n",
      "             Model  Macro Precision  Macro Recall  Macro F1 Score  Accuracy\n",
      "   GloVe-Avg + MLP         0.697206      0.560076        0.553429    0.7710\n",
      "   GloVe-Emb + CNN         0.794911      0.724936        0.748882    0.8360\n",
      "GloVe-Emb + BiLSTM         0.787062      0.731656        0.752115    0.8345\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "MODEL_DIR = \"./models\"\n",
    "TEST_PATH = \"./splits/edos_test.csv\"\n",
    "\n",
    "edos_test = pd.read_csv(TEST_PATH)\n",
    "X_test_texts = edos_test[\"clean_text\"].astype(str).tolist()\n",
    "y_test = edos_test[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = joblib.load(f\"{MODEL_DIR}/edos_glove_tokenizer.joblib\")\n",
    "embedding_matrix = np.load(f\"{MODEL_DIR}/edos_glove_embedding_matrix.npy\")\n",
    "\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test_texts), maxlen=MAX_LEN)\n",
    "\n",
    "X_test_avg = np.array([\n",
    "    np.mean(embedding_matrix[x[x > 0]], axis=0) if len(x[x > 0]) > 0 else np.zeros(EMBEDDING_DIM)\n",
    "    for x in X_test_seq\n",
    "])\n",
    "\n",
    "model_files = {\n",
    "    \"GloVe-Avg + MLP\": f\"{MODEL_DIR}/edos_glove_mlp.h5\",\n",
    "    \"GloVe-Emb + CNN\": f\"{MODEL_DIR}/edos_glove_cnn.h5\",\n",
    "    \"GloVe-Emb + BiLSTM\": f\"{MODEL_DIR}/edos_glove_bilstm.h5\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_path in model_files.items():\n",
    "    model = load_model(model_path)\n",
    "    if \"MLP\" in model_name:\n",
    "        preds = model.predict(X_test_avg)\n",
    "    else:\n",
    "        preds = model.predict(X_test_seq)\n",
    "    y_pred = (preds > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "    print(f\"\\n {model_name} on EDOS Test Set:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results.append([model_name, precision, recall, f1, acc])\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Model\", \"Macro Precision\", \"Macro Recall\", \"Macro F1 Score\", \"Accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c6b40954-574c-4135-bb39-1c28d668fc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: tokenizer and embedding matrix for EDOS GloVe models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/edos_train.csv\")\n",
    "texts = train_df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MAX_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "GLOVE_PATH = \"/Users/sandyajaleshkumar/Desktop/Practicum/glove.6B/glove.6B.100d.txt\"\n",
    "MODEL_DIR = \"./models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word, coefs = values[0], np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "\n",
    "joblib.dump(tokenizer, f\"{MODEL_DIR}/edos_glove_tokenizer.joblib\")\n",
    "np.save(f\"{MODEL_DIR}/edos_glove_embedding_matrix.npy\", embedding_matrix)\n",
    "\n",
    "print(\" Saved: tokenizer and embedding matrix for EDOS GloVe models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a605a4c-3e95-42fe-b0fd-018a34843aeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Section C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "72fb1ead-86ac-4cbb-a0a9-4c124eb0539c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT + MLP on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8470    0.9135    0.8790      1515\n",
      "           1     0.6421    0.4845    0.5523       485\n",
      "\n",
      "    accuracy                         0.8095      2000\n",
      "   macro avg     0.7445    0.6990    0.7157      2000\n",
      "weighted avg     0.7973    0.8095    0.7998      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n",
      "\n",
      "BERT + BiLSTM on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8681    0.9426    0.9038      1515\n",
      "           1     0.7549    0.5526    0.6381       485\n",
      "\n",
      "    accuracy                         0.8480      2000\n",
      "   macro avg     0.8115    0.7476    0.7709      2000\n",
      "weighted avg     0.8406    0.8480    0.8394      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "BERT + CNN on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8831    0.9373    0.9094      1515\n",
      "           1     0.7577    0.6124    0.6773       485\n",
      "\n",
      "    accuracy                         0.8585      2000\n",
      "   macro avg     0.8204    0.7748    0.7933      2000\n",
      "weighted avg     0.8527    0.8585    0.8531      2000\n",
      "\n",
      "\n",
      " ALBERT + MLP on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8116    0.9498    0.8753      1515\n",
      "           1     0.6652    0.3113    0.4242       485\n",
      "\n",
      "    accuracy                         0.7950      2000\n",
      "   macro avg     0.7384    0.6306    0.6497      2000\n",
      "weighted avg     0.7761    0.7950    0.7659      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ALBERT + BiLSTM on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8764    0.9406    0.9074      1515\n",
      "           1     0.7594    0.5856    0.6612       485\n",
      "\n",
      "    accuracy                         0.8545      2000\n",
      "   macro avg     0.8179    0.7631    0.7843      2000\n",
      "weighted avg     0.8480    0.8545    0.8477      2000\n",
      "\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "ALBERT + CNN on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8725    0.9393    0.9046      1515\n",
      "           1     0.7507    0.5711    0.6487       485\n",
      "\n",
      "    accuracy                         0.8500      2000\n",
      "   macro avg     0.8116    0.7552    0.7767      2000\n",
      "weighted avg     0.8429    0.8500    0.8426      2000\n",
      "\n",
      "\n",
      " ELECTRA + MLP on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7791    0.9617    0.8609      1515\n",
      "           1     0.5538    0.1485    0.2341       485\n",
      "\n",
      "    accuracy                         0.7645      2000\n",
      "   macro avg     0.6665    0.5551    0.5475      2000\n",
      "weighted avg     0.7245    0.7645    0.7089      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ELECTRA + BiLSTM on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8539    0.9294    0.8900      1515\n",
      "           1     0.6952    0.5031    0.5837       485\n",
      "\n",
      "    accuracy                         0.8260      2000\n",
      "   macro avg     0.7745    0.7162    0.7369      2000\n",
      "weighted avg     0.8154    0.8260    0.8157      2000\n",
      "\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "ELECTRA + CNN on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8364    0.9109    0.8720      1515\n",
      "           1     0.6143    0.4433    0.5150       485\n",
      "\n",
      "    accuracy                         0.7975      2000\n",
      "   macro avg     0.7253    0.6771    0.6935      2000\n",
      "weighted avg     0.7825    0.7975    0.7854      2000\n",
      "\n",
      "\n",
      "=== Transformer-Based Model Summary ===\n",
      "           Model  Macro Precision  Macro Recall  Macro F1 Score  Accuracy\n",
      "      BERT + MLP         0.744539      0.699034        0.715650    0.8095\n",
      "   BERT + BiLSTM         0.811507      0.747576        0.770946    0.8480\n",
      "      BERT + CNN         0.820369      0.774832        0.793346    0.8585\n",
      "    ALBERT + MLP         0.738408      0.630588        0.649731    0.7950\n",
      " ALBERT + BiLSTM         0.817871      0.763081        0.784294    0.8545\n",
      "    ALBERT + CNN         0.811574      0.755204        0.776676    0.8500\n",
      "   ELECTRA + MLP         0.666495      0.555085        0.547502    0.7645\n",
      "ELECTRA + BiLSTM         0.774504      0.716233        0.736872    0.8260\n",
      "   ELECTRA + CNN         0.725325      0.677095        0.693504    0.7975\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "model_groups = {\n",
    "    \"BERT\": {\n",
    "        \"embed\": np.load(\"./models/edos_bert_embed_test_seq.npy\"),\n",
    "        \"mlp\": \"./models/edos_bert_mlp.joblib\",\n",
    "        \"bilstm\": \"./models/edos_bert_bilstm.h5\",\n",
    "        \"cnn\": \"./models/edos_bert_cnn.h5\"\n",
    "    },\n",
    "    \"ALBERT\": {\n",
    "        \"embed\": np.load(\"./models/edos_albert_embed_test_seq.npy\"),\n",
    "        \"mlp\": \"./models/edos_albert_mlp.joblib\",\n",
    "        \"bilstm\": \"./models/edos_albert_bilstm.h5\",\n",
    "        \"cnn\": \"./models/edos_albert_cnn.h5\"\n",
    "    },\n",
    "    \"ELECTRA\": {\n",
    "        \"embed\": np.load(\"./models/edos_electra_embed_test_seq.npy\"),\n",
    "        \"mlp\": \"./models/edos_electra_mlp.joblib\",\n",
    "        \"bilstm\": \"./models/edos_electra_bilstm.h5\",\n",
    "        \"cnn\": \"./models/edos_electra_cnn.h5\"\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, paths in model_groups.items():\n",
    "    X = paths[\"embed\"]\n",
    "\n",
    "    # MLP \n",
    "    X_avg = np.mean(X, axis=1)\n",
    "    mlp = joblib.load(paths[\"mlp\"])\n",
    "    y_pred = mlp.predict(X_avg)\n",
    "    print(f\"\\n {model_name} + MLP on EDOS Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([f\"{model_name} + MLP\", p, r, f1, acc])\n",
    "\n",
    "    # BiLSTM \n",
    "    bilstm = load_model(paths[\"bilstm\"])\n",
    "    y_pred = (bilstm.predict(X) > 0.5).astype(int).reshape(-1)\n",
    "    print(f\"\\n{model_name} + BiLSTM on EDOS Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([f\"{model_name} + BiLSTM\", p, r, f1, acc])\n",
    "\n",
    "    # CNN \n",
    "    cnn = load_model(paths[\"cnn\"])\n",
    "    y_pred = (cnn.predict(X) > 0.5).astype(int).reshape(-1)\n",
    "    print(f\"\\n{model_name} + CNN on EDOS Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([f\"{model_name} + CNN\", p, r, f1, acc])\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"Model\", \"Macro Precision\", \"Macro Recall\", \"Macro F1 Score\", \"Accuracy\"])\n",
    "print(\"\\n=== Transformer-Based Model Summary ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f66b8343-f0c9-4d6b-9f88-e2f506b60156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT embeddings (EDOS test): 100%|█| 2000/2000 [01:35<00:00, 20.97i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: edos_albert_embed_test_seq.npy with shape (2000, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_embeddings = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT embeddings (EDOS test)\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            seq_embed = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            all_embeddings.append(seq_embed)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "X_embed = get_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/edos_albert_embed_test_seq.npy\", X_embed)\n",
    "\n",
    "print(\" Saved: edos_albert_embed_test_seq.npy with shape\", X_embed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "895e1b93-cb93-40b7-b8ca-c96fc5b75b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA embeddings (EDOS test): 100%|█| 2000/2000 [01:49<00:00, 18.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: edos_electra_embed_test_seq.npy with shape (2000, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_embeddings = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA embeddings (EDOS test)\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            seq_embed = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            all_embeddings.append(seq_embed)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "X_embed = get_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/edos_electra_embed_test_seq.npy\", X_embed)\n",
    "\n",
    "print(\"Saved: edos_electra_embed_test_seq.npy with shape\", X_embed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7817c8-533e-4c65-ab50-ddb845273863",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Davidson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed0ee5-2427-422d-ad02-93bc232117f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Section A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "95ecac8f-b48d-4b58-b042-63a14c18ca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Davidson_SVC on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4688    0.2098    0.2899       143\n",
      "           1     0.9173    0.9541    0.9354      1919\n",
      "           2     0.8377    0.8417    0.8397       417\n",
      "\n",
      "    accuracy                         0.8923      2479\n",
      "   macro avg     0.7413    0.6686    0.6883      2479\n",
      "weighted avg     0.8781    0.8923    0.8820      2479\n",
      "\n",
      "\n",
      " Davidson_LR on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4865    0.1259    0.2000       143\n",
      "           1     0.9054    0.9672    0.9352      1919\n",
      "           2     0.8546    0.8034    0.8282       417\n",
      "\n",
      "    accuracy                         0.8911      2479\n",
      "   macro avg     0.7488    0.6321    0.6545      2479\n",
      "weighted avg     0.8727    0.8911    0.8748      2479\n",
      "\n",
      "\n",
      " Davidson_RF on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3529    0.0420    0.0750       143\n",
      "           1     0.8787    0.9776    0.9255      1919\n",
      "           2     0.8716    0.6835    0.7661       417\n",
      "\n",
      "    accuracy                         0.8741      2479\n",
      "   macro avg     0.7011    0.5677    0.5889      2479\n",
      "weighted avg     0.8472    0.8741    0.8496      2479\n",
      "\n",
      "\n",
      " Davidson_XGB on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5614    0.2238    0.3200       143\n",
      "           1     0.9416    0.9411    0.9414      1919\n",
      "           2     0.7877    0.9520    0.8621       417\n",
      "\n",
      "    accuracy                         0.9016      2479\n",
      "   macro avg     0.7636    0.7056    0.7078      2479\n",
      "weighted avg     0.8938    0.9016    0.8922      2479\n",
      "\n",
      "\n",
      " Davidson_MLP on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2426    0.2308    0.2366       143\n",
      "           1     0.9031    0.9229    0.9129      1919\n",
      "           2     0.8115    0.7434    0.7760       417\n",
      "\n",
      "    accuracy                         0.8528      2479\n",
      "   macro avg     0.6524    0.6324    0.6418      2479\n",
      "weighted avg     0.8496    0.8528    0.8508      2479\n",
      "\n",
      "\n",
      "=== Davidson Model Summary ===\n",
      "       Model  Macro Precision  Macro Recall  Macro F1 Score  Accuracy\n",
      "Davidson_SVC         0.741264      0.668553        0.688315  0.892295\n",
      " Davidson_LR         0.748815      0.632134        0.654477  0.891085\n",
      " Davidson_RF         0.701063      0.567668        0.588878  0.874143\n",
      "Davidson_XGB         0.763569      0.705643        0.707822  0.901573\n",
      "Davidson_MLP         0.652425      0.632350        0.641805  0.852763\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "X_test = df[\"clean_text\"]\n",
    "y_test = df[\"label\"].astype(int)\n",
    "\n",
    "model_paths = {\n",
    "    \"Davidson_SVC\": \"./models/davidson_svc.joblib\",\n",
    "    \"Davidson_LR\": \"./models/davidson_lr.joblib\",\n",
    "    \"Davidson_RF\": \"./models/davidson_rf.joblib\",\n",
    "    \"Davidson_XGB\": \"./models/davidson_xgb.joblib\",\n",
    "    \"Davidson_MLP\": \"./models/davidson_mlp.joblib\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, path in model_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        model = joblib.load(path)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"\\n {name} on Davidson Test Set:\")\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        results.append([name, precision, recall, f1, acc])\n",
    "    else:\n",
    "        print(f\" Model not found: {path}\")\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"Model\", \"Macro Precision\", \"Macro Recall\", \"Macro F1 Score\", \"Accuracy\"])\n",
    "print(\"\\n=== Davidson Model Summary ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9482de7-923c-45a6-a96f-42d3a1f29424",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Section B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ea4a58da-97ad-45c0-9848-76ec52090fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GloVe-Avg + MLP on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2973    0.0769    0.1222       143\n",
      "           1     0.8811    0.9573    0.9176      1919\n",
      "           2     0.7731    0.6619    0.7132       417\n",
      "\n",
      "    accuracy                         0.8568      2479\n",
      "   macro avg     0.6505    0.5654    0.5843      2479\n",
      "weighted avg     0.8292    0.8568    0.8373      2479\n",
      "\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GloVe-Emb + CNN on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4815    0.0909    0.1529       143\n",
      "           1     0.8999    0.9599    0.9289      1919\n",
      "           2     0.8000    0.7770    0.7883       417\n",
      "\n",
      "    accuracy                         0.8790      2479\n",
      "   macro avg     0.7271    0.6093    0.6234      2479\n",
      "weighted avg     0.8589    0.8790    0.8605      2479\n",
      "\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      " GloVe-Emb + BiLSTM on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4444    0.1958    0.2718       143\n",
      "           1     0.9199    0.9401    0.9299      1919\n",
      "           2     0.7758    0.8465    0.8096       417\n",
      "\n",
      "    accuracy                         0.8814      2479\n",
      "   macro avg     0.7134    0.6608    0.6705      2479\n",
      "weighted avg     0.8683    0.8814    0.8717      2479\n",
      "\n",
      "\n",
      "=== Davidson GloVe-Based Model Summary ===\n",
      "             Model  Macro Precision  Macro Recall  Macro F1 Score  Accuracy\n",
      "   GloVe-Avg + MLP         0.650487      0.565354        0.584328  0.856797\n",
      "   GloVe-Emb + CNN         0.727112      0.609254        0.623386  0.878983\n",
      "GloVe-Emb + BiLSTM         0.713402      0.660800        0.670458  0.881404\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "MODEL_DIR = \"./models\"\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = joblib.load(f\"{MODEL_DIR}/davidson_glove_tokenizer.joblib\")\n",
    "embedding_matrix = np.load(f\"{MODEL_DIR}/davidson_glove_embedding_matrix.npy\")\n",
    "\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN)\n",
    "\n",
    "X_test_avg = np.array([\n",
    "    np.mean(embedding_matrix[x[x > 0]], axis=0) if len(x[x > 0]) > 0 else np.zeros(EMBEDDING_DIM)\n",
    "    for x in X_test_seq\n",
    "])\n",
    "\n",
    "model_files = {\n",
    "    \"GloVe-Avg + MLP\": f\"{MODEL_DIR}/davidson_glove_mlp.h5\",\n",
    "    \"GloVe-Emb + CNN\": f\"{MODEL_DIR}/davidson_glove_cnn.h5\",\n",
    "    \"GloVe-Emb + BiLSTM\": f\"{MODEL_DIR}/davidson_glove_bilstm.h5\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_path in model_files.items():\n",
    "    model = load_model(model_path)\n",
    "    if \"MLP\" in model_name:\n",
    "        preds = model.predict(X_test_avg)\n",
    "    else:\n",
    "        preds = model.predict(X_test_seq)\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "    print(f\"\\n {model_name} on Davidson Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([model_name, precision, recall, f1, acc])\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"Model\", \"Macro Precision\", \"Macro Recall\", \"Macro F1 Score\", \"Accuracy\"])\n",
    "print(\"\\n=== Davidson GloVe-Based Model Summary ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e13041f1-b991-4705-8193-c6b8abcc0f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: davidson_glove_tokenizer.joblib and davidson_glove_embedding_matrix.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "GLOVE_PATH = \"/Users/sandyajaleshkumar/Desktop/Practicum/glove.6B/glove.6B.100d.txt\"\n",
    "MODEL_DIR = \"./models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/davidson_train.csv\")\n",
    "texts = train_df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word, coefs = values[0], np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "\n",
    "joblib.dump(tokenizer, f\"{MODEL_DIR}/davidson_glove_tokenizer.joblib\")\n",
    "np.save(f\"{MODEL_DIR}/davidson_glove_embedding_matrix.npy\", embedding_matrix)\n",
    "\n",
    "print(\" Saved: davidson_glove_tokenizer.joblib and davidson_glove_embedding_matrix.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666a07c-e81a-4b69-8696-97c3f9099391",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Section C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7f6729b8-a49a-4486-bfc6-dc73dcc8e09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT + MLP on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3953    0.1189    0.1828       143\n",
      "           1     0.8729    0.9594    0.9141      1919\n",
      "           2     0.7982    0.6259    0.7016       417\n",
      "\n",
      "    accuracy                         0.8548      2479\n",
      "   macro avg     0.6888    0.5680    0.5995      2479\n",
      "weighted avg     0.8328    0.8548    0.8362      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT + BiLSTM on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4561    0.1818    0.2600       143\n",
      "           1     0.9218    0.9646    0.9427      1919\n",
      "           2     0.8551    0.8489    0.8520       417\n",
      "\n",
      "    accuracy                         0.9000      2479\n",
      "   macro avg     0.7443    0.6651    0.6849      2479\n",
      "weighted avg     0.8837    0.9000    0.8881      2479\n",
      "\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      " BERT + CNN on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5455    0.1259    0.2045       143\n",
      "           1     0.9290    0.9484    0.9386      1919\n",
      "           2     0.7844    0.9161    0.8451       417\n",
      "\n",
      "    accuracy                         0.8955      2479\n",
      "   macro avg     0.7530    0.6635    0.6628      2479\n",
      "weighted avg     0.8826    0.8955    0.8806      2479\n",
      "\n",
      "\n",
      " ALBERT + MLP on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3279    0.1399    0.1961       143\n",
      "           1     0.8918    0.9364    0.9136      1919\n",
      "           2     0.7146    0.6906    0.7024       417\n",
      "\n",
      "    accuracy                         0.8491      2479\n",
      "   macro avg     0.6448    0.5890    0.6040      2479\n",
      "weighted avg     0.8295    0.8491    0.8367      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
      "\n",
      " ALBERT + BiLSTM on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4118    0.0490    0.0875       143\n",
      "           1     0.9141    0.9599    0.9365      1919\n",
      "           2     0.8009    0.8585    0.8287       417\n",
      "\n",
      "    accuracy                         0.8903      2479\n",
      "   macro avg     0.7089    0.6224    0.6176      2479\n",
      "weighted avg     0.8661    0.8903    0.8694      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      " ALBERT + CNN on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.1049    0.1734       143\n",
      "           1     0.9097    0.9552    0.9319      1919\n",
      "           2     0.8018    0.8345    0.8179       417\n",
      "\n",
      "    accuracy                         0.8858      2479\n",
      "   macro avg     0.7372    0.6315    0.6410      2479\n",
      "weighted avg     0.8679    0.8858    0.8689      2479\n",
      "\n",
      "\n",
      " ELECTRA + MLP on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3636    0.0280    0.0519       143\n",
      "           1     0.8341    0.9745    0.8988      1919\n",
      "           2     0.7478    0.4053    0.5257       417\n",
      "\n",
      "    accuracy                         0.8241      2479\n",
      "   macro avg     0.6485    0.4692    0.4921      2479\n",
      "weighted avg     0.7924    0.8241    0.7872      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ELECTRA + BiLSTM on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3187    0.2028    0.2479       143\n",
      "           1     0.9066    0.9510    0.9283      1919\n",
      "           2     0.8427    0.7578    0.7980       417\n",
      "\n",
      "    accuracy                         0.8754      2479\n",
      "   macro avg     0.6893    0.6372    0.6580      2479\n",
      "weighted avg     0.8619    0.8754    0.8671      2479\n",
      "\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      " ELECTRA + CNN on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.1748    0.2591       143\n",
      "           1     0.9047    0.9495    0.9265      1919\n",
      "           2     0.7831    0.7794    0.7812       417\n",
      "\n",
      "    accuracy                         0.8762      2479\n",
      "   macro avg     0.7293    0.6346    0.6556      2479\n",
      "weighted avg     0.8609    0.8762    0.8636      2479\n",
      "\n",
      "\n",
      "=== Davidson Transformer-Based Model Summary ===\n",
      "           Model  Macro Precision  Macro Recall  Macro F1 Score  Accuracy\n",
      "      BERT + MLP         0.688813      0.568045        0.599503  0.854780\n",
      "   BERT + BiLSTM         0.744342      0.665101        0.684897  0.899960\n",
      "      BERT + CNN         0.752965      0.663451        0.662769  0.895522\n",
      "    ALBERT + MLP         0.644773      0.588978        0.604030  0.849133\n",
      " ALBERT + BiLSTM         0.708934      0.622446        0.617552  0.890278\n",
      "    ALBERT + CNN         0.737174      0.631537        0.641049  0.885841\n",
      "   ELECTRA + MLP         0.648500      0.469238        0.492144  0.824123\n",
      "ELECTRA + BiLSTM         0.689318      0.637202        0.658041  0.875353\n",
      "   ELECTRA + CNN         0.729267      0.634552        0.655612  0.876160\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "model_groups = {\n",
    "    \"BERT\": {\n",
    "        \"embed\": np.load(\"./models/davidson_bert_embed_test_seq.npy\"),\n",
    "        \"mlp\": \"./models/davidson_bert_mlp.joblib\",\n",
    "        \"bilstm\": \"./models/davidson_bert_bilstm_best.h5\",\n",
    "        \"cnn\": \"./models/davidson_bert_cnn_best.h5\"\n",
    "    },\n",
    "    \"ALBERT\": {\n",
    "        \"embed\": np.load(\"./models/davidson_albert_embed_test_seq.npy\"),\n",
    "        \"mlp\": \"./models/davidson_albert_mlp.joblib\",\n",
    "        \"bilstm\": \"./models/davidson_albert_bilstm_best.h5\",\n",
    "        \"cnn\": \"./models/davidson_albert_cnn_best.h5\"\n",
    "    },\n",
    "    \"ELECTRA\": {\n",
    "        \"embed\": np.load(\"./models/davidson_electra_embed_test_seq.npy\"),\n",
    "        \"mlp\": \"./models/davidson_electra_mlp.joblib\",\n",
    "        \"bilstm\": \"./models/davidson_electra_bilstm_best.h5\",\n",
    "        \"cnn\": \"./models/davidson_electra_cnn_best.h5\"\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, paths in model_groups.items():\n",
    "    X = paths[\"embed\"]\n",
    "\n",
    "    # MLP \n",
    "    X_avg = np.mean(X, axis=1)\n",
    "    mlp = joblib.load(paths[\"mlp\"])\n",
    "    y_pred = mlp.predict(X_avg)\n",
    "    print(f\"\\n {model_name} + MLP on Davidson Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([f\"{model_name} + MLP\", p, r, f1, acc])\n",
    "\n",
    "    # BiLSTM\n",
    "    bilstm = load_model(paths[\"bilstm\"])\n",
    "    y_pred = np.argmax(bilstm.predict(X), axis=1)\n",
    "    print(f\"\\n {model_name} + BiLSTM on Davidson Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([f\"{model_name} + BiLSTM\", p, r, f1, acc])\n",
    "\n",
    "    # CNN\n",
    "    cnn = load_model(paths[\"cnn\"])\n",
    "    y_pred = np.argmax(cnn.predict(X), axis=1)\n",
    "    print(f\"\\n {model_name} + CNN on Davidson Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([f\"{model_name} + CNN\", p, r, f1, acc])\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"Model\", \"Macro Precision\", \"Macro Recall\", \"Macro F1 Score\", \"Accuracy\"])\n",
    "print(\"\\n=== Davidson Transformer-Based Model Summary ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5235249d-e8b3-40e7-82c8-5d1716fec1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT embeddings for Davidson test: 100%|█| 2479/2479 [01:56<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: davidson_albert_embed_test_seq.npy with shape (2479, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    embeddings = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT embeddings for Davidson test\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            seq_embed = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            embeddings.append(seq_embed)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "X_embed = get_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/davidson_albert_embed_test_seq.npy\", X_embed)\n",
    "print(\" Saved: davidson_albert_embed_test_seq.npy with shape\", X_embed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "79f8c841-675a-46db-afe8-5f85100c401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ELECTRA embeddings (Davidson test): 100%|█| 2479/2479 [02:20<00:00, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: davidson_electra_embed_test_seq.npy with shape (2479, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    embeddings = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ELECTRA embeddings (Davidson test)\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            seq_embed = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            embeddings.append(seq_embed)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "X_embed = get_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/davidson_electra_embed_test_seq.npy\", X_embed)\n",
    "print(\" Saved: davidson_electra_embed_test_seq.npy with shape\", X_embed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c065d9-d4ff-4077-be2c-17195b96e7d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HateXplain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2358c813-09e5-4e78-bf10-2f7b3ad51fb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Section A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "faf6129b-0f88-432e-9702-f95506f057a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " HateXplain_SVC on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6066    0.6894    0.6454       821\n",
      "           1     0.4028    0.3412    0.3694       595\n",
      "           2     0.6090    0.5876    0.5981       599\n",
      "\n",
      "    accuracy                         0.5563      2015\n",
      "   macro avg     0.5395    0.5394    0.5376      2015\n",
      "weighted avg     0.5471    0.5563    0.5499      2015\n",
      "\n",
      "\n",
      " HateXplain_LR on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6116    0.7479    0.6729       821\n",
      "           1     0.4491    0.3412    0.3878       595\n",
      "           2     0.6440    0.6010    0.6218       599\n",
      "\n",
      "    accuracy                         0.5841      2015\n",
      "   macro avg     0.5682    0.5633    0.5608      2015\n",
      "weighted avg     0.5732    0.5841    0.5735      2015\n",
      "\n",
      "\n",
      " HateXplain_RF on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5791    0.8246    0.6804       821\n",
      "           1     0.5425    0.2790    0.3685       595\n",
      "           2     0.6667    0.6010    0.6321       599\n",
      "\n",
      "    accuracy                         0.5970      2015\n",
      "   macro avg     0.5961    0.5682    0.5603      2015\n",
      "weighted avg     0.5943    0.5970    0.5739      2015\n",
      "\n",
      "\n",
      " HateXplain_XGB on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6069    0.7710    0.6792       821\n",
      "           1     0.4887    0.3630    0.4166       595\n",
      "           2     0.6811    0.6027    0.6395       599\n",
      "\n",
      "    accuracy                         0.6005      2015\n",
      "   macro avg     0.5922    0.5789    0.5784      2015\n",
      "weighted avg     0.5941    0.6005    0.5898      2015\n",
      "\n",
      "\n",
      " HateXplain_MLP on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5691    0.5615    0.5653       821\n",
      "           1     0.3540    0.3748    0.3641       595\n",
      "           2     0.5374    0.5159    0.5264       599\n",
      "\n",
      "    accuracy                         0.4928      2015\n",
      "   macro avg     0.4868    0.4841    0.4853      2015\n",
      "weighted avg     0.4962    0.4928    0.4943      2015\n",
      "\n",
      "\n",
      "=== HateXplain Model Evaluation Summary ===\n",
      "         Model  Macro Precision  Macro Recall  Macro F1 Score  Accuracy\n",
      "HateXplain_SVC         0.539473      0.539409        0.537647  0.556328\n",
      " HateXplain_LR         0.568225      0.563349        0.560804  0.584119\n",
      " HateXplain_RF         0.596093      0.568199        0.560338  0.597022\n",
      "HateXplain_XGB         0.592241      0.578902        0.578425  0.600496\n",
      "HateXplain_MLP         0.486832      0.484053        0.485261  0.492804\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "X_test = df[\"clean_text\"]\n",
    "y_test = df[\"label\"].astype(int)\n",
    "\n",
    "model_paths = {\n",
    "    \"HateXplain_SVC\": \"./models/hatexplain_svc.joblib\",\n",
    "    \"HateXplain_LR\": \"./models/hatexplain_lr.joblib\",\n",
    "    \"HateXplain_RF\": \"./models/hatexplain_rf.joblib\",\n",
    "    \"HateXplain_XGB\": \"./models/hatexplain_xgb.joblib\",\n",
    "    \"HateXplain_MLP\": \"./models/hatexplain_mlp.joblib\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, path in model_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        model = joblib.load(path)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"\\n {name} on HateXplain Test Set:\")\n",
    "        print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        results.append([name, precision, recall, f1, acc])\n",
    "    else:\n",
    "        print(f\" Model not found: {path}\")\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"Model\", \"Macro Precision\", \"Macro Recall\", \"Macro F1 Score\", \"Accuracy\"])\n",
    "print(\"\\n=== HateXplain Model Evaluation Summary ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff50258-5134-4200-a648-0ddaa6727cc5",
   "metadata": {},
   "source": [
    "### Section B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c4065a28-c0fb-40cd-a8d2-dd48305eef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GloVe-Avg + MLP on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5788    0.6663    0.6195       821\n",
      "           1     0.4513    0.3429    0.3897       595\n",
      "           2     0.5647    0.5826    0.5735       599\n",
      "\n",
      "    accuracy                         0.5459      2015\n",
      "   macro avg     0.5316    0.5306    0.5276      2015\n",
      "weighted avg     0.5370    0.5459    0.5380      2015\n",
      "\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GloVe-Emb + CNN on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5718    0.7905    0.6636       821\n",
      "           1     0.5294    0.2269    0.3176       595\n",
      "           2     0.6144    0.6411    0.6275       599\n",
      "\n",
      "    accuracy                         0.5797      2015\n",
      "   macro avg     0.5719    0.5528    0.5362      2015\n",
      "weighted avg     0.5719    0.5797    0.5507      2015\n",
      "\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      " GloVe-Emb + BiLSTM on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5067    0.6480    0.5687       821\n",
      "           1     0.4266    0.2050    0.2770       595\n",
      "           2     0.4669    0.5292    0.4961       599\n",
      "\n",
      "    accuracy                         0.4819      2015\n",
      "   macro avg     0.4667    0.4607    0.4472      2015\n",
      "weighted avg     0.4712    0.4819    0.4610      2015\n",
      "\n",
      "\n",
      "=== HateXplain GloVe-Based Model Summary ===\n",
      "             Model  Macro Precision  Macro Recall  Macro F1 Score  Accuracy\n",
      "   GloVe-Avg + MLP         0.531629      0.530585        0.527568  0.545906\n",
      "   GloVe-Emb + CNN         0.571873      0.552820        0.536232  0.579653\n",
      "GloVe-Emb + BiLSTM         0.466701      0.460749        0.447242  0.481886\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "MODEL_DIR = \"./models\"\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = joblib.load(f\"{MODEL_DIR}/hatexplain_glove_tokenizer.joblib\")\n",
    "embedding_matrix = np.load(f\"{MODEL_DIR}/hatexplain_glove_embedding_matrix.npy\")\n",
    "\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN)\n",
    "\n",
    "X_test_avg = np.array([\n",
    "    np.mean(embedding_matrix[x[x > 0]], axis=0) if len(x[x > 0]) > 0 else np.zeros(EMBEDDING_DIM)\n",
    "    for x in X_test_seq\n",
    "])\n",
    "\n",
    "model_files = {\n",
    "    \"GloVe-Avg + MLP\": f\"{MODEL_DIR}/hatexplain_glove_mlp.h5\",\n",
    "    \"GloVe-Emb + CNN\": f\"{MODEL_DIR}/hatexplain_glove_cnn.h5\",\n",
    "    \"GloVe-Emb + BiLSTM\": f\"{MODEL_DIR}/hatexplain_glove_bilstm.h5\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_path in model_files.items():\n",
    "    model = load_model(model_path)\n",
    "    if \"MLP\" in model_name:\n",
    "        preds = model.predict(X_test_avg)\n",
    "    else:\n",
    "        preds = model.predict(X_test_seq)\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "    print(f\"\\n {model_name} on HateXplain Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([model_name, p, r, f1, acc])\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"Model\", \"Macro Precision\", \"Macro Recall\", \"Macro F1 Score\", \"Accuracy\"])\n",
    "print(\"\\n=== HateXplain GloVe-Based Model Summary ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "54667fcc-e141-4667-855a-6f8cef72248c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: HateXplain tokenizer and GloVe embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_WORDS = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "GLOVE_PATH = \"/Users/sandyajaleshkumar/Desktop/Practicum/glove.6B/glove.6B.100d.txt\"\n",
    "MODEL_DIR = \"./models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/hatexplain_train.csv\")\n",
    "texts = train_df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "\n",
    "joblib.dump(tokenizer, f\"{MODEL_DIR}/hatexplain_glove_tokenizer.joblib\")\n",
    "np.save(f\"{MODEL_DIR}/hatexplain_glove_embedding_matrix.npy\", embedding_matrix)\n",
    "\n",
    "print(\" Saved: HateXplain tokenizer and GloVe embedding matrix.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709d373-e7dd-4ead-97fd-ba9136e0f364",
   "metadata": {},
   "source": [
    "### Section C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "08b231f4-158c-47d2-81c8-eb69fc4bdd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT + MLP on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5861    0.7296    0.6500       821\n",
      "           1     0.4545    0.3193    0.3751       595\n",
      "           2     0.6209    0.5960    0.6082       599\n",
      "\n",
      "    accuracy                         0.5687      2015\n",
      "   macro avg     0.5538    0.5483    0.5444      2015\n",
      "weighted avg     0.5576    0.5687    0.5564      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
      "\n",
      " BERT + BiLSTM on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6013    0.7990    0.6862       821\n",
      "           1     0.5663    0.2370    0.3341       595\n",
      "           2     0.6326    0.7129    0.6703       599\n",
      "\n",
      "    accuracy                         0.6074      2015\n",
      "   macro avg     0.6000    0.5830    0.5635      2015\n",
      "weighted avg     0.6003    0.6074    0.5775      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      " BERT + CNN on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6036    0.8124    0.6926       821\n",
      "           1     0.5081    0.2639    0.3473       595\n",
      "           2     0.6589    0.6611    0.6600       599\n",
      "\n",
      "    accuracy                         0.6055      2015\n",
      "   macro avg     0.5902    0.5791    0.5667      2015\n",
      "weighted avg     0.5918    0.6055    0.5810      2015\n",
      "\n",
      "\n",
      " ALBERT + MLP on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5577    0.7479    0.6389       821\n",
      "           1     0.5152    0.1143    0.1871       595\n",
      "           2     0.4808    0.6277    0.5445       599\n",
      "\n",
      "    accuracy                         0.5251      2015\n",
      "   macro avg     0.5179    0.4966    0.4568      2015\n",
      "weighted avg     0.5223    0.5251    0.4774      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\n",
      " ALBERT + BiLSTM on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5856    0.7747    0.6670       821\n",
      "           1     0.4569    0.2941    0.3579       595\n",
      "           2     0.6667    0.6077    0.6358       599\n",
      "\n",
      "    accuracy                         0.5831      2015\n",
      "   macro avg     0.5697    0.5588    0.5536      2015\n",
      "weighted avg     0.5717    0.5831    0.5665      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\n",
      " ALBERT + CNN on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5698    0.8551    0.6839       821\n",
      "           1     0.5219    0.2202    0.3097       595\n",
      "           2     0.6692    0.5943    0.6295       599\n",
      "\n",
      "    accuracy                         0.5901      2015\n",
      "   macro avg     0.5870    0.5565    0.5410      2015\n",
      "weighted avg     0.5852    0.5901    0.5572      2015\n",
      "\n",
      "\n",
      " ELECTRA + MLP on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5639    0.7734    0.6523       821\n",
      "           1     0.4523    0.1832    0.2608       595\n",
      "           2     0.5448    0.5893    0.5662       599\n",
      "\n",
      "    accuracy                         0.5444      2015\n",
      "   macro avg     0.5203    0.5153    0.4931      2015\n",
      "weighted avg     0.5253    0.5444    0.5111      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
      "\n",
      " ELECTRA + BiLSTM on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6066    0.7661    0.6771       821\n",
      "           1     0.4712    0.2891    0.3583       595\n",
      "           2     0.6427    0.6578    0.6502       599\n",
      "\n",
      "    accuracy                         0.5931      2015\n",
      "   macro avg     0.5735    0.5710    0.5619      2015\n",
      "weighted avg     0.5774    0.5931    0.5750      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      " ELECTRA + CNN on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5979    0.8149    0.6897       821\n",
      "           1     0.5074    0.2874    0.3670       595\n",
      "           2     0.6583    0.6144    0.6356       599\n",
      "\n",
      "    accuracy                         0.5995      2015\n",
      "   macro avg     0.5879    0.5722    0.5641      2015\n",
      "weighted avg     0.5891    0.5995    0.5783      2015\n",
      "\n",
      "\n",
      "=== HateXplain Transformer-Based Model Summary ===\n",
      "           Model  Macro Precision  Macro Recall  Macro F1 Score  Accuracy\n",
      "      BERT + MLP         0.553840      0.548306        0.544443  0.568734\n",
      "   BERT + BiLSTM         0.600047      0.582952        0.563548  0.607444\n",
      "      BERT + CNN         0.590204      0.579130        0.566657  0.605459\n",
      "    ALBERT + MLP         0.517882      0.496622        0.456840  0.525062\n",
      " ALBERT + BiLSTM         0.569740      0.558821        0.553566  0.583127\n",
      "    ALBERT + CNN         0.586963      0.556516        0.541034  0.590074\n",
      "   ELECTRA + MLP         0.520326      0.515319        0.493070  0.544417\n",
      "ELECTRA + BiLSTM         0.573510      0.570992        0.561857  0.593052\n",
      "   ELECTRA + CNN         0.587864      0.572204        0.564074  0.599504\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "model_groups = {\n",
    "    \"BERT\": {\n",
    "        \"embed\": np.load(\"./models/hatexplain_bert_embed_test_seq.npy\"),\n",
    "        \"mlp\": \"./models/hatexplain_bert_mlp.joblib\",\n",
    "        \"bilstm\": \"./models/hatexplain_bert_bilstm_best.h5\",\n",
    "        \"cnn\": \"./models/hatexplain_bert_cnn_best.h5\"\n",
    "    },\n",
    "    \"ALBERT\": {\n",
    "        \"embed\": np.load(\"./models/hatexplain_albert_embed_test_seq.npy\"),\n",
    "        \"mlp\": \"./models/hatexplain_albert_mlp.joblib\",\n",
    "        \"bilstm\": \"./models/hatexplain_albert_bilstm_best.h5\",\n",
    "        \"cnn\": \"./models/hatexplain_albert_cnn_best.h5\"\n",
    "    },\n",
    "    \"ELECTRA\": {\n",
    "        \"embed\": np.load(\"./models/hatexplain_electra_embed_test_seq.npy\"),\n",
    "        \"mlp\": \"./models/hatexplain_electra_mlp.joblib\",\n",
    "        \"bilstm\": \"./models/hatexplain_electra_bilstm_best.h5\",\n",
    "        \"cnn\": \"./models/hatexplain_electra_cnn_best.h5\"\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, paths in model_groups.items():\n",
    "    X = paths[\"embed\"]\n",
    "\n",
    "    #MLP\n",
    "    X_avg = np.mean(X, axis=1)\n",
    "    mlp = joblib.load(paths[\"mlp\"])\n",
    "    y_pred = mlp.predict(X_avg)\n",
    "    print(f\"\\n {model_name} + MLP on HateXplain Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([f\"{model_name} + MLP\", p, r, f1, acc])\n",
    "\n",
    "    # BiLSTM\n",
    "    bilstm = load_model(paths[\"bilstm\"])\n",
    "    y_pred = np.argmax(bilstm.predict(X), axis=1)\n",
    "    print(f\"\\n {model_name} + BiLSTM on HateXplain Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([f\"{model_name} + BiLSTM\", p, r, f1, acc])\n",
    "\n",
    "    # CNN\n",
    "    cnn = load_model(paths[\"cnn\"])\n",
    "    y_pred = np.argmax(cnn.predict(X), axis=1)\n",
    "    print(f\"\\n {model_name} + CNN on HateXplain Test Set:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    results.append([f\"{model_name} + CNN\", p, r, f1, acc])\n",
    "\n",
    "summary_df = pd.DataFrame(results, columns=[\"Model\", \"Macro Precision\", \"Macro Recall\", \"Macro F1 Score\", \"Accuracy\"])\n",
    "print(\"\\n=== HateXplain Transformer-Based Model Summary ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0544fba0-cac3-4384-bd90-8e85d6d0516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALBERT embeddings (HateXplain): 100%|███████| 2015/2015 [01:42<00:00, 19.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved hatexplain_albert_embed_test_seq.npy (2015, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    device = next(model.parameters()).device\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"ALBERT embeddings (HateXplain)\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.squeeze(0).cpu().numpy())  \n",
    "    return np.array(embeddings)\n",
    "\n",
    "X_embed = get_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/hatexplain_albert_embed_test_seq.npy\", X_embed)\n",
    "print(\" Saved hatexplain_albert_embed_test_seq.npy\", X_embed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7d53461a-fb5d-476a-b341-c093921f16a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ELECTRA embeddings (HateXplain): 100%|██████| 2015/2015 [01:54<00:00, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved hatexplain_electra_embed_test_seq.npy (2015, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"google/electra-base-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    device = next(model.parameters()).device\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"ELECTRA embeddings (HateXplain)\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.squeeze(0).cpu().numpy())  \n",
    "    return np.array(embeddings)\n",
    "\n",
    "X_embed = get_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/hatexplain_electra_embed_test_seq.npy\", X_embed)\n",
    "print(\"Saved hatexplain_electra_embed_test_seq.npy\", X_embed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3097f606-bcf8-4163-a9ff-bee15d00adc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT embeddings (HateXplain): 100%|█████████| 2015/2015 [01:37<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved hatexplain_bert_embed_test_seq.npy (2015, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    device = next(model.parameters()).device\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"BERT embeddings (HateXplain)\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.squeeze(0).cpu().numpy())  \n",
    "    return np.array(embeddings)\n",
    "\n",
    "X_embed = get_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/hatexplain_bert_embed_test_seq.npy\", X_embed)\n",
    "print(\"Saved hatexplain_bert_embed_test_seq.npy\", X_embed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdde9f1-6a32-4c04-a8d2-55f8f363ea3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Error Analysis (on ensembles based on validation results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba0969-13f1-43cd-9899-6f38136364d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2bc3b5d3-b0b7-4179-948f-ad8499da0c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS: Stacking Ensemble (Logistic Regression) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Sexist     0.8727    0.9505    0.9100      1515\n",
      "      Sexist     0.7857    0.5670    0.6587       485\n",
      "\n",
      "    accuracy                         0.8575      2000\n",
      "   macro avg     0.8292    0.7588    0.7843      2000\n",
      "weighted avg     0.8516    0.8575    0.8490      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svc_class1 = svc_probs[:, 1] if svc_probs.ndim == 2 else svc_probs\n",
    "glove_class1 = glove_probs.flatten()\n",
    "bert_class1 = bert_probs.flatten()\n",
    "\n",
    "X_stack = np.vstack([svc_class1, glove_class1, bert_class1]).T\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_stack, y_true)\n",
    "y_pred_stack = meta_clf.predict(X_stack)\n",
    "\n",
    "print(\"\\n=== EDOS: Stacking Ensemble (Logistic Regression) ===\")\n",
    "print(classification_report(y_true, y_pred_stack, target_names=[\"Non-Sexist\", \"Sexist\"], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_stack)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-Sexist\", \"Sexist\"])\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Greens\", values_format=\"d\")\n",
    "plt.title(\"EDOS Test Set – Stacking Ensemble (LogReg)\\nConfusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "b05d3c92-cbd1-46c4-b255-edc15ddf7a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2015 and the array at index 1 has size 2000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[393], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m bert_class1 \u001b[38;5;241m=\u001b[39m bert_probs\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# === Stack features and predict ===\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m X_stack \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([svc_class1, glove_class1, bert_class1])\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     13\u001b[0m meta_clf \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m     14\u001b[0m meta_clf\u001b[38;5;241m.\u001b[39mfit(X_stack, y_true)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2015 and the array at index 1 has size 2000"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svc_class1 = svc_probs[:, 1] if svc_probs.ndim == 2 else svc_probs\n",
    "glove_class1 = glove_probs.flatten()\n",
    "bert_class1 = bert_probs.flatten()\n",
    "\n",
    "X_stack = np.vstack([svc_class1, glove_class1, bert_class1]).T\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_stack, y_true)\n",
    "y_pred_stack = meta_clf.predict(X_stack)\n",
    "\n",
    "print(\"\\n=== EDOS: Stacking Ensemble (Logistic Regression) ===\")\n",
    "print(classification_report(y_true, y_pred_stack, target_names=[\"Non-Sexist\", \"Sexist\"], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_stack)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Non-Sexist\", \"Sexist\"])\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Greens\", values_format=\"d\")\n",
    "plt.title(\"EDOS Test Set – Stacking Ensemble (LogReg)\\nConfusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dfeee9fb-b8f3-408c-a0e0-efbc58628206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Exported 210 false negatives for human review.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "false_neg_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred_stack)) if true == 1 and pred == 0]\n",
    "\n",
    "false_neg_df = pd.DataFrame({\n",
    "    \"text\": [texts[i] for i in false_neg_indices],\n",
    "    \"true_label\": [y_true[i] for i in false_neg_indices],\n",
    "    \"predicted_label\": [y_pred_stack[i] for i in false_neg_indices],\n",
    "    \"svc_prob\": [svc_class1[i] for i in false_neg_indices],\n",
    "    \"glove_prob\": [glove_class1[i] for i in false_neg_indices],\n",
    "    \"bert_prob\": [bert_class1[i] for i in false_neg_indices],\n",
    "})\n",
    "\n",
    "false_neg_df.to_csv(\"edos_false_negatives.csv\", index=False)\n",
    "print(f\"\\n Exported {len(false_neg_df)} false negatives for human review.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d8cdc3e5-2378-43db-b2a5-9237809e7e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tagged and saved 210 false negatives with model agreement info.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"edos_false_negatives.csv\")\n",
    "\n",
    "df[\"svc_pred\"]   = (df[\"svc_prob\"] > 0.5).astype(int)\n",
    "df[\"glove_pred\"] = (df[\"glove_prob\"] > 0.5).astype(int)\n",
    "df[\"bert_pred\"]  = (df[\"bert_prob\"] > 0.5).astype(int)\n",
    "\n",
    "df[\"models_predicted_sexist\"] = df[[\"svc_pred\", \"glove_pred\", \"bert_pred\"]].sum(axis=1)\n",
    "\n",
    "df[\"avg_model_confidence\"] = df[[\"svc_prob\", \"glove_prob\", \"bert_prob\"]].mean(axis=1)\n",
    "\n",
    "df_sorted = df.sort_values(by=\"avg_model_confidence\", ascending=True)\n",
    "\n",
    "df_sorted.to_csv(\"edos_false_negatives_tagged.csv\", index=False)\n",
    "print(f\"\\n Tagged and saved {len(df)} false negatives with model agreement info.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c3602961-5281-4fb3-89a9-8ef727bfe023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified sample (~21 rows) saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x0/50_cl34x1zg9wnnjkygh0svc0000gn/T/ipykernel_1310/452228298.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stratified_sample = df.groupby(\"models_predicted_sexist\", group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"edos_false_negatives_tagged.csv\")\n",
    "\n",
    "percent = 0.10  # 10%\n",
    "stratified_sample = df.groupby(\"models_predicted_sexist\", group_keys=False).apply(\n",
    "    lambda g: g.sample(frac=percent, random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "stratified_sample.to_csv(\"edos_false_negatives_stratified_pct.csv\", index=False)\n",
    "print(f\"Stratified sample (~{len(stratified_sample)} rows) saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "96b7b4c0-9e54-4b1a-adf9-8caaca79a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sampled 48 rows (max 20 per group) saved to edos_false_negatives_stratified_dynamic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x0/50_cl34x1zg9wnnjkygh0svc0000gn/T/ipykernel_1310/1444016650.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled = df.groupby(\"models_predicted_sexist\", group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"edos_false_negatives_tagged.csv\")\n",
    "\n",
    "sampled = df.groupby(\"models_predicted_sexist\", group_keys=False).apply(\n",
    "    lambda g: g.sample(n=min(20, len(g)), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "sampled.to_csv(\"edos_false_negatives_stratified_dynamic.csv\", index=False)\n",
    "print(f\" Sampled {len(sampled)} rows (max 20 per group) saved to edos_false_negatives_stratified_dynamic.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41623d84-3db9-46ea-8fdb-323ca883de02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DAVIDSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "61ccfcf3-e18e-4b0f-a105-63f5a245733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON TEST: Weighted Soft Voting Ensemble ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5490    0.1958    0.2887       143\n",
      "           1     0.9281    0.9687    0.9480      1919\n",
      "           2     0.8682    0.8849    0.8765       417\n",
      "\n",
      "    accuracy                         0.9100      2479\n",
      "   macro avg     0.7818    0.6831    0.7044      2479\n",
      "weighted avg     0.8962    0.9100    0.8979      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = [0.1, 0.2, 0.8]  \n",
    "avg_probs = (\n",
    "    weights[0] * svc_probs +\n",
    "    weights[1] * glove_probs +\n",
    "    weights[2] * bert_probs\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Weighted Soft Voting Ensemble ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8c62d485-b39b-4c60-a077-bcc38dff70ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "labels = [\"Hate\", \"Offensive\", \"Neither\"]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Davidson Test Set – Weighted Soft Voting Ensemble\\nConfusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "7e6497ff-fff9-4472-a539-d6aeeb4715e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 56 false negatives to davidson_false_negatives_tagged.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "false_neg_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true in [0, 1] and pred == 2]\n",
    "\n",
    "svc_class2   = svc_probs[:, 2] if svc_probs.ndim == 2 else svc_probs\n",
    "glove_class2 = glove_probs[:, 2] if glove_probs.ndim == 2 else glove_probs\n",
    "bert_class2  = bert_probs[:, 2] if bert_probs.ndim == 2 else bert_probs\n",
    "\n",
    "fn_df = pd.DataFrame({\n",
    "    \"text\": [texts[i] for i in false_neg_indices],\n",
    "    \"true_label\": [y_true[i] for i in false_neg_indices],\n",
    "    \"predicted_label\": [y_pred[i] for i in false_neg_indices],\n",
    "    \"svc_prob\": [svc_class2[i] for i in false_neg_indices],\n",
    "    \"glove_prob\": [glove_class2[i] for i in false_neg_indices],\n",
    "    \"bert_prob\": [bert_class2[i] for i in false_neg_indices],\n",
    "})\n",
    "\n",
    "fn_df[\"svc_pred\"]   = (fn_df[\"svc_prob\"] < 0.5).astype(int)\n",
    "fn_df[\"glove_pred\"] = (fn_df[\"glove_prob\"] < 0.5).astype(int)\n",
    "fn_df[\"bert_pred\"]  = (fn_df[\"bert_prob\"] < 0.5).astype(int)\n",
    "fn_df[\"models_predicted_sexist\"] = fn_df[[\"svc_pred\", \"glove_pred\", \"bert_pred\"]].sum(axis=1)\n",
    "fn_df[\"avg_model_confidence\"] = fn_df[[\"svc_prob\", \"glove_prob\", \"bert_prob\"]].mean(axis=1)\n",
    "\n",
    "fn_df.to_csv(\"davidson_false_negatives_tagged.csv\", index=False)\n",
    "print(f\" Saved {len(fn_df)} false negatives to davidson_false_negatives_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "487bf733-5343-4813-9729-6431d895dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " False Negatives by Model Agreement Level:\n",
      "models_predicted_sexist\n",
      "0    23\n",
      "1    17\n",
      "2    14\n",
      "3     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "false_neg_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true in [0, 1] and pred == 2]\n",
    "\n",
    "svc_class2   = svc_probs[:, 2] if svc_probs.ndim == 2 else svc_probs\n",
    "glove_class2 = glove_probs[:, 2] if glove_probs.ndim == 2 else glove_probs\n",
    "bert_class2  = bert_probs[:, 2] if bert_probs.ndim == 2 else bert_probs\n",
    "\n",
    "fn_df = pd.DataFrame({\n",
    "    \"text\": [texts[i] for i in false_neg_indices],\n",
    "    \"true_label\": [y_true[i] for i in false_neg_indices],\n",
    "    \"predicted_label\": [y_pred[i] for i in false_neg_indices],\n",
    "    \"svc_prob\": [svc_class2[i] for i in false_neg_indices],\n",
    "    \"glove_prob\": [glove_class2[i] for i in false_neg_indices],\n",
    "    \"bert_prob\": [bert_class2[i] for i in false_neg_indices],\n",
    "})\n",
    "\n",
    "fn_df[\"svc_pred\"]   = (fn_df[\"svc_prob\"] < 0.5).astype(int)\n",
    "fn_df[\"glove_pred\"] = (fn_df[\"glove_prob\"] < 0.5).astype(int)\n",
    "fn_df[\"bert_pred\"]  = (fn_df[\"bert_prob\"] < 0.5).astype(int)\n",
    "\n",
    "fn_df[\"models_predicted_sexist\"] = fn_df[[\"svc_pred\", \"glove_pred\", \"bert_pred\"]].sum(axis=1)\n",
    "\n",
    "print(\"\\n False Negatives by Model Agreement Level:\")\n",
    "print(fn_df[\"models_predicted_sexist\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f7020b53-a079-48b6-9089-b4bd12a34487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Types:\n",
      "true_label\n",
      "Hate         115\n",
      "Offensive     40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Model Agreement (on wrong label):\n",
      "models_predicted_final\n",
      "3    106\n",
      "2     36\n",
      "1     13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "false_neg_indices = [\n",
    "    i for i, (true, pred) in enumerate(zip(y_true, y_pred))\n",
    "    if (true == 0 and pred in [1, 2]) or  \n",
    "       (true == 1 and pred == 2)          \n",
    "]\n",
    "\n",
    "svc_class_pred   = np.argmax(svc_probs, axis=1)\n",
    "glove_class_pred = np.argmax(glove_probs, axis=1)\n",
    "bert_class_pred  = np.argmax(bert_probs, axis=1)\n",
    "\n",
    "fn_df = pd.DataFrame({\n",
    "    \"text\": [texts[i] for i in false_neg_indices],\n",
    "    \"true_label\": [y_true[i] for i in false_neg_indices],\n",
    "    \"predicted_label\": [y_pred[i] for i in false_neg_indices],\n",
    "    \"svc_prob\": [svc_probs[i][y_pred[i]] for i in false_neg_indices],\n",
    "    \"glove_prob\": [glove_probs[i][y_pred[i]] for i in false_neg_indices],\n",
    "    \"bert_prob\": [bert_probs[i][y_pred[i]] for i in false_neg_indices],\n",
    "    \"svc_pred\": [svc_class_pred[i] for i in false_neg_indices],\n",
    "    \"glove_pred\": [glove_class_pred[i] for i in false_neg_indices],\n",
    "    \"bert_pred\": [bert_class_pred[i] for i in false_neg_indices],\n",
    "})\n",
    "\n",
    "fn_df[\"models_predicted_final\"] = (\n",
    "    (fn_df[\"svc_pred\"] == fn_df[\"predicted_label\"]).astype(int) +\n",
    "    (fn_df[\"glove_pred\"] == fn_df[\"predicted_label\"]).astype(int) +\n",
    "    (fn_df[\"bert_pred\"] == fn_df[\"predicted_label\"]).astype(int)\n",
    ")\n",
    "\n",
    "fn_df[\"avg_model_confidence\"] = fn_df[[\"svc_prob\", \"glove_prob\", \"bert_prob\"]].mean(axis=1)\n",
    "\n",
    "print(\"False Negative Types:\")\n",
    "print(fn_df[\"true_label\"].value_counts().rename({0: \"Hate\", 1: \"Offensive\"}))\n",
    "print(\"\\nModel Agreement (on wrong label):\")\n",
    "print(fn_df[\"models_predicted_final\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ec536-9162-455e-b159-210bd6790ddc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "deae15bc-6589-4783-bffb-efebc836285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "labels = [\"Hate\", \"Offensive\", \"Normal\"]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"HateXplain Test Set – Weighted Soft Voting Ensemble\\nConfusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "22297399-3944-454b-872a-9ce3018bf73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "labels = [\"Hate\", \"Offensive\", \"Normal\"]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "plt.figure(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"HateXplain Test Set – Weighted Soft Voting Ensemble\\nConfusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e6f8ad08-6ba5-4fe1-b4c7-e96b116f1649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: hatexplain_confused_hate_offensive_stratified.csv\n"
     ]
    }
   ],
   "source": [
    "confused_indices = [\n",
    "    i for i, (true, pred) in enumerate(zip(y_true, y_pred))\n",
    "    if (true == 0 and pred == 1) or (true == 1 and pred == 0)\n",
    "]\n",
    "\n",
    "confused_df = pd.DataFrame({\n",
    "    \"text\": [texts[i] for i in confused_indices],\n",
    "    \"true_label\": [y_true[i] for i in confused_indices],\n",
    "    \"predicted_label\": [y_pred[i] for i in confused_indices],\n",
    "})\n",
    "\n",
    "samples = []\n",
    "for label, group in confused_df.groupby(\"true_label\"):\n",
    "    n = min(len(group), 20)\n",
    "    samples.append(resample(group, n_samples=n, random_state=42, replace=False))\n",
    "\n",
    "final_df = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "final_df.to_csv(\"hatexplain_confused_hate_offensive_stratified.csv\", index=False)\n",
    "print(\" Saved: hatexplain_confused_hate_offensive_stratified.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a3e54fa2-984e-4b5c-bb37-b33c79ccde64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " False Negatives by Model Agreement Level (HateXplain):\n",
      "models_predicted_toxic\n",
      "0    75\n",
      "1    62\n",
      "2    66\n",
      "3    29\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "false_neg_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true in [0, 1] and pred == 2]\n",
    "\n",
    "svc_class2   = svc_probs[:, 2]\n",
    "glove_class2 = glove_probs[:, 2]\n",
    "bert_class2  = bert_probs[:, 2]\n",
    "\n",
    "fn_df = pd.DataFrame({\n",
    "    \"text\": [texts[i] for i in false_neg_indices],\n",
    "    \"true_label\": [y_true[i] for i in false_neg_indices],\n",
    "    \"predicted_label\": [y_pred[i] for i in false_neg_indices],\n",
    "    \"svc_prob\": [svc_class2[i] for i in false_neg_indices],\n",
    "    \"glove_prob\": [glove_class2[i] for i in false_neg_indices],\n",
    "    \"bert_prob\": [bert_class2[i] for i in false_neg_indices],\n",
    "})\n",
    "\n",
    "fn_df[\"svc_pred\"]   = (fn_df[\"svc_prob\"] < 0.5).astype(int)\n",
    "fn_df[\"glove_pred\"] = (fn_df[\"glove_prob\"] < 0.5).astype(int)\n",
    "fn_df[\"bert_pred\"]  = (fn_df[\"bert_prob\"] < 0.5).astype(int)\n",
    "\n",
    "fn_df[\"models_predicted_toxic\"] = fn_df[[\"svc_pred\", \"glove_pred\", \"bert_pred\"]].sum(axis=1)\n",
    "\n",
    "print(\"\\n False Negatives by Model Agreement Level (HateXplain):\")\n",
    "print(fn_df[\"models_predicted_toxic\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "0d703291-af54-4546-8273-08be5caa0c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 774 misclassified rows to hatexplain_all_misclassifications_tagged.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values  # true labels\n",
    "\n",
    "svc_class2 = svc_probs[:, 2] if svc_probs.ndim == 2 else svc_probs\n",
    "glove_class2 = glove_probs[:, 2] if glove_probs.ndim == 2 else glove_probs\n",
    "bert_class2 = bert_probs[:, 2] if bert_probs.ndim == 2 else bert_probs\n",
    "\n",
    "false_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true != pred]\n",
    "\n",
    "fn_df = pd.DataFrame({\n",
    "    \"text\": [texts[i] for i in false_indices],\n",
    "    \"true_label\": [y_true[i] for i in false_indices],\n",
    "    \"predicted_label\": [y_pred[i] for i in false_indices],\n",
    "    \"svc_prob\": [svc_class2[i] for i in false_indices],\n",
    "    \"glove_prob\": [glove_class2[i] for i in false_indices],\n",
    "    \"bert_prob\": [bert_class2[i] for i in false_indices],\n",
    "})\n",
    "\n",
    "fn_df[\"svc_pred\"] = (fn_df[\"svc_prob\"] < 0.5).astype(int)\n",
    "fn_df[\"glove_pred\"] = (fn_df[\"glove_prob\"] < 0.5).astype(int)\n",
    "fn_df[\"bert_pred\"] = (fn_df[\"bert_prob\"] < 0.5).astype(int)\n",
    "\n",
    "fn_df[\"models_predicted_non_normal\"] = fn_df[[\"svc_pred\", \"glove_pred\", \"bert_pred\"]].sum(axis=1)\n",
    "fn_df[\"avg_model_confidence\"] = fn_df[[\"svc_prob\", \"glove_prob\", \"bert_prob\"]].mean(axis=1)\n",
    "\n",
    "fn_df.to_csv(\"hatexplain_all_misclassifications_tagged.csv\", index=False)\n",
    "print(f\" Saved {len(fn_df)} misclassified rows to hatexplain_all_misclassifications_tagged.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391820e-de03-49f1-adc0-4d45fc77a645",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ensembles 2.0 (on ensembles based on the test results) & files saved for error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ffad1-1272-4ffa-91b6-f6ecf1b67488",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDOS Ensemble (XGB + GloVe-CNN + BERT-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92fd9e8d-7ebe-4114-bf06-0d612d5c760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "\n",
      "=== EDOS TEST: Soft Voting Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8619    0.9683    0.9120      1515\n",
      "           1     0.8389    0.5155    0.6386       485\n",
      "\n",
      "    accuracy                         0.8585      2000\n",
      "   macro avg     0.8504    0.7419    0.7753      2000\n",
      "weighted avg     0.8563    0.8585    0.8457      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/edos_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)  # shape: (N, 2)\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_cnn.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)  \n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_cnn.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\") \n",
    "bert_probs = bert_model.predict(bert_X)  \n",
    "\n",
    "min_len = min(len(xgb_probs), len(glove_probs), len(bert_probs), len(y_true))\n",
    "xgb_probs = xgb_probs[:min_len]\n",
    "glove_probs = glove_probs[:min_len]\n",
    "bert_probs = bert_probs[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "avg_probs = (xgb_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = (avg_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== EDOS TEST: Soft Voting Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a327573e-1d35-48c1-ace2-36c1af8680b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\n",
      "=== EDOS TEST: Weighted Soft Voting (XGB + GloVe-CNN + BERT-CNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8786    0.9604    0.9177      1515\n",
      "           1     0.8256    0.5856    0.6852       485\n",
      "\n",
      "    accuracy                         0.8695      2000\n",
      "   macro avg     0.8521    0.7730    0.8014      2000\n",
      "weighted avg     0.8658    0.8695    0.8613      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/edos_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_cnn.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_cnn.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "min_len = min(len(xgb_probs), len(glove_probs), len(bert_probs), len(y_true))\n",
    "xgb_probs = xgb_probs[:min_len]\n",
    "glove_probs = glove_probs[:min_len]\n",
    "bert_probs = bert_probs[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "w1, w2, w3 = 0.2, 0.2, 0.6  \n",
    "final_probs = (w1 * xgb_probs) + (w2 * glove_probs) + (w3 * bert_probs)\n",
    "y_pred = (final_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== EDOS TEST: Weighted Soft Voting (XGB + GloVe-CNN + BERT-CNN) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6e7ee8e-42a1-478b-9380-774742cff87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS TEST: Weighted Soft Voting (XGB + GloVe-CNN + BERT-CNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8836    0.9518    0.9164      1515\n",
      "           1     0.8016    0.6082    0.6917       485\n",
      "\n",
      "    accuracy                         0.8685      2000\n",
      "   macro avg     0.8426    0.7800    0.8041      2000\n",
      "weighted avg     0.8637    0.8685    0.8619      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1, w2, w3 = 0.1, 0.1, 0.8  \n",
    "final_probs = (w1 * xgb_probs) + (w2 * glove_probs) + (w3 * bert_probs)\n",
    "y_pred = (final_probs[:, 1] > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== EDOS TEST: Weighted Soft Voting (XGB + GloVe-CNN + BERT-CNN) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da448a1-b3fc-4826-b704-3f04353106e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "=== EDOS TEST: Stacking Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8851    0.9558    0.9191      1515\n",
      "           1     0.8159    0.6124    0.6996       485\n",
      "\n",
      "    accuracy                         0.8725      2000\n",
      "   macro avg     0.8505    0.7841    0.8094      2000\n",
      "weighted avg     0.8683    0.8725    0.8659      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/edos_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "xgb_class1 = xgb_probs[:, 1]\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_cnn.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "glove_class1 = glove_probs.flatten() \n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_cnn.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "bert_class1 = bert_probs.flatten() \n",
    "\n",
    "min_len = min(len(xgb_class1), len(glove_class1), len(bert_class1), len(y_true))\n",
    "xgb_class1 = xgb_class1[:min_len]\n",
    "glove_class1 = glove_class1[:min_len]\n",
    "bert_class1 = bert_class1[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "X_meta = np.vstack([xgb_class1, glove_class1, bert_class1]).T\n",
    "\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "y_pred = meta_clf.predict(X_meta)\n",
    "\n",
    "print(\"\\n=== EDOS TEST: Stacking Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7943022e-703c-43f8-8c32-e703b890956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "=== EDOS TEST: Stacking Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8851    0.9558    0.9191      1515\n",
      "           1     0.8159    0.6124    0.6996       485\n",
      "\n",
      "    accuracy                         0.8725      2000\n",
      "   macro avg     0.8505    0.7841    0.8094      2000\n",
      "weighted avg     0.8683    0.8725    0.8659      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/edos_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "xgb_class1 = xgb_probs[:, 1]\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_cnn.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "glove_class1 = glove_probs.flatten()  \n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_cnn.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "bert_class1 = bert_probs.flatten() \n",
    "\n",
    "min_len = min(len(xgb_class1), len(glove_class1), len(bert_class1), len(y_true))\n",
    "xgb_class1 = xgb_class1[:min_len]\n",
    "glove_class1 = glove_class1[:min_len]\n",
    "bert_class1 = bert_class1[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "X_meta = np.vstack([xgb_class1, glove_class1, bert_class1]).T\n",
    "\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "y_pred = meta_clf.predict(X_meta)\n",
    "\n",
    "print(\"\\n=== EDOS TEST: Stacking Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "325a05b0-5353-4539-936c-f34bd9e688bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/edos_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "xgb_class1 = xgb_probs[:, 1]\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_cnn.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "glove_class1 = glove_probs.flatten()\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_cnn.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "bert_class1 = bert_probs.flatten()\n",
    "\n",
    "min_len = min(len(xgb_class1), len(glove_class1), len(bert_class1), len(y_true))\n",
    "xgb_class1 = xgb_class1[:min_len]\n",
    "glove_class1 = glove_class1[:min_len]\n",
    "bert_class1 = bert_class1[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "X_meta = np.vstack([xgb_class1, glove_class1, bert_class1]).T\n",
    "\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "y_pred = meta_clf.predict(X_meta)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=[\"Non-Sexist\", \"Sexist\"], columns=[\"Pred: Non-Sexist\", \"Pred: Sexist\"])\n",
    "conf_matrix_df.to_csv(\"edos_stacking_confusion_matrix.csv\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"EDOS: Stacking Ensemble Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f357f6a2-1561-4c39-a007-bfc133654b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('EDOS.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bd373c3-3155-4764-aebe-845e9026ffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "Saved false negatives analysis to edos_false_negatives_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/edos_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "xgb_class1 = xgb_probs[:, 1]\n",
    "xgb_preds = (xgb_class1 > 0.5).astype(int)\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_cnn.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X).flatten()\n",
    "glove_preds = (glove_probs > 0.5).astype(int)\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_cnn.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X).flatten()\n",
    "bert_preds = (bert_probs > 0.5).astype(int)\n",
    "\n",
    "min_len = min(len(y_true), len(xgb_class1), len(glove_probs), len(bert_probs))\n",
    "texts = texts[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "xgb_preds = xgb_preds[:min_len]\n",
    "glove_preds = glove_preds[:min_len]\n",
    "bert_preds = bert_preds[:min_len]\n",
    "\n",
    "X_meta = np.vstack([xgb_class1[:min_len], glove_probs[:min_len], bert_probs[:min_len]]).T\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "ensemble_preds = meta_clf.predict(X_meta)\n",
    "\n",
    "false_negatives = (y_true == 1) & (ensemble_preds == 0)\n",
    "\n",
    "df_fn = pd.DataFrame({\n",
    "    \"text\": np.array(texts)[false_negatives],\n",
    "    \"true_label\": y_true[false_negatives],\n",
    "    \"ensemble_pred\": ensemble_preds[false_negatives],\n",
    "    \"xgb_pred\": xgb_preds[false_negatives],\n",
    "    \"glove_pred\": glove_preds[false_negatives],\n",
    "    \"bert_pred\": bert_preds[false_negatives]\n",
    "})\n",
    "df_fn[\"models_correct\"] = df_fn[[\"xgb_pred\", \"glove_pred\", \"bert_pred\"]].sum(axis=1)\n",
    "\n",
    "df_fn.to_csv(\"edos_false_negatives_analysis.csv\", index=False)\n",
    "print(\"Saved false negatives analysis to edos_false_negatives_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbd0893c-e925-4999-81f4-c8d71f77aa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "Saved both false negatives and false positives analysis CSVs.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# === Load test data ===\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/edos_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "xgb_class1 = xgb_probs[:, 1]\n",
    "xgb_preds = (xgb_class1 > 0.5).astype(int)\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_cnn.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X).flatten()\n",
    "glove_preds = (glove_probs > 0.5).astype(int)\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_cnn.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X).flatten()\n",
    "bert_preds = (bert_probs > 0.5).astype(int)\n",
    "\n",
    "min_len = min(len(y_true), len(xgb_class1), len(glove_probs), len(bert_probs))\n",
    "texts = texts[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "xgb_preds = xgb_preds[:min_len]\n",
    "glove_preds = glove_preds[:min_len]\n",
    "bert_preds = bert_preds[:min_len]\n",
    "\n",
    "X_meta = np.vstack([xgb_class1[:min_len], glove_probs[:min_len], bert_probs[:min_len]]).T\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "ensemble_preds = meta_clf.predict(X_meta)\n",
    "\n",
    "false_negatives = (y_true == 1) & (ensemble_preds == 0)\n",
    "df_fn = pd.DataFrame({\n",
    "    \"text\": np.array(texts)[false_negatives],\n",
    "    \"true_label\": y_true[false_negatives],\n",
    "    \"ensemble_pred\": ensemble_preds[false_negatives],\n",
    "    \"xgb_pred\": xgb_preds[false_negatives],\n",
    "    \"glove_pred\": glove_preds[false_negatives],\n",
    "    \"bert_pred\": bert_preds[false_negatives]\n",
    "})\n",
    "df_fn[\"models_correct\"] = df_fn[[\"xgb_pred\", \"glove_pred\", \"bert_pred\"]].sum(axis=1)\n",
    "df_fn.to_csv(\"edos_false_negatives_analysis.csv\", index=False)\n",
    "\n",
    "false_positives = (y_true == 0) & (ensemble_preds == 1)\n",
    "df_fp = pd.DataFrame({\n",
    "    \"text\": np.array(texts)[false_positives],\n",
    "    \"true_label\": y_true[false_positives],\n",
    "    \"ensemble_pred\": ensemble_preds[false_positives],\n",
    "    \"xgb_pred\": xgb_preds[false_positives],\n",
    "    \"glove_pred\": glove_preds[false_positives],\n",
    "    \"bert_pred\": bert_preds[false_positives]\n",
    "})\n",
    "df_fp[\"models_correct\"] = df_fp[[\"xgb_pred\", \"glove_pred\", \"bert_pred\"]].sum(axis=1)\n",
    "df_fp.to_csv(\"edos_false_positives_analysis.csv\", index=False)\n",
    "\n",
    "print(\"Saved both false negatives and false positives analysis CSVs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2bcc55b-ca2a-40f1-b70a-2d0171757493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 126 Group 0 false negatives to edos_group0_false_negatives.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_fn = pd.read_csv(\"edos_false_negatives_analysis.csv\")\n",
    "\n",
    "group_0_errors = df_fn[df_fn[\"models_correct\"] == 0]\n",
    "\n",
    "group_0_errors.to_csv(\"edos_group0_false_negatives.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(group_0_errors)} Group 0 false negatives to edos_group0_false_negatives.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a3c32f1-af4d-49b0-aabb-6f051722e695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "Saved combined model error analysis to edos_model_errors.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/edos_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "xgb_class1 = xgb_probs[:, 1]\n",
    "xgb_preds = (xgb_class1 > 0.5).astype(int)\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_cnn.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X).flatten()\n",
    "glove_preds = (glove_probs > 0.5).astype(int)\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_cnn.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X).flatten()\n",
    "bert_preds = (bert_probs > 0.5).astype(int)\n",
    "\n",
    "min_len = min(len(y_true), len(xgb_class1), len(glove_probs), len(bert_probs))\n",
    "texts = texts[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "xgb_preds = xgb_preds[:min_len]\n",
    "glove_preds = glove_preds[:min_len]\n",
    "bert_preds = bert_preds[:min_len]\n",
    "\n",
    "X_meta = np.vstack([xgb_class1[:min_len], glove_probs[:min_len], bert_probs[:min_len]]).T\n",
    "meta_clf = LogisticRegression()\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "ensemble_preds = meta_clf.predict(X_meta)\n",
    "\n",
    "false_negatives = (y_true == 1) & (ensemble_preds == 0)\n",
    "false_positives = (y_true == 0) & (ensemble_preds == 1)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text\": texts,\n",
    "    \"true_label\": y_true,\n",
    "    \"ensemble_pred\": ensemble_preds,\n",
    "    \"xgb_pred\": xgb_preds,\n",
    "    \"glove_pred\": glove_preds,\n",
    "    \"bert_pred\": bert_preds\n",
    "})\n",
    "df[\"models_correct\"] = df[[\"xgb_pred\", \"glove_pred\", \"bert_pred\"]].eq(df[\"true_label\"], axis=0).sum(axis=1)\n",
    "df[\"error_type\"] = np.where(false_negatives, \"False Negative\",\n",
    "                    np.where(false_positives, \"False Positive\", \"Correct\"))\n",
    "\n",
    "error_df = df[df[\"error_type\"] != \"Correct\"]\n",
    "\n",
    "error_df.to_csv(\"edos_model_errors.csv\", index=False)\n",
    "print(\"Saved combined model error analysis to edos_model_errors.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db14c79b-8247-44da-bc2f-ad7c2806c14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cases where all models predicted incorrectly to edos_all_models_failed.csv\n"
     ]
    }
   ],
   "source": [
    "all_models_wrong = (\n",
    "    (xgb_preds != y_true) &\n",
    "    (glove_preds != y_true) &\n",
    "    (bert_preds != y_true)\n",
    ")\n",
    "\n",
    "all_wrong_df = df[all_models_wrong]\n",
    "\n",
    "all_wrong_df.to_csv(\"edos_all_models_failed.csv\", index=False)\n",
    "print(\"Saved cases where all models predicted incorrectly to edos_all_models_failed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd14b6-37b1-4faf-9189-38b5e32638dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DADVIDSON Ensemble (XGB + GloVe-CNN + BERT-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03676926-dc3a-482a-884c-301146f6744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\n",
      "=== DAVIDSON TEST: Soft Voting Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5926    0.1119    0.1882       143\n",
      "           1     0.9264    0.9640    0.9448      1919\n",
      "           2     0.8418    0.9185    0.8784       417\n",
      "\n",
      "    accuracy                         0.9072      2479\n",
      "   macro avg     0.7869    0.6648    0.6705      2479\n",
      "weighted avg     0.8929    0.9072    0.8900      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values \n",
    "\n",
    "xgb = joblib.load(\"./models/davidson_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/davidson_glove_cnn.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_cnn_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "min_len = min(len(xgb_probs), len(glove_probs), len(bert_probs), len(y_true))\n",
    "xgb_probs = xgb_probs[:min_len]\n",
    "glove_probs = glove_probs[:min_len]\n",
    "bert_probs = bert_probs[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "avg_probs = (xgb_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Soft Voting Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b032026b-252f-49a8-b91e-5204e158eefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON TEST: Weighted Soft Voting Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6071    0.1189    0.1988       143\n",
      "           1     0.9295    0.9625    0.9457      1919\n",
      "           2     0.8319    0.9257    0.8763       417\n",
      "\n",
      "    accuracy                         0.9076      2479\n",
      "   macro avg     0.7895    0.6690    0.6736      2479\n",
      "weighted avg     0.8945    0.9076    0.8910      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "w1, w2, w3 = 0.2, 0.3, 0.5\n",
    "\n",
    "weighted_probs = w1 * xgb_probs + w2 * glove_probs + w3 * bert_probs\n",
    "y_pred_weighted = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Weighted Soft Voting Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\")\n",
    "print(classification_report(y_true, y_pred_weighted, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a739330f-38cf-48ef-a3cc-172691808fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON TEST: Stacking Ensemble (Logistic Regression) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5517    0.2238    0.3184       143\n",
      "           1     0.9345    0.9588    0.9465      1919\n",
      "           2     0.8518    0.9233    0.8861       417\n",
      "\n",
      "    accuracy                         0.9104      2479\n",
      "   macro avg     0.7793    0.7020    0.7170      2479\n",
      "weighted avg     0.8985    0.9104    0.9001      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_meta = np.hstack([xgb_probs, glove_probs, bert_probs]) \n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000)\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "y_pred_stack = meta_clf.predict(X_meta)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Stacking Ensemble (Logistic Regression) ===\")\n",
    "print(classification_report(y_true, y_pred_stack, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eff4dea9-e525-4b7f-b157-37e5e23ad916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON TEST: Stacking Ensemble (Logistic Regression) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5517    0.2238    0.3184       143\n",
      "           1     0.9345    0.9588    0.9465      1919\n",
      "           2     0.8518    0.9233    0.8861       417\n",
      "\n",
      "    accuracy                         0.9104      2479\n",
      "   macro avg     0.7793    0.7020    0.7170      2479\n",
      "weighted avg     0.8985    0.9104    0.9001      2479\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_meta = np.hstack([xgb_probs, glove_probs, bert_probs])  \n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000)\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "\n",
    "y_pred_stack = meta_clf.predict(X_meta)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Stacking Ensemble (Logistic Regression) ===\")\n",
    "print(classification_report(y_true, y_pred_stack, digits=4))\n",
    "\n",
    "labels = [\"Hate (0)\", \"Offensive (1)\", \"Neither (2)\"]\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_stack)\n",
    "conf_df = pd.DataFrame(conf_matrix, index=labels, columns=[f\"Pred: {l}\" for l in labels])\n",
    "\n",
    "conf_df.to_csv(\"davidson_stacking_confusion_matrix.csv\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Davidson: Stacking Ensemble Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0a42a18-ff67-4789-b17f-8a6610ab34c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('Davidson.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29419305-aeb7-4d54-a2fc-703ad7f91532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 222 false negatives to davidson_false_negatives_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "false_negatives_idx = np.where(y_true != y_pred_stack)[0]\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "xgb_preds = np.argmax(xgb_probs, axis=1)\n",
    "glove_preds = np.argmax(glove_probs, axis=1)\n",
    "bert_preds = np.argmax(bert_probs, axis=1)\n",
    "\n",
    "fn_data = []\n",
    "for i in false_negatives_idx:\n",
    "    fn_data.append({\n",
    "        \"text\": texts[i],\n",
    "        \"true_label\": y_true[i],\n",
    "        \"ensemble_pred\": y_pred_stack[i],\n",
    "        \"xgb_pred\": xgb_preds[i],\n",
    "        \"glove_pred\": glove_preds[i],\n",
    "        \"bert_pred\": bert_preds[i],\n",
    "        \"models_correct\": sum([\n",
    "            xgb_preds[i] == y_true[i],\n",
    "            glove_preds[i] == y_true[i],\n",
    "            bert_preds[i] == y_true[i]\n",
    "        ])\n",
    "    })\n",
    "\n",
    "fn_df = pd.DataFrame(fn_data)\n",
    "\n",
    "fn_df.to_csv(\"davidson_false_negatives_analysis.csv\", index=False)\n",
    "print(f\"Saved {len(fn_df)} false negatives to davidson_false_negatives_analysis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e819fcae-b839-42a5-be14-2d29c174eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 141 group-0 hard false negatives.\n"
     ]
    }
   ],
   "source": [
    "group_0 = fn_df[fn_df[\"models_correct\"] == 0]\n",
    "group_0.to_csv(\"davidson_group0_false_negatives.csv\", index=False)\n",
    "print(f\"Saved {len(group_0)} group-0 hard false negatives.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b81a99-7ebb-4c33-8a8a-926c3dba8bf9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HateXplain Ensemble (XGB + GloVe-CNN + BERT-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43341c4f-9aa3-4f48-9590-c80c158fe890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "=== HATEXPLAIN TEST: Soft Voting Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5854    0.8392    0.6897       821\n",
      "           1     0.5253    0.2622    0.3498       595\n",
      "           2     0.6802    0.6144    0.6456       599\n",
      "\n",
      "    accuracy                         0.6020      2015\n",
      "   macro avg     0.5970    0.5719    0.5617      2015\n",
      "weighted avg     0.5958    0.6020    0.5762      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/hatexplain_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/hatexplain_glove_cnn.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/hatexplain_bert_cnn_best.h5\")\n",
    "bert_X = np.load(\"./models/hatexplain_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "min_len = min(len(xgb_probs), len(glove_probs), len(bert_probs), len(y_true))\n",
    "xgb_probs = xgb_probs[:min_len]\n",
    "glove_probs = glove_probs[:min_len]\n",
    "bert_probs = bert_probs[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "avg_probs = (xgb_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Soft Voting Ensemble (XGB + GloVe-CNN + BERT-CNN) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3259bb15-bd27-471f-8262-7ab7eae238cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\n",
      "=== HATEXPLAIN TEST: Weighted Voting (XGB + GloVe-CNN + BERT-CNN) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5976    0.8392    0.6981       821\n",
      "           1     0.5254    0.2437    0.3330       595\n",
      "           2     0.6706    0.6561    0.6633       599\n",
      "\n",
      "    accuracy                         0.6089      2015\n",
      "   macro avg     0.5979    0.5797    0.5648      2015\n",
      "weighted avg     0.5980    0.6089    0.5799      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/hatexplain_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/hatexplain_glove_cnn.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/hatexplain_bert_cnn_best.h5\")\n",
    "bert_X = np.load(\"./models/hatexplain_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "min_len = min(len(xgb_probs), len(glove_probs), len(bert_probs), len(y_true))\n",
    "xgb_probs = xgb_probs[:min_len]\n",
    "glove_probs = glove_probs[:min_len]\n",
    "bert_probs = bert_probs[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "weights = [0.1, 0.1, 0.8]\n",
    "final_probs = (\n",
    "    weights[0] * xgb_probs +\n",
    "    weights[1] * glove_probs +\n",
    "    weights[2] * bert_probs\n",
    ")\n",
    "y_pred = np.argmax(final_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Weighted Voting (XGB + GloVe-CNN + BERT-CNN) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac49f954-b4ba-49d7-a84d-31aa499f72a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN TEST: Stacking Ensemble (LogReg) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6262    0.8100    0.7063       821\n",
      "           1     0.4881    0.3445    0.4039       595\n",
      "           2     0.6904    0.6144    0.6502       599\n",
      "\n",
      "    accuracy                         0.6144      2015\n",
      "   macro avg     0.6016    0.5896    0.5868      2015\n",
      "weighted avg     0.6045    0.6144    0.6003      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_meta = np.hstack([\n",
    "    xgb_probs,\n",
    "    glove_probs,\n",
    "    bert_probs\n",
    "])\n",
    "meta_clf = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "y_pred_stack = meta_clf.predict(X_meta)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Stacking Ensemble (LogReg) ===\")\n",
    "print(classification_report(y_true, y_pred_stack, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7da49f8c-ba74-4e58-84a3-f3a04e1b8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 764 false negatives to hatexplain_false_negatives_analysis.csv\n",
      "Saved 501 hard cases (group 0) to hatexplain_group0_false_negatives.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "xgb_probs = np.load(\"hatexplain_xgb_probs.npy\")        \n",
    "glove_probs = np.load(\"hatexplain_glove_probs.npy\")    \n",
    "bert_probs = np.load(\"hatexplain_bert_probs.npy\")     \n",
    "\n",
    "min_len = min(len(xgb_probs), len(glove_probs), len(bert_probs), len(y_true))\n",
    "xgb_probs = xgb_probs[:min_len]\n",
    "glove_probs = glove_probs[:min_len]\n",
    "bert_probs = bert_probs[:min_len]\n",
    "texts = texts[:min_len]\n",
    "y_true = y_true[:min_len]\n",
    "\n",
    "X_meta = np.hstack([xgb_probs, glove_probs, bert_probs])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "meta_clf = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "y_pred_stack = meta_clf.predict(X_meta)\n",
    "\n",
    "xgb_preds = np.argmax(xgb_probs, axis=1)\n",
    "glove_preds = np.argmax(glove_probs, axis=1)\n",
    "bert_preds = np.argmax(bert_probs, axis=1)\n",
    "\n",
    "false_negatives_idx = np.where(y_true != y_pred_stack)[0]\n",
    "\n",
    "records = []\n",
    "for i in false_negatives_idx:\n",
    "    correct_count = sum([\n",
    "        xgb_preds[i] == y_true[i],\n",
    "        glove_preds[i] == y_true[i],\n",
    "        bert_preds[i] == y_true[i]\n",
    "    ])\n",
    "    records.append({\n",
    "        \"text\": texts[i],\n",
    "        \"true_label\": y_true[i],\n",
    "        \"ensemble_pred\": y_pred_stack[i],\n",
    "        \"xgb_pred\": xgb_preds[i],\n",
    "        \"glove_pred\": glove_preds[i],\n",
    "        \"bert_pred\": bert_preds[i],\n",
    "        \"models_correct\": correct_count\n",
    "    })\n",
    "\n",
    "error_df = pd.DataFrame(records)\n",
    "error_df.to_csv(\"hatexplain_false_negatives_analysis.csv\", index=False)\n",
    "print(f\"Saved {len(error_df)} false negatives to hatexplain_false_negatives_analysis.csv\")\n",
    "\n",
    "group_0 = error_df[error_df[\"models_correct\"] == 0]\n",
    "group_0.to_csv(\"hatexplain_group0_false_negatives.csv\", index=False)\n",
    "print(f\"Saved {len(group_0)} hard cases (group 0) to hatexplain_group0_false_negatives.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ee02d-b3e7-4b5b-8bd9-d90f0e361c75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HateXplain Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "2b2afaab-6cae-4e8e-b64f-4b9e187088a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 65ms/step\n",
      "\n",
      "=== HATEXPLAIN TEST: Soft Voting Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6358    0.7954    0.7067       821\n",
      "           1     0.5182    0.3345    0.4065       595\n",
      "           2     0.6639    0.6694    0.6667       599\n",
      "\n",
      "    accuracy                         0.6218      2015\n",
      "   macro avg     0.6060    0.5998    0.5933      2015\n",
      "weighted avg     0.6095    0.6218    0.6062      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb_vectorizer, xgb_model = joblib.load(\"./models/hatexplain_os_xgb.joblib\")\n",
    "xgb_X = xgb_vectorizer.transform(texts)\n",
    "xgb_probs = xgb_model.predict_proba(xgb_X)\n",
    "\n",
    "glove_model = load_model(\"./models/hatexplain_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/hatexplain_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/hatexplain_bert_embed_test.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "avg_probs = (xgb_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Soft Voting Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "76d8e0e6-f9b5-4b22-abb1-780b9088b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN TEST: Weighted Voting Ensemble ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6507    0.7759    0.7078       821\n",
      "           1     0.4978    0.3731    0.4265       595\n",
      "           2     0.6610    0.6511    0.6560       599\n",
      "\n",
      "    accuracy                         0.6199      2015\n",
      "   macro avg     0.6031    0.6000    0.5968      2015\n",
      "weighted avg     0.6086    0.6199    0.6093      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weighted_probs = (0.5 * xgb_probs) + (0.3 * glove_probs) + (0.2 * bert_probs)\n",
    "y_pred_weighted = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Weighted Voting Ensemble ===\")\n",
    "print(classification_report(y_true, y_pred_weighted, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "9ff0c5c6-55e4-4613-b1a5-8eee7430e564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN TEST: Weighted Voting Ensemble ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6177    0.7990    0.6968       821\n",
      "           1     0.5443    0.2891    0.3776       595\n",
      "           2     0.6499    0.6912    0.6699       599\n",
      "\n",
      "    accuracy                         0.6164      2015\n",
      "   macro avg     0.6040    0.5931    0.5814      2015\n",
      "weighted avg     0.6056    0.6164    0.5945      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weighted_probs = (0.3 * xgb_probs) + (0.1 * glove_probs) + (0.6 * bert_probs)\n",
    "y_pred_weighted = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Weighted Voting Ensemble ===\")\n",
    "print(classification_report(y_true, y_pred_weighted, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "4359e5e2-1fb5-40aa-a5ed-2c9ff4c85d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
      "\n",
      "=== HATEXPLAIN TEST: Stacking Ensemble (XGB + GloVe + BERT) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6412    0.7966    0.7105       821\n",
      "           1     0.4926    0.3361    0.3996       595\n",
      "           2     0.6740    0.6628    0.6684       599\n",
      "\n",
      "    accuracy                         0.6208      2015\n",
      "   macro avg     0.6026    0.5985    0.5928      2015\n",
      "weighted avg     0.6071    0.6208    0.6062      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/hatexplain_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/hatexplain_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/hatexplain_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/hatexplain_bert_embed_test.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "meta_X = np.hstack([xgb_probs, glove_probs, bert_probs])\n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "meta_clf.fit(meta_X, y_true)  \n",
    "\n",
    "y_pred = meta_clf.predict(meta_X)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Stacking Ensemble (XGB + GloVe + BERT) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "beef49bc-5677-46f3-8004-01967f99d333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "\n",
      "=== HATEXPLAIN TEST: Stacking Ensemble (XGB + GloVe + BERT) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6412    0.7966    0.7105       821\n",
      "           1     0.4926    0.3361    0.3996       595\n",
      "           2     0.6740    0.6628    0.6684       599\n",
      "\n",
      "    accuracy                         0.6208      2015\n",
      "   macro avg     0.6026    0.5985    0.5928      2015\n",
      "weighted avg     0.6071    0.6208    0.6062      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/hatexplain_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/hatexplain_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/hatexplain_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/hatexplain_bert_embed_test.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "meta_X = np.hstack([xgb_probs, glove_probs, bert_probs])\n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "meta_clf.fit(meta_X, y_true)\n",
    "y_pred = meta_clf.predict(meta_X)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Stacking Ensemble (XGB + GloVe + BERT) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "labels = [\"Normal (0)\", \"Offensive (1)\", \"Hate (2)\"]\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=[f\"Pred: {l}\" for l in labels])\n",
    "\n",
    "cm_df.to_csv(\"hatexplain_stacking_confusion_matrix.csv\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"HateXplain: Stacking Ensemble Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b099189-60a4-4c18-813a-d36d693c4a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ALBERT embeddings: 100%|█████████| 2015/2015 [01:35<00:00, 21.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved ALBERT test embeddings to ./models/hatexplain_albert_embed_test.npy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "MAX_LEN = 100\n",
    "\n",
    "def get_albert_sequence_embeddings(texts, tokenizer, model, max_len=MAX_LEN):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting ALBERT embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            seq_embeds = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_embeddings.append(seq_embeds)\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "embeds = get_albert_sequence_embeddings(texts, tokenizer, model, MAX_LEN)\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "np.save(\"./models/hatexplain_albert_embed_test.npy\", embeds)\n",
    "print(\" Saved ALBERT test embeddings to ./models/hatexplain_albert_embed_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d85a3246-b63f-48ea-9ca5-fc4df1158419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step\n",
      "\n",
      "=== HATEXPLAIN TEST: Soft Voting Ensemble (XGB + GloVe-BiLSTM + ALBERT-BiLSTM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6159    0.8124    0.7006       821\n",
      "           1     0.5296    0.3008    0.3837       595\n",
      "           2     0.6633    0.6578    0.6605       599\n",
      "\n",
      "    accuracy                         0.6154      2015\n",
      "   macro avg     0.6029    0.5903    0.5816      2015\n",
      "weighted avg     0.6045    0.6154    0.5951      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/hatexplain_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/hatexplain_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "albert_model = load_model(\"./models/hatexplain_albert_bilstm_best.h5\")\n",
    "albert_X = np.load(\"./models/hatexplain_albert_embed_test.npy\")\n",
    "albert_probs = albert_model.predict(albert_X)\n",
    "\n",
    "avg_probs = (xgb_probs + glove_probs + albert_probs) / 3\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Soft Voting Ensemble (XGB + GloVe-BiLSTM + ALBERT-BiLSTM) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc4ef579-54a0-4153-9807-1f110895dd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HATEXPLAIN TEST: Weighted Soft Voting (XGB + GloVe-BiLSTM + ALBERT-BiLSTM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6243    0.8076    0.7042       821\n",
      "           1     0.5230    0.3244    0.4004       595\n",
      "           2     0.6627    0.6461    0.6543       599\n",
      "\n",
      "    accuracy                         0.6169      2015\n",
      "   macro avg     0.6033    0.5927    0.5863      2015\n",
      "weighted avg     0.6058    0.6169    0.5997      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_xgb = 0.5\n",
    "w_glove = 0.3\n",
    "w_albert = 0.2\n",
    "\n",
    "weighted_probs = (w_xgb * xgb_probs) + (w_glove * glove_probs) + (w_albert * albert_probs)\n",
    "y_pred = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== HATEXPLAIN TEST: Weighted Soft Voting (XGB + GloVe-BiLSTM + ALBERT-BiLSTM) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40c36e-3ed4-4c9b-95bd-259308418141",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Davidson Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3822d045-d2a2-43d3-8b8f-4123c8a135e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step\n",
      "\n",
      "=== DAVIDSON TEST: Soft Voting (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.0699    0.1266       143\n",
      "           1     0.9032    0.9776    0.9389      1919\n",
      "           2     0.8682    0.8058    0.8358       417\n",
      "\n",
      "    accuracy                         0.8963      2479\n",
      "   macro avg     0.8127    0.6178    0.6338      2479\n",
      "weighted avg     0.8837    0.8963    0.8747      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb = joblib.load(\"./models/davidson_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/davidson_glove_bilstm_best.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "avg_probs = (xgb_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Soft Voting (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3b01692-7394-4633-8051-94d59d8ecd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DAVIDSON TEST: Weighted Soft Voting Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.0839    0.1491       143\n",
      "           1     0.9044    0.9766    0.9391      1919\n",
      "           2     0.8663    0.8082    0.8362       417\n",
      "\n",
      "    accuracy                         0.8967      2479\n",
      "   macro avg     0.8125    0.6229    0.6415      2479\n",
      "weighted avg     0.8843    0.8967    0.8762      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0.70, 0.68, 0.73])  \n",
    "weights = weights / weights.sum()      \n",
    "\n",
    "ensemble_probs = (\n",
    "    weights[0] * xgb_probs +\n",
    "    weights[1] * glove_probs +\n",
    "    weights[2] * bert_probs\n",
    ")\n",
    "\n",
    "y_pred = np.argmax(ensemble_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Weighted Soft Voting Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "174e08de-0bff-4375-8ab2-1b5f23897f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "\n",
      "=== DAVIDSON TEST: Stacking Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4545    0.2083    0.2857        24\n",
      "           1     0.9453    0.9524    0.9488       399\n",
      "           2     0.7952    0.9041    0.8462        73\n",
      "\n",
      "    accuracy                         0.9093       496\n",
      "   macro avg     0.7317    0.6883    0.6936       496\n",
      "weighted avg     0.8994    0.9093    0.9016       496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "xgb_model = joblib.load(\"./models/davidson_xgb.joblib\")\n",
    "xgb_probs = xgb_model.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/davidson_glove_bilstm.h5\")\n",
    "glove_tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(glove_tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "stacked_features = np.hstack([xgb_probs, glove_probs, bert_probs])\n",
    "\n",
    "X_stack_train, X_stack_val, y_stack_train, y_stack_val = train_test_split(\n",
    "    stacked_features, y_true, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000)\n",
    "meta_clf.fit(X_stack_train, y_stack_train)\n",
    "\n",
    "y_pred = meta_clf.predict(X_stack_val)\n",
    "\n",
    "print(\"\\n=== DAVIDSON TEST: Stacking Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\")\n",
    "print(classification_report(y_stack_val, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c623e-302e-44ff-b4f2-cea374763813",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## EDOS Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "53eb32a0-a33e-4dba-9c03-6657ea5d3b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "\n",
      "=== EDOS TEST: Soft Voting Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8876    0.8752    0.8814      1515\n",
      "           1     0.6265    0.6536    0.6398       485\n",
      "\n",
      "    accuracy                         0.8215      2000\n",
      "   macro avg     0.7570    0.7644    0.7606      2000\n",
      "weighted avg     0.8242    0.8215    0.8228      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "vectorizer, xgb_model = joblib.load(\"./models/edos_os_xgb.joblib\")\n",
    "xgb_X = vectorizer.transform(texts)\n",
    "xgb_probs = xgb_model.predict_proba(xgb_X)\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_bilstm.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")  \n",
    "\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "avg_probs = (xgb_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== EDOS TEST: Soft Voting Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "858d42e0-f92d-4586-9ca8-8522f493efbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EDOS TEST: Weighted Soft Voting (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8876    0.8752    0.8814      1515\n",
      "           1     0.6265    0.6536    0.6398       485\n",
      "\n",
      "    accuracy                         0.8215      2000\n",
      "   macro avg     0.7570    0.7644    0.7606      2000\n",
      "weighted avg     0.8242    0.8215    0.8228      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_xgb = 0.2\n",
    "w_glove = 0.2\n",
    "w_bert = 0.6\n",
    "\n",
    "weighted_probs = (w_xgb * xgb_probs) + (w_glove * glove_probs) + (w_bert * bert_probs)\n",
    "y_pred = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== EDOS TEST: Weighted Soft Voting (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c22688e2-8525-45d0-841b-31c61c744e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step\n",
      "\n",
      "=== EDOS TEST: Stacking Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8749    0.9512    0.9114      1515\n",
      "           1     0.7904    0.5753    0.6659       485\n",
      "\n",
      "    accuracy                         0.8600      2000\n",
      "   macro avg     0.8326    0.7632    0.7887      2000\n",
      "weighted avg     0.8544    0.8600    0.8519      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "vectorizer, xgb_model = joblib.load(\"./models/edos_os_xgb.joblib\")\n",
    "xgb_X = vectorizer.transform(texts)\n",
    "xgb_probs = xgb_model.predict_proba(xgb_X)\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_bilstm.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")  # shape: (N, 100, 768)\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "stacked_features = np.concatenate([xgb_probs, glove_probs, bert_probs], axis=1)\n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000)\n",
    "meta_clf.fit(stacked_features, y_true)\n",
    "y_pred = meta_clf.predict(stacked_features)\n",
    "\n",
    "print(\"\\n=== EDOS TEST: Stacking Ensemble (XGB + GloVe-BiLSTM + BERT-BiLSTM) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd8dc2-fabe-479f-8d9a-5e5a1c4e19f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c2758e5-926a-4e84-bd0f-ec6e83f63e7c",
   "metadata": {},
   "source": [
    "# Cross Dataset Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c5055-eb1e-4687-8876-b537a32f5dbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Davidson Ensemble on HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bb7c05e7-fb01-4db3-a712-13ba31a707f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: hatexplain_test_remapped_for_davidson.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "\n",
    "remap = {2: 0, 1: 1, 0: 2}\n",
    "df[\"label\"] = df[\"label\"].map(remap)\n",
    "\n",
    "df.to_csv(\"./splits/hatexplain_test_remapped_for_davidson.csv\", index=False)\n",
    "\n",
    "print(\"Saved: hatexplain_test_remapped_for_davidson.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e49d21b2-b5a1-4a5d-b78b-36e39b669334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT sequence embeddings: 100%|██| 2015/2015 [02:30<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: davidson_bert_embed_test_on_hatexplain.npy (shape: (2015, 100, 768) )\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test_remapped_for_davidson.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy() \n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)  \n",
    "\n",
    "bert_seq = get_bert_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/davidson_bert_embed_test_on_hatexplain.npy\", bert_seq)\n",
    "\n",
    "print(\"Saved: davidson_bert_embed_test_on_hatexplain.npy (shape:\", bert_seq.shape, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b4314-a3d9-47d2-8259-eb7819c5e877",
   "metadata": {},
   "source": [
    "### Soft Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4683973f-3eb6-4ec6-afa4-65304f8dbe73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
      "\n",
      "=== CROSS-DATASET TEST: Davidson Ensemble on HateXplain ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5257    0.3072    0.3878       599\n",
      "           1     0.3363    0.5630    0.4211       595\n",
      "           2     0.5202    0.4239    0.4671       821\n",
      "\n",
      "    accuracy                         0.4303      2015\n",
      "   macro avg     0.4607    0.4314    0.4253      2015\n",
      "weighted avg     0.4675    0.4303    0.4299      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test_remapped_for_davidson.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_svc_prob.joblib\")  \n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/davidson_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test_on_hatexplain.npy\")  \n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "avg_probs = (svc_probs + glove_probs + bert_probs) / 3\n",
    "y_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== CROSS-DATASET TEST: Davidson Ensemble on HateXplain ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6849cd27-78d7-42fd-b23d-2888d93035a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2015,3) (2478,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === Weighted Soft Voting Ensemble ===\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Example weights: SVC = 0.5, GloVe = 0.3, BERT = 0.2\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m weighted_probs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m svc_probs) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m glove_probs) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m bert_probs)\n\u001b[1;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(weighted_probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# === Evaluation ===\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2015,3) (2478,3) "
     ]
    }
   ],
   "source": [
    "weighted_probs = (0.1 * svc_probs) + (0.1 * glove_probs) + (0.8 * bert_probs)\n",
    "y_pred = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== CROSS-DATASET TEST: Weighted Soft Voting (Davidson Ensemble on HateXplain) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56057879-d5bd-4f09-b06c-f888761f76e7",
   "metadata": {},
   "source": [
    "### Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "651b018a-fabc-4c6b-aad5-c24268766e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step\n",
      "\n",
      "=== CROSS-DATASET TEST: Stacking Ensemble (Davidson Ensemble on HateXplain) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5506    0.4357    0.4865       599\n",
      "           1     0.2889    0.0218    0.0406       595\n",
      "           2     0.4699    0.8563    0.6068       821\n",
      "\n",
      "    accuracy                         0.4849      2015\n",
      "   macro avg     0.4365    0.4379    0.3780      2015\n",
      "weighted avg     0.4405    0.4849    0.4039      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test_remapped_for_davidson.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_svc_prob.joblib\")\n",
    "svc_preds = svc.predict(texts)  # shape: (n_samples,)\n",
    "\n",
    "glove_model = load_model(\"./models/davidson_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_preds = np.argmax(glove_model.predict(glove_X), axis=1)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test_on_hatexplain.npy\")\n",
    "bert_preds = np.argmax(bert_model.predict(bert_X), axis=1)\n",
    "\n",
    "meta_X = np.vstack([svc_preds, glove_preds, bert_preds]).T \n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
    "meta_clf.fit(meta_X, y_true)  \n",
    "\n",
    "y_pred = meta_clf.predict(meta_X)\n",
    "\n",
    "print(\"\\n=== CROSS-DATASET TEST: Stacking Ensemble (Davidson Ensemble on HateXplain) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b7cea-b6c8-4b7e-a919-534f40f86c90",
   "metadata": {},
   "source": [
    "### STACKING (with Probabilities) (best performing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c3baeade-144d-4428-a795-39f6c1351647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step\n",
      "\n",
      "=== STACKING (with Probabilities): Davidson Ensemble on HateXplain ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5543    0.4858    0.5178       599\n",
      "           1     0.4212    0.2605    0.3219       595\n",
      "           2     0.5152    0.7040    0.5950       821\n",
      "\n",
      "    accuracy                         0.5082      2015\n",
      "   macro avg     0.4969    0.4834    0.4782      2015\n",
      "weighted avg     0.4990    0.5082    0.4914      2015\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test_remapped_for_davidson.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)  \n",
    "\n",
    "glove_model = load_model(\"./models/davidson_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X) \n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test_on_hatexplain.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)  \n",
    "\n",
    "meta_X = np.hstack([svc_probs, glove_probs, bert_probs]) \n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
    "meta_clf.fit(meta_X, y_true)  \n",
    "\n",
    "y_pred = meta_clf.predict(meta_X)\n",
    "print(\"\\n=== STACKING (with Probabilities): Davidson Ensemble on HateXplain ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf2022-a80c-49da-9523-1e840a4ca709",
   "metadata": {},
   "source": [
    "### STACKING (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "157feb05-b99a-405c-8f1e-e48bf2a3b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
      "\n",
      " STACKING (Random Forest): Trained on Davidson-val, Tested on HateXplain\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4788    0.4157    0.4450       599\n",
      "           1     0.3234    0.4941    0.3910       595\n",
      "           2     0.5256    0.3752    0.4378       821\n",
      "\n",
      "    accuracy                         0.4223      2015\n",
      "   macro avg     0.4426    0.4283    0.4246      2015\n",
      "weighted avg     0.4520    0.4223    0.4261      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_meta_train = np.load(\"./models/davidson_meta_X_val.npy\")\n",
    "y_meta_train = np.load(\"./models/davidson_meta_y_val.npy\")\n",
    "\n",
    "meta_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "meta_clf.fit(X_meta_train, y_meta_train)\n",
    "joblib.dump(meta_clf, \"./models/davidson_meta_rf_clf.joblib\")\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test_remapped_for_davidson.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/davidson_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test_on_hatexplain.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "X_meta_test = np.hstack([svc_probs, glove_probs, bert_probs])\n",
    "\n",
    "y_pred = meta_clf.predict(X_meta_test)\n",
    "print(\"\\n STACKING (Random Forest): Trained on Davidson-val, Tested on HateXplain\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfb80a-257b-4035-9141-60565af555b5",
   "metadata": {},
   "source": [
    "### STACKING (MLPClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a93b8bda-0825-4d42-8451-9c563525475d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
      "\n",
      " STACKING (MLPClassifier): Trained on Davidson-val, Tested on HateXplain\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4512    0.3856    0.4158       599\n",
      "           1     0.3337    0.5109    0.4037       595\n",
      "           2     0.5439    0.3922    0.4558       821\n",
      "\n",
      "    accuracy                         0.4253      2015\n",
      "   macro avg     0.4429    0.4296    0.4251      2015\n",
      "weighted avg     0.4543    0.4253    0.4285      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_meta_train = np.load(\"./models/davidson_meta_X_val.npy\")\n",
    "y_meta_train = np.load(\"./models/davidson_meta_y_val.npy\")\n",
    "\n",
    "meta_clf = MLPClassifier(hidden_layer_sizes=(64,), max_iter=500, random_state=42)\n",
    "meta_clf.fit(X_meta_train, y_meta_train)\n",
    "joblib.dump(meta_clf, \"./models/davidson_meta_mlp_clf.joblib\")\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test_remapped_for_davidson.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/davidson_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test_on_hatexplain.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "X_meta_test = np.hstack([svc_probs, glove_probs, bert_probs])\n",
    "\n",
    "y_pred = meta_clf.predict(X_meta_test)\n",
    "print(\"\\n STACKING (MLPClassifier): Trained on Davidson-val, Tested on HateXplain\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e0478-5621-4581-ac05-d3a9a5580141",
   "metadata": {},
   "source": [
    "### OPTIMAL WEIGHTED VOTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a1847f04-7303-49dd-a75b-4d5be9e736ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n",
      " Best Weights (SVC, GloVe, BERT): (0.4, 0.0, 0.6) with Macro F1 = 0.7292\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "val_df = pd.read_csv(\"./splits/davidson_val.csv\")\n",
    "texts = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = val_df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/davidson_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_val.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "best_f1 = 0\n",
    "best_weights = (1/3, 1/3, 1/3)\n",
    "for w1 in np.arange(0, 1.1, 0.1):\n",
    "    for w2 in np.arange(0, 1.1 - w1, 0.1):\n",
    "        w3 = 1.0 - w1 - w2\n",
    "        if w3 < 0 or w3 > 1:\n",
    "            continue\n",
    "        weighted_probs = (w1 * svc_probs) + (w2 * glove_probs) + (w3 * bert_probs)\n",
    "        y_pred = np.argmax(weighted_probs, axis=1)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_weights = (w1, w2, w3)\n",
    "\n",
    "print(f\" Best Weights (SVC, GloVe, BERT): {best_weights} with Macro F1 = {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "079c5283-3d75-4258-9c18-dda05dc34f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
      "\n",
      " OPTIMAL WEIGHTED VOTING: Davidson Ensemble on HateXplain\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5011    0.3706    0.4261       599\n",
      "           1     0.3380    0.5630    0.4224       595\n",
      "           2     0.5353    0.3788    0.4437       821\n",
      "\n",
      "    accuracy                         0.4308      2015\n",
      "   macro avg     0.4582    0.4375    0.4307      2015\n",
      "weighted avg     0.4669    0.4308    0.4322      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test_remapped_for_davidson.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/davidson_bert_embed_test_on_hatexplain.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "weighted_probs = (0.4 * svc_probs) + (0.6 * bert_probs)\n",
    "y_pred = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "print(\"\\n OPTIMAL WEIGHTED VOTING: Davidson Ensemble on HateXplain\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488b31c-6f76-4f58-9e59-d60dd96ac29f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## HateXplain Ensemble on Davidson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4a1b00da-d60a-48cb-bb11-322969c1d547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: davidson_test_remapped_for_hatexplain.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "\n",
    "remap = {0: 2, 1: 1, 2: 0}\n",
    "df[\"label\"] = df[\"label\"].map(remap)\n",
    "\n",
    "df.to_csv(\"./splits/davidson_test_remapped_for_hatexplain.csv\", index=False)\n",
    "\n",
    "print(\" Saved: davidson_test_remapped_for_hatexplain.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3d5fbe27-58e2-4bc4-a0fd-7ac0f75b3ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n",
      "\n",
      "=== CROSS-DATASET TEST: HateXplain Ensemble on Davidson ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2498    0.8777    0.3889       417\n",
      "           1     0.9000    0.4268    0.5790      1919\n",
      "           2     0.3462    0.2517    0.2915       143\n",
      "\n",
      "    accuracy                         0.4925      2479\n",
      "   macro avg     0.4987    0.5187    0.4198      2479\n",
      "weighted avg     0.7587    0.4925    0.5304      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/davidson_test_remapped_for_hatexplain.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/hatexplain_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/hatexplain_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/hatexplain_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/hatexplain_bert_embed_on_davidson.npy\") \n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "weighted_probs = (0.8 * svc_probs) + (0.3 * glove_probs) + (0.2 * bert_probs)\n",
    "y_pred = np.argmax(weighted_probs, axis=1)\n",
    "\n",
    "print(\"\\n=== CROSS-DATASET TEST: HateXplain Ensemble on Davidson ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9f7ee3cc-5dfd-4d38-83d0-b53057f42cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT sequence embeddings: 100%|██| 2479/2479 [02:01<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: ./models/hatexplain_bert_embed_on_davidson.npy with shape (2479, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test_remapped_for_hatexplain.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_seq = []\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "bert_embed = get_bert_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/hatexplain_bert_embed_on_davidson.npy\", bert_embed)\n",
    "\n",
    "print(\" Saved: ./models/hatexplain_bert_embed_on_davidson.npy with shape\", bert_embed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7a50d5e6-b113-4a6a-9227-ad3e8cbdfddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n",
      "\n",
      " STACKING (Probabilities): HateXplain-Trained Ensemble on Davidson\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6642    0.2182    0.3285       417\n",
      "           1     0.8089    0.9729    0.8834      1919\n",
      "           2     0.5000    0.1189    0.1921       143\n",
      "\n",
      "    accuracy                         0.7967      2479\n",
      "   macro avg     0.6577    0.4367    0.4680      2479\n",
      "weighted avg     0.7668    0.7967    0.7502      2479\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test_remapped_for_hatexplain.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/hatexplain_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)\n",
    "\n",
    "glove_model = load_model(\"./models/hatexplain_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "\n",
    "bert_model = load_model(\"./models/hatexplain_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/hatexplain_bert_embed_on_davidson.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "\n",
    "X_meta = np.hstack([svc_probs, glove_probs, bert_probs]) \n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "y_pred = meta_clf.predict(X_meta)\n",
    "\n",
    "print(\"\\n STACKING (Probabilities): HateXplain-Trained Ensemble on Davidson\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d979ca7-0ead-45f2-b70e-2e5ed5e2772e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training on Davidson to test on EDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5b45b34b-a49d-4232-a050-debd5f2eb150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Davidson binary files saved:\n",
      " - davidson_train_binary.csv\n",
      " - davidson_val_binary.csv\n",
      " - davidson_test_binary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "binary_map = {0: 1, 1: 1, 2: 0}\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/davidson_train.csv\")\n",
    "train_df[\"label\"] = train_df[\"label\"].map(binary_map)\n",
    "train_df.to_csv(\"./splits/davidson_train_binary.csv\", index=False)\n",
    "\n",
    "val_df = pd.read_csv(\"./splits/davidson_val.csv\")\n",
    "val_df[\"label\"] = val_df[\"label\"].map(binary_map)\n",
    "val_df.to_csv(\"./splits/davidson_val_binary.csv\", index=False)\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "test_df[\"label\"] = test_df[\"label\"].map(binary_map)\n",
    "test_df.to_csv(\"./splits/davidson_test_binary.csv\", index=False)\n",
    "\n",
    "print(\" Davidson binary files saved:\")\n",
    "print(\" - davidson_train_binary.csv\")\n",
    "print(\" - davidson_val_binary.csv\")\n",
    "print(\" - davidson_test_binary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a282d762-7bd0-4506-bd37-280201e19390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TF-IDF + SVC (Binary):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8402    0.8341    0.8372       416\n",
      "           1     0.9666    0.9680    0.9673      2062\n",
      "\n",
      "    accuracy                         0.9455      2478\n",
      "   macro avg     0.9034    0.9011    0.9022      2478\n",
      "weighted avg     0.9454    0.9455    0.9454      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/davidson_train_binary.csv\")\n",
    "val_df = pd.read_csv(\"./splits/davidson_val_binary.csv\")\n",
    "X_train, y_train = train_df[\"clean_text\"], train_df[\"label\"]\n",
    "X_val, y_val = val_df[\"clean_text\"], val_df[\"label\"]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "    (\"svc\", SVC(probability=True, kernel=\"linear\", C=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "joblib.dump(pipeline, \"./models/davidson_binary_svc_prob.joblib\")\n",
    "print(\"\\n TF-IDF + SVC (Binary):\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4fcf5096-c295-4439-bb0f-194c05d73a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved remapped binary Davidson test set: davidson_test_binary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/davidson_test.csv\")\n",
    "\n",
    "binary_map = {0: 1, 1: 1, 2: 0}\n",
    "test_df[\"label\"] = test_df[\"label\"].map(binary_map)\n",
    "\n",
    "test_df.to_csv(\"./splits/davidson_test_binary.csv\", index=False)\n",
    "print(\"Saved remapped binary Davidson test set: davidson_test_binary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "abf780a4-ef16-4ae3-ba07-7860c1426b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF + SVC (Binary) on Davidson Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8582    0.8561    0.8571       417\n",
      "           1     0.9709    0.9714    0.9712      2062\n",
      "\n",
      "    accuracy                         0.9520      2479\n",
      "   macro avg     0.9145    0.9138    0.9141      2479\n",
      "weighted avg     0.9520    0.9520    0.9520      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/davidson_test_binary.csv\")\n",
    "X_test, y_test = test_df[\"clean_text\"], test_df[\"label\"]\n",
    "\n",
    "pipeline = joblib.load(\"./models/davidson_binary_svc_prob.joblib\")\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"\\nTF-IDF + SVC (Binary) on Davidson Test:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "60b16fb9-c4f4-4ef9-9bf0-06a47021aab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 - 21s - 33ms/step - accuracy: 0.8984 - loss: 0.2419 - val_accuracy: 0.9205 - val_loss: 0.1873\n",
      "Epoch 2/10\n",
      "620/620 - 21s - 34ms/step - accuracy: 0.9253 - loss: 0.1691 - val_accuracy: 0.9262 - val_loss: 0.1683\n",
      "Epoch 3/10\n",
      "620/620 - 21s - 34ms/step - accuracy: 0.9338 - loss: 0.1516 - val_accuracy: 0.9282 - val_loss: 0.1624\n",
      "Epoch 4/10\n",
      "620/620 - 21s - 34ms/step - accuracy: 0.9426 - loss: 0.1382 - val_accuracy: 0.9346 - val_loss: 0.1581\n",
      "Epoch 5/10\n",
      "620/620 - 21s - 34ms/step - accuracy: 0.9473 - loss: 0.1280 - val_accuracy: 0.9358 - val_loss: 0.1564\n",
      "Epoch 6/10\n",
      "620/620 - 21s - 34ms/step - accuracy: 0.9538 - loss: 0.1153 - val_accuracy: 0.9342 - val_loss: 0.1552\n",
      "Epoch 7/10\n",
      "620/620 - 21s - 34ms/step - accuracy: 0.9571 - loss: 0.1071 - val_accuracy: 0.9383 - val_loss: 0.1591\n",
      "Epoch 8/10\n",
      "620/620 - 21s - 34ms/step - accuracy: 0.9623 - loss: 0.0982 - val_accuracy: 0.9374 - val_loss: 0.1627\n",
      "Epoch 9/10\n",
      "620/620 - 21s - 34ms/step - accuracy: 0.9667 - loss: 0.0862 - val_accuracy: 0.9350 - val_loss: 0.1659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/davidson_binary_glove_tokenizer.joblib']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "MAX_LEN = 100\n",
    "MAX_WORDS = 10000\n",
    "EMBED_DIM = 100\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "GLOVE_PATH = \"/Users/sandyajaleshkumar/Desktop/Practicum/glove.6B/glove.6B.100d.txt\"  \n",
    "\n",
    "train_df = pd.read_csv(\"./splits/davidson_train_binary.csv\")\n",
    "val_df = pd.read_csv(\"./splits/davidson_val_binary.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "texts_val = val_df[\"clean_text\"].tolist()\n",
    "y_train = train_df[\"label\"].values\n",
    "y_val = val_df[\"label\"].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(texts_train), maxlen=MAX_LEN)\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(texts_val), maxlen=MAX_LEN)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(len(word_index) + 1, EMBED_DIM, weights=[embedding_matrix], input_length=MAX_LEN, trainable=False),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "          epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stop], verbose=2)\n",
    "\n",
    "model.save(\"./models/davidson_binary_glove_bilstm.h5\")\n",
    "joblib.dump(tokenizer, \"./models/davidson_binary_glove_tokenizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ce31851c-13b1-4bac-99f8-53b3bcdf79a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      " GloVe + BiLSTM (Binary) on Davidson Val/Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8504    0.7380    0.7902       416\n",
      "           1     0.9485    0.9738    0.9610      2062\n",
      "\n",
      "    accuracy                         0.9342      2478\n",
      "   macro avg     0.8995    0.8559    0.8756      2478\n",
      "weighted avg     0.9320    0.9342    0.9323      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_val_binary.csv\")  \n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = joblib.load(\"./models/davidson_binary_glove_tokenizer.joblib\")\n",
    "model = load_model(\"./models/davidson_binary_glove_bilstm.h5\")\n",
    "\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "\n",
    "y_probs = model.predict(X)\n",
    "y_pred = (y_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"\\n GloVe + BiLSTM (Binary) on Davidson Val/Test:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "620205cf-021d-478b-b099-67e618e65773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      " GloVe + BiLSTM (Binary) on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8447    0.7698    0.8055       417\n",
      "           1     0.9543    0.9714    0.9627      2062\n",
      "\n",
      "    accuracy                         0.9375      2479\n",
      "   macro avg     0.8995    0.8706    0.8841      2479\n",
      "weighted avg     0.9358    0.9375    0.9363      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test_binary.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "tokenizer = joblib.load(\"./models/davidson_binary_glove_tokenizer.joblib\")\n",
    "model = load_model(\"./models/davidson_binary_glove_bilstm.h5\")\n",
    "\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "\n",
    "y_probs = model.predict(X)\n",
    "y_pred = (y_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"\\n GloVe + BiLSTM (Binary) on Davidson Test Set:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7eaf2526-4739-4d22-8c3c-d0498a7572da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT sequence embeddings: 100%|█| 19826/19826 [18:29<00:00, 17.87it/s\n",
      "Extracting BERT sequence embeddings: 100%|██| 2478/2478 [02:14<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: BERT sequence embeddings for Davidson train and val\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/davidson_train_binary.csv\")\n",
    "val_df = pd.read_csv(\"./splits/davidson_val_binary.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val = val_df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_seq = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy() \n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_bert_sequence_embeddings(texts_train, tokenizer, model)\n",
    "X_val = get_bert_sequence_embeddings(texts_val, tokenizer, model)\n",
    "np.save(\"./models/davidson_bert_embed_train_seq.npy\", X_train)\n",
    "np.save(\"./models/davidson_bert_embed_val_seq.npy\", X_val)\n",
    "\n",
    "print(\" Saved: BERT sequence embeddings for Davidson train and val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4d12c284-2697-432c-92c7-9c31e3a65b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "620/620 - 81s - 130ms/step - accuracy: 0.9097 - loss: 0.2206 - val_accuracy: 0.9274 - val_loss: 0.1692\n",
      "Epoch 2/10\n",
      "620/620 - 64s - 104ms/step - accuracy: 0.9429 - loss: 0.1393 - val_accuracy: 0.9399 - val_loss: 0.1385\n",
      "Epoch 3/10\n",
      "620/620 - 69s - 111ms/step - accuracy: 0.9562 - loss: 0.1118 - val_accuracy: 0.9459 - val_loss: 0.1281\n",
      "Epoch 4/10\n",
      "620/620 - 44s - 71ms/step - accuracy: 0.9637 - loss: 0.0937 - val_accuracy: 0.9403 - val_loss: 0.1467\n",
      "Epoch 5/10\n",
      "620/620 - 45s - 73ms/step - accuracy: 0.9706 - loss: 0.0790 - val_accuracy: 0.9431 - val_loss: 0.1233\n",
      "Epoch 6/10\n",
      "620/620 - 47s - 75ms/step - accuracy: 0.9785 - loss: 0.0599 - val_accuracy: 0.9427 - val_loss: 0.1463\n",
      "Epoch 7/10\n",
      "620/620 - 42s - 68ms/step - accuracy: 0.9823 - loss: 0.0487 - val_accuracy: 0.9451 - val_loss: 0.1666\n",
      "Epoch 8/10\n",
      "620/620 - 44s - 71ms/step - accuracy: 0.9870 - loss: 0.0379 - val_accuracy: 0.9342 - val_loss: 0.2025\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT + BiLSTM (Binary) on Davidson Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8266    0.8365    0.8315       416\n",
      "           1     0.9669    0.9646    0.9658      2062\n",
      "\n",
      "    accuracy                         0.9431      2478\n",
      "   macro avg     0.8968    0.9006    0.8987      2478\n",
      "weighted avg     0.9434    0.9431    0.9432      2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train = np.load(\"./models/davidson_bert_embed_train_seq.npy\")  \n",
    "X_val = np.load(\"./models/davidson_bert_embed_val_seq.npy\")\n",
    "\n",
    "y_train = pd.read_csv(\"./splits/davidson_train_binary.csv\")[\"label\"].values\n",
    "y_val = pd.read_csv(\"./splits/davidson_val_binary.csv\")[\"label\"].values\n",
    "\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64), input_shape=(100, 768)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "          epochs=10, batch_size=32, callbacks=[early_stop], verbose=2)\n",
    "\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"\\n BERT + BiLSTM (Binary) on Davidson Validation:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "model.save(\"./models/davidson_binary_bert_bilstm.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f20e6ed-4194-4fd1-a975-f07626c95e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step\n",
      "\n",
      " BERT + BiLSTM (Binary) on Davidson Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8491    0.8633    0.8561       417\n",
      "           1     0.9723    0.9690    0.9706      2062\n",
      "\n",
      "    accuracy                         0.9512      2479\n",
      "   macro avg     0.9107    0.9161    0.9134      2479\n",
      "weighted avg     0.9515    0.9512    0.9514      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_test = np.load(\"./models/davidson_bert_embed_test_seq.npy\")\n",
    "y_test = pd.read_csv(\"./splits/davidson_test_binary.csv\")[\"label\"].values\n",
    "\n",
    "model = load_model(\"./models/davidson_binary_bert_bilstm.h5\")\n",
    "\n",
    "y_probs = model.predict(X_test)\n",
    "y_pred = (y_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"\\n BERT + BiLSTM (Binary) on Davidson Test Set:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e6f939c-7549-416d-8025-fc0dbd99be3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT test embeddings: 100%|██████| 2479/2479 [02:03<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: ./models/davidson_bert_embed_test_seq.npy with shape (2479, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test_binary.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_seq = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT test embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()  # (100, 768)\n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_test = get_bert_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/davidson_bert_embed_test_seq.npy\", X_test)\n",
    "\n",
    "print(\" Saved: ./models/davidson_bert_embed_test_seq.npy with shape\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b09a4cec-3b13-4a43-bdaa-9db01fd9466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step\n",
      "\n",
      " SOFT VOTING ENSEMBLE (Binary): Davidson Test Set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8819    0.8777    0.8798       417\n",
      "           1     0.9753    0.9762    0.9758      2062\n",
      "\n",
      "    accuracy                         0.9597      2479\n",
      "   macro avg     0.9286    0.9270    0.9278      2479\n",
      "weighted avg     0.9596    0.9597    0.9596      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/davidson_test_binary.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_binary_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)[:, 1] \n",
    "\n",
    "glove_model = load_model(\"./models/davidson_binary_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_binary_glove_tokenizer.joblib\")\n",
    "X_glove = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(X_glove).reshape(-1)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_binary_bert_bilstm.h5\")\n",
    "X_bert = np.load(\"./models/davidson_bert_embed_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(X_bert).reshape(-1)\n",
    "\n",
    "svc_weight = 0.3\n",
    "glove_weight = 0.3\n",
    "bert_weight = 0.4\n",
    "\n",
    "ensemble_probs = (\n",
    "    svc_weight * svc_probs +\n",
    "    glove_weight * glove_probs +\n",
    "    bert_weight * bert_probs\n",
    ")\n",
    "\n",
    "y_pred = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n SOFT VOTING ENSEMBLE (Binary): Davidson Test Set\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28ea6304-2237-445d-b68d-ac00294c0ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n",
      "\n",
      " CROSS-DATASET: Davidson-Trained Ensemble on EDOS Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8836    0.3307    0.4813      1515\n",
      "           1     0.2924    0.8639    0.4369       485\n",
      "\n",
      "    accuracy                         0.4600      2000\n",
      "   macro avg     0.5880    0.5973    0.4591      2000\n",
      "weighted avg     0.7402    0.4600    0.4705      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "edos_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = edos_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = edos_df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/davidson_binary_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)[:, 1]  \n",
    "\n",
    "glove_model = load_model(\"./models/davidson_binary_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/davidson_binary_glove_tokenizer.joblib\")\n",
    "X_glove = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(X_glove).reshape(-1)\n",
    "\n",
    "bert_model = load_model(\"./models/davidson_binary_bert_bilstm.h5\")\n",
    "X_bert = np.load(\"./models/davidson_bert_embed_on_edos_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(X_bert).reshape(-1)\n",
    "\n",
    "ensemble_probs = (\n",
    "    0.3 * svc_probs +\n",
    "    0.3 * glove_probs +\n",
    "    0.4 * bert_probs\n",
    ")\n",
    "y_pred = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n CROSS-DATASET: Davidson-Trained Ensemble on EDOS Test\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0bc651e-3385-4278-9e04-4d664230f509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings (EDOS): 100%|████| 2000/2000 [01:44<00:00, 19.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: davidson_bert_embed_on_edos_test_seq.npy with shape (2000, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_seq = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT embeddings (EDOS)\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy() \n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_bert = get_bert_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/davidson_bert_embed_on_edos_test_seq.npy\", X_bert)\n",
    "\n",
    "print(\" Saved: davidson_bert_embed_on_edos_test_seq.npy with shape\", X_bert.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ec237-6cd7-4b7a-a035-07884ee5b486",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training of HateXplain to test on EDOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2033c30-24d4-49d4-a257-71d47881a7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HateXplain binary splits:\n",
      " - hatexplain_train_binary.csv\n",
      " - hatexplain_val_binary.csv\n",
      " - hatexplain_test_binary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/hatexplain_train.csv\")\n",
    "train_df[\"label\"] = train_df[\"label\"].map({0: 0, 1: 1, 2: 1})\n",
    "train_df.to_csv(\"./splits/hatexplain_train_binary.csv\", index=False)\n",
    "\n",
    "val_df = pd.read_csv(\"./splits/hatexplain_val.csv\")\n",
    "val_df[\"label\"] = val_df[\"label\"].map({0: 0, 1: 1, 2: 1})\n",
    "val_df.to_csv(\"./splits/hatexplain_val_binary.csv\", index=False)\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "test_df[\"label\"] = test_df[\"label\"].map({0: 0, 1: 1, 2: 1})\n",
    "test_df.to_csv(\"./splits/hatexplain_test_binary.csv\", index=False)\n",
    "\n",
    "print(\"Saved HateXplain binary splits:\")\n",
    "print(\" - hatexplain_train_binary.csv\")\n",
    "print(\" - hatexplain_val_binary.csv\")\n",
    "print(\" - hatexplain_test_binary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c3362f-0847-4875-a628-f20f442451d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TF-IDF + SVC (Binary) on HateXplain:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6513    0.6029    0.6262       821\n",
      "           1     0.7402    0.7781    0.7587      1194\n",
      "\n",
      "    accuracy                         0.7067      2015\n",
      "   macro avg     0.6958    0.6905    0.6924      2015\n",
      "weighted avg     0.7040    0.7067    0.7047      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/hatexplain_train_binary.csv\")\n",
    "val_df = pd.read_csv(\"./splits/hatexplain_val_binary.csv\")\n",
    "X_train, y_train = train_df[\"clean_text\"], train_df[\"label\"]\n",
    "X_val, y_val = val_df[\"clean_text\"], val_df[\"label\"]\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=10000)),\n",
    "    (\"svc\", SVC(probability=True, kernel=\"linear\", C=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "joblib.dump(pipeline, \"./models/hatexplain_binary_svc_prob.joblib\")\n",
    "print(\"\\n TF-IDF + SVC (Binary) on HateXplain:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1547be7-7d45-4c9c-83d6-22e8f24461ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 - 20s - 39ms/step - accuracy: 0.6491 - loss: 0.6189 - val_accuracy: 0.6998 - val_loss: 0.5805\n",
      "Epoch 2/10\n",
      "504/504 - 18s - 36ms/step - accuracy: 0.7049 - loss: 0.5631 - val_accuracy: 0.7097 - val_loss: 0.5659\n",
      "Epoch 3/10\n",
      "504/504 - 18s - 35ms/step - accuracy: 0.7188 - loss: 0.5475 - val_accuracy: 0.7107 - val_loss: 0.5624\n",
      "Epoch 4/10\n",
      "504/504 - 18s - 35ms/step - accuracy: 0.7282 - loss: 0.5339 - val_accuracy: 0.7176 - val_loss: 0.5564\n",
      "Epoch 5/10\n",
      "504/504 - 18s - 35ms/step - accuracy: 0.7356 - loss: 0.5210 - val_accuracy: 0.7345 - val_loss: 0.5513\n",
      "Epoch 6/10\n",
      "504/504 - 18s - 36ms/step - accuracy: 0.7475 - loss: 0.5058 - val_accuracy: 0.7241 - val_loss: 0.5507\n",
      "Epoch 7/10\n",
      "504/504 - 17s - 35ms/step - accuracy: 0.7547 - loss: 0.4944 - val_accuracy: 0.7151 - val_loss: 0.5544\n",
      "Epoch 8/10\n",
      "504/504 - 19s - 37ms/step - accuracy: 0.7668 - loss: 0.4802 - val_accuracy: 0.7206 - val_loss: 0.5620\n",
      "Epoch 9/10\n",
      "504/504 - 19s - 37ms/step - accuracy: 0.7778 - loss: 0.4653 - val_accuracy: 0.7325 - val_loss: 0.5501\n",
      "Epoch 10/10\n",
      "504/504 - 18s - 35ms/step - accuracy: 0.7875 - loss: 0.4461 - val_accuracy: 0.7261 - val_loss: 0.5685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./models/hatexplain_binary_glove_tokenizer.joblib']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "\n",
    "MAX_LEN = 100\n",
    "MAX_WORDS = 10000\n",
    "EMBED_DIM = 100\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "GLOVE_PATH = \"/Users/sandyajaleshkumar/Desktop/Practicum/glove.6B/glove.6B.100d.txt\" \n",
    "\n",
    "train_df = pd.read_csv(\"./splits/hatexplain_train_binary.csv\")\n",
    "val_df = pd.read_csv(\"./splits/hatexplain_val_binary.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].tolist()\n",
    "texts_val = val_df[\"clean_text\"].tolist()\n",
    "y_train = train_df[\"label\"].values\n",
    "y_val = val_df[\"label\"].values\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(texts_train), maxlen=MAX_LEN)\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(texts_val), maxlen=MAX_LEN)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_PATH, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        vec = embeddings_index.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[i] = vec\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(len(word_index) + 1, EMBED_DIM, weights=[embedding_matrix], input_length=MAX_LEN, trainable=False),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "          epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stop], verbose=2)\n",
    "\n",
    "model.save(\"./models/hatexplain_binary_glove_bilstm.h5\")\n",
    "joblib.dump(tokenizer, \"./models/hatexplain_binary_glove_tokenizer.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6da9d97a-b08d-4f6b-b481-ceafd47c067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      " GloVe + BiLSTM (Binary) on HateXplain Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6695    0.6784    0.6739       821\n",
      "           1     0.7768    0.7697    0.7732      1194\n",
      "\n",
      "    accuracy                         0.7325      2015\n",
      "   macro avg     0.7232    0.7241    0.7236      2015\n",
      "weighted avg     0.7331    0.7325    0.7328      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "val_df = pd.read_csv(\"./splits/hatexplain_val_binary.csv\")\n",
    "texts = val_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = val_df[\"label\"].astype(int).values\n",
    "\n",
    "model = load_model(\"./models/hatexplain_binary_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_binary_glove_tokenizer.joblib\")\n",
    "\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "\n",
    "y_probs = model.predict(X_val)\n",
    "y_pred = (y_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"\\n GloVe + BiLSTM (Binary) on HateXplain Validation Set:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac9f33b9-9671-4ae0-90d7-95ee65471c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT sequence embeddings: 100%|█| 16118/16118 [20:55<00:00, 12.84it/s\n",
      "Extracting BERT sequence embeddings: 100%|██| 2015/2015 [03:06<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved BERT sequence embeddings for HateXplain train and val.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_df = pd.read_csv(\"./splits/hatexplain_train_binary.csv\")\n",
    "val_df = pd.read_csv(\"./splits/hatexplain_val_binary.csv\")\n",
    "\n",
    "texts_train = train_df[\"clean_text\"].astype(str).tolist()\n",
    "texts_val = val_df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_seq = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT sequence embeddings\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()  # shape: (100, 768)\n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_train = get_bert_sequence_embeddings(texts_train, tokenizer, model)\n",
    "X_val = get_bert_sequence_embeddings(texts_val, tokenizer, model)\n",
    "np.save(\"./models/hatexplain_bert_embed_train_seq.npy\", X_train)\n",
    "np.save(\"./models/hatexplain_bert_embed_val_seq.npy\", X_val)\n",
    "\n",
    "print(\"Saved BERT sequence embeddings for HateXplain train and val.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d80892a-3fd2-4121-a1f2-27d0f738a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "504/504 - 49s - 98ms/step - accuracy: 0.6722 - loss: 0.6029 - val_accuracy: 0.7201 - val_loss: 0.5498\n",
      "Epoch 2/10\n",
      "504/504 - 47s - 94ms/step - accuracy: 0.7250 - loss: 0.5400 - val_accuracy: 0.7360 - val_loss: 0.5312\n",
      "Epoch 3/10\n",
      "504/504 - 95s - 188ms/step - accuracy: 0.7515 - loss: 0.5038 - val_accuracy: 0.7444 - val_loss: 0.5247\n",
      "Epoch 4/10\n",
      "504/504 - 83s - 164ms/step - accuracy: 0.7735 - loss: 0.4742 - val_accuracy: 0.7380 - val_loss: 0.5220\n",
      "Epoch 5/10\n",
      "504/504 - 68s - 134ms/step - accuracy: 0.7992 - loss: 0.4349 - val_accuracy: 0.7385 - val_loss: 0.5454\n",
      "Epoch 6/10\n",
      "504/504 - 57s - 114ms/step - accuracy: 0.8311 - loss: 0.3829 - val_accuracy: 0.7325 - val_loss: 0.5958\n",
      "Epoch 7/10\n",
      "504/504 - 62s - 124ms/step - accuracy: 0.8628 - loss: 0.3223 - val_accuracy: 0.7350 - val_loss: 0.6328\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BERT + BiLSTM (Binary) on HateXplain Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6802    0.6736    0.6769       821\n",
      "           1     0.7770    0.7822    0.7796      1194\n",
      "\n",
      "    accuracy                         0.7380      2015\n",
      "   macro avg     0.7286    0.7279    0.7282      2015\n",
      "weighted avg     0.7376    0.7380    0.7378      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train = np.load(\"./models/hatexplain_bert_embed_train_seq.npy\")\n",
    "X_val = np.load(\"./models/hatexplain_bert_embed_val_seq.npy\")\n",
    "\n",
    "y_train = pd.read_csv(\"./splits/hatexplain_train_binary.csv\")[\"label\"].values\n",
    "y_val = pd.read_csv(\"./splits/hatexplain_val_binary.csv\")[\"label\"].values\n",
    "\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64), input_shape=(100, 768)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=10, batch_size=32,\n",
    "          callbacks=[early_stop], verbose=2)\n",
    "\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"\\n BERT + BiLSTM (Binary) on HateXplain Validation Set:\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "model.save(\"./models/hatexplain_binary_bert_bilstm.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e959057-2fe9-43de-ab4c-dbc319e89e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n",
      "\n",
      " HateXplain-Trained Ensemble on EDOS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8196    0.5578    0.6638      1515\n",
      "           1     0.3086    0.6165    0.4113       485\n",
      "\n",
      "    accuracy                         0.5720      2000\n",
      "   macro avg     0.5641    0.5871    0.5375      2000\n",
      "weighted avg     0.6957    0.5720    0.6026      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/hatexplain_binary_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)[:, 1]  \n",
    "\n",
    "glove_model = load_model(\"./models/hatexplain_binary_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_binary_glove_tokenizer.joblib\")\n",
    "X_glove = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(X_glove).reshape(-1)\n",
    "\n",
    "bert_model = load_model(\"./models/hatexplain_binary_bert_bilstm.h5\")\n",
    "X_bert = np.load(\"./models/hatexplain_bert_embed_on_edos_test_seq.npy\")  \n",
    "\n",
    "ensemble_probs = (\n",
    "    0.5 * svc_probs +\n",
    "    0.3 * glove_probs +\n",
    "    0.2 * bert_probs\n",
    ")\n",
    "y_pred = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n HateXplain-Trained Ensemble on EDOS Test Set:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b4a3a91-ba24-4659-bcfc-a9292e25bb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings (EDOS): 100%|████| 2000/2000 [02:50<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: hatexplain_bert_embed_on_edos_test_seq.npy with shape (2000, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = bert_model.encode(edos_test_texts)\n",
    "np.save('./models/hatexplain_bert_embed_on_edos_test_seq.npy', embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28437bf3-b990-4d9f-a6da-cbae68196322",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training of EDOS to test on HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b3902c1-eee3-4407-8c1c-d23c199a13c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n",
      "\n",
      " EDOS-Trained Ensemble on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4167    0.9318    0.5758       821\n",
      "           1     0.6872    0.1030    0.1792      1194\n",
      "\n",
      "    accuracy                         0.4407      2015\n",
      "   macro avg     0.5519    0.5174    0.3775      2015\n",
      "weighted avg     0.5769    0.4407    0.3408      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test_binary.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/edos_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)[:, 1]\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "X_glove = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(X_glove).reshape(-1)\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_bilstm.h5\")\n",
    "X_bert = np.load(\"./models/edos_bert_embed_on_hatexplain_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(X_bert).reshape(-1)\n",
    "\n",
    "ensemble_probs = (\n",
    "    0.4 * svc_probs +\n",
    "    0.3 * glove_probs +\n",
    "    0.3 * bert_probs\n",
    ")\n",
    "y_pred = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n EDOS-Trained Ensemble on HateXplain Test Set:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2590c477-437c-4d8f-be8f-5482a9626c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\n",
      " EDOS-Trained Ensemble on HateXplain Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4160    0.9233    0.5736       821\n",
      "           1     0.6736    0.1089    0.1875      1194\n",
      "\n",
      "    accuracy                         0.4407      2015\n",
      "   macro avg     0.5448    0.5161    0.3805      2015\n",
      "weighted avg     0.5686    0.4407    0.3448      2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test_binary.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/edos_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)[:, 1]\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "X_glove = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(X_glove).reshape(-1)\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_bilstm.h5\")\n",
    "X_bert = np.load(\"./models/edos_bert_embed_on_hatexplain_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(X_bert).reshape(-1)\n",
    "\n",
    "ensemble_probs = (\n",
    "    0.5 * svc_probs +\n",
    "    0.3 * glove_probs +\n",
    "    0.2 * bert_probs\n",
    ")\n",
    "y_pred = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n EDOS-Trained Ensemble on HateXplain Test Set:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c3af7ef-5e26-4341-922c-0b58f2f9f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings (EDOS → HateXplain): 100%|█| 2015/2015 [02:09<00:00, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: edos_bert_embed_on_hatexplain_test_seq.npy with shape (2015, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/hatexplain_test_binary.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_seq = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT embeddings (EDOS → HateXplain)\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()  \n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_bert = get_bert_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/edos_bert_embed_on_hatexplain_test_seq.npy\", X_bert)\n",
    "\n",
    "print(\"Saved: edos_bert_embed_on_hatexplain_test_seq.npy with shape\", X_bert.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dae8e8-fbe6-4d0c-b897-e942c6a4c95b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training of EDOS to test on Davidson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "288f3112-f29d-4823-817c-39bb8d6e22c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n",
      "\n",
      " EDOS-Trained Ensemble on Davidson Binary Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3355    0.9976    0.5021       417\n",
      "           1     0.9992    0.6004    0.7501      2062\n",
      "\n",
      "    accuracy                         0.6672      2479\n",
      "   macro avg     0.6673    0.7990    0.6261      2479\n",
      "weighted avg     0.8875    0.6672    0.7084      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test_binary.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = df[\"label\"].astype(int).values\n",
    "\n",
    "svc = joblib.load(\"./models/edos_svc_prob.joblib\")\n",
    "svc_probs = svc.predict_proba(texts)[:, 1]\n",
    "\n",
    "glove_model = load_model(\"./models/edos_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "X_glove = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(X_glove).reshape(-1)\n",
    "\n",
    "bert_model = load_model(\"./models/edos_bert_bilstm.h5\")\n",
    "X_bert = np.load(\"./models/edos_bert_embed_on_davidson_test_seq.npy\")\n",
    "bert_probs = bert_model.predict(X_bert).reshape(-1)\n",
    "\n",
    "ensemble_probs = (\n",
    "    0.4 * svc_probs +\n",
    "    0.3 * glove_probs +\n",
    "    0.3 * bert_probs\n",
    ")\n",
    "y_pred = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n EDOS-Trained Ensemble on Davidson Binary Test Set:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "444db5c4-5f57-43da-9d14-ca95d1eb1035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting BERT embeddings (EDOS → Davidson): 100%|█| 2479/2479 [02:18<00:00, 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: edos_bert_embed_on_davidson_test_seq.npy with shape (2479, 100, 768)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"./splits/davidson_test_binary.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_bert_sequence_embeddings(texts, tokenizer, model, max_len=100):\n",
    "    all_seq = []\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Extracting BERT embeddings (EDOS → Davidson)\"):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                               truncation=True, max_length=max_len)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            all_seq.append(last_hidden)\n",
    "    return np.array(all_seq)\n",
    "\n",
    "X_bert = get_bert_sequence_embeddings(texts, tokenizer, model)\n",
    "np.save(\"./models/edos_bert_embed_on_davidson_test_seq.npy\", X_bert)\n",
    "\n",
    "print(\" Saved: edos_bert_embed_on_davidson_test_seq.npy with shape\", X_bert.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb93db-ca00-4e36-879e-11a2cfee2947",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# AUC-ROC  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "43fb5f2f-7092-426c-a4b0-b5ab51e6bb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "xgb_probs = np.load(\"edos_xgb_probs.npy\")        \n",
    "glove_probs = np.load(\"edos_glove_probs.npy\")    \n",
    "bert_probs = np.load(\"edos_bert_probs.npy\")      \n",
    "y_true = np.load(\"edos_y_true.npy\")              \n",
    "\n",
    "assert set(np.unique(y_true)) <= {0, 1}, \"Only for binary classification.\"\n",
    "y_true_bin = label_binarize(y_true, classes=[0, 1])  \n",
    "\n",
    "X_meta = np.hstack([xgb_probs, glove_probs, bert_probs]) \n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000)\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "y_pred_probs = meta_clf.predict_proba(X_meta)\n",
    "\n",
    "if y_pred_probs.shape[1] == 2:\n",
    "    positive_probs = y_pred_probs[:, 1]\n",
    "else:\n",
    "    positive_probs = np.zeros_like(y_true, dtype=float)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, positive_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fpr, tpr, label=f\"Stacking Ensemble (AUC = {roc_auc:.2f})\", color=\"darkblue\", lw=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve — EDOS Stacking Ensemble\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "4a014929-b501-4126-81c3-5c1c91efed92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      " Saved all required .npy files for EDOS stacking ensemble.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/edos_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "#TF-IDF + XGB\n",
    "xgb = joblib.load(\"./models/edos_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "np.save(\"edos_xgb_probs.npy\", xgb_probs)\n",
    "\n",
    "#GloVe + CNN\n",
    "glove_model = load_model(\"./models/edos_glove_cnn.h5\")\n",
    "tokenizer = joblib.load(\"./models/edos_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "np.save(\"edos_glove_probs.npy\", glove_probs)\n",
    "\n",
    "#BERT + CNN\n",
    "bert_model = load_model(\"./models/edos_bert_cnn.h5\")\n",
    "bert_X = np.load(\"./models/edos_bert_embed_test_seq.npy\")  \n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "np.save(\"edos_bert_probs.npy\", bert_probs)\n",
    "\n",
    "\n",
    "np.save(\"edos_y_true.npy\", y_true)\n",
    "\n",
    "print(\" Saved all required .npy files for EDOS stacking ensemble.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "bd40c7e6-b56f-4782-866a-a43e4fcc8d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "xgb_probs = np.load(\"hatexplain_xgb_probs.npy\")      \n",
    "glove_probs = np.load(\"hatexplain_glove_probs.npy\") \n",
    "bert_probs = np.load(\"hatexplain_bert_probs.npy\")   \n",
    "y_true = np.load(\"hatexplain_y_true.npy\")           \n",
    "\n",
    "classes = [0, 1, 2]\n",
    "y_true_bin = label_binarize(y_true, classes=classes)  \n",
    "\n",
    "X_meta = np.hstack([xgb_probs, glove_probs, bert_probs])  \n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "meta_clf.fit(X_meta, y_true)\n",
    "y_pred_probs = meta_clf.predict_proba(X_meta)  \n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['darkorange', 'seagreen', 'steelblue']\n",
    "labels = [\"Normal (0)\", \"Offensive (1)\", \"Hate (2)\"]\n",
    "for i in range(3):\n",
    "    plt.plot(fpr[i], tpr[i], color=colors[i], lw=2, label=f\"{labels[i]} (AUC = {roc_auc[i]:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC Curve — HateXplain Stacking Ensemble')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "814614c6-6694-4301-a92c-a218d1dbdeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "All model outputs and labels saved as .npy files.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "test_df = pd.read_csv(\"./splits/hatexplain_test.csv\")\n",
    "texts = test_df[\"clean_text\"].astype(str).tolist()\n",
    "y_true = test_df[\"label\"].astype(int).values\n",
    "\n",
    "#TF-IDF + XGB\n",
    "xgb = joblib.load(\"./models/hatexplain_xgb.joblib\")\n",
    "xgb_probs = xgb.predict_proba(texts)\n",
    "np.save(\"hatexplain_xgb_probs.npy\", xgb_probs)\n",
    "\n",
    "#GloVe + BiLSTM \n",
    "glove_model = load_model(\"./models/hatexplain_glove_bilstm.h5\")\n",
    "tokenizer = joblib.load(\"./models/hatexplain_glove_tokenizer.joblib\")\n",
    "glove_X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
    "glove_probs = glove_model.predict(glove_X)\n",
    "np.save(\"hatexplain_glove_probs.npy\", glove_probs)\n",
    "\n",
    "#BERT + BiLSTM \n",
    "bert_model = load_model(\"./models/hatexplain_bert_bilstm_best.h5\")\n",
    "bert_X = np.load(\"./models/hatexplain_bert_embed_test.npy\")\n",
    "bert_probs = bert_model.predict(bert_X)\n",
    "np.save(\"hatexplain_bert_probs.npy\", bert_probs)\n",
    "\n",
    "\n",
    "np.save(\"hatexplain_y_true.npy\", y_true)\n",
    "\n",
    "print(\"All model outputs and labels saved as .npy files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
